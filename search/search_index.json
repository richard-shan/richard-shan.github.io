{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#richard-shan","title":"Richard Shan","text":""},{"location":"about/","title":"About","text":""},{"location":"about/#about-me","title":"About Me","text":""},{"location":"coding/","title":"Coding","text":""},{"location":"coding/#here-are-some-of-my-coding-projects","title":"Here are some of my coding projects:","text":""},{"location":"coding/#macro-maker","title":"Macro Maker","text":""},{"location":"conrad/","title":"BrailleBox - Conrad Spirit of Innovation Challenge","text":""},{"location":"conrad/#overview","title":"Overview","text":"<p>I want to create a 3x2 solenoid array that can display braille characters by pushing solenoids up and down to create dots. This solenoid array will be connected to a Raspberry Pi, which in turn will be connected to an ESP32CAM. The camera will take a picture of a page of text, then perform OCR (optical character recognition) to extract a string of text from the image. That string of text will be converted to braille, which will be displayed on the solenoid array by flashing each character for 1 second at a time. This device will essentially allow for live-time conversion of any text into braille, which I hope will increase accessibility to books and the like.</p>"},{"location":"conrad/#brainstorming-process","title":"Brainstorming Process","text":""},{"location":"conrad/#initial-thoughts","title":"Initial Thoughts","text":"<p>My idea was to design a text to braille converter, which a blind person could use by moving the device over a page of text to convert it into braille. The braille translation of the English text would then be represented via a series of up/down pins which the user could use to interpret the information. The device was to be a rectangular box that would use an internal camera to interpret and OCR text, which could then be translated into braille and displayed via a series of servo motors pushing up metal rods on the top of the box. The pins would be in groups of six, each group representing a single braille character.</p> <p>However, I talked to Stuart Christhilf who had thought of a similar mechanism for his initial final project. He originally planned to create a dynamic clock to display the time using blocks of wood that acould be pushed out or pulled back via servos. However, when building his project, he realized that fitting so many servos into such a small space was completely unfeasible and warned me from doing the same. My initial design is shown in the following image:</p> <p> </p> <p>I then decided to use electromagnets for my pins, instead of a servo. The pins themselves would be a small magnetic rod sitting on top of an electromagnet. The small electromagnet could be powered on and off via a microcontroller. When the electromagnet was off, the pin would simply rest on top of the electromagnet, and the pin would be flush against the top of the board, forming the down position of the pin. If the pin needed to pop up, the microcontroller would power the electromagnet which would then emit a repelling magnetic charge. That magnetic force would then repel the pin slightly upwards, forming the up position of the pin. To represent a braille character, the microcontroller would push the specific pins into the up position that together would form the 6-dot pattern of the character.</p> <p>I also decided to move the camera out of the box. That would allow for more simple wiring and internal organization of the box, and allow the operator to more easily use the device. Moving the camera out means that the user would only need to move a small camera container across the page of text, instead of dragging the entire device. Here is my modified design:</p> <p> </p>"},{"location":"conrad/#significant-changes","title":"Significant Changes","text":"<p>Although a large part of my project remains the same, I've changed some aspects of my project. Namely, I've decided to use a Raspberry Pi as a central controller and connect it to 5 separate ATTiny412 chips, which will each be responsible for controlling 6 electromagnets to represent 1 braille character. Each ATTiny412 and 6 electromagnet setup will be on its own PCB, and receive data from the controlling Raspberry Pi. Additionally, I decided to create an elevated case for the ESP32 camera so that the image would have a better angle and thus an easier time being processed for OCR, and so that more light could come into the camera lens from the unobstructed sides. Lastly, I decided I wanted to wirelessly transmit data from the ESP32 camera to the Raspberry Pi for processing. I worked with both serial communication and WiFi connectivity previously so I hope to sum it all together and wirelessly transmit data between these two controllers.</p> <p>Here is an updated system diagram which maps out all the parts of my project.</p> <p> </p>"},{"location":"conrad/#feasibility","title":"Feasibility","text":"<p>However, after doing research, I realized that having 30 solenoids would be unfeasible. Instead, I decided to scale my project down to just having 6 solenoids, as this would still accomplish the mission of displaying braille for a reader. I would then flash each braille character for 1 second on the 6 solenoid array. This change allows me to worry less about power budget and ensures that I have a ready final project on my presentation date.</p>"},{"location":"conrad/#bill-of-materials","title":"Bill of Materials","text":""},{"location":"conrad/#components","title":"Components","text":""},{"location":"conrad/#braille-box-cad","title":"Braille Box CAD","text":""},{"location":"conrad/#initial-design","title":"Initial Design","text":"<p>I decided to first model my design in Fusion360, as I had prior experience working with Fusion and was pretty comfortable using it. When I started out with Autodesk Fusion, Kevin Kennedy's Fusion tutorials were a massive help.</p> <p>I first started off with a rectangular prism to act as the main body of the design.</p> <p> </p> <p>Next, I filleted the box to round out the edges.</p> <p> </p> <p>I then created a sketch on the top of the box, where I created six circles. These 6 circles represent the holes where I will put metal pins into that can pop up and down depending on what needs to be represented.</p> <p> </p> <p>I extruded the circles downward as holes. This creates the actual space where the pins will be placed.</p> <p> </p> <p>Finally, I used the pattern feature to repeat the sketch and extrusion across the top of the box. This created a total of 5 evenly spaced sets of 6 pins. With each set of 6 pins representing a single braille character, one iteration of pin setups can represent five letters.</p> <p> </p>"},{"location":"conrad/#improved-design","title":"Improved Design","text":"<p>As I had made my initial design early on, it did not reflect the changes I had made to my final project, most notably scaling down the amount of solenoids from 5 arrays of 3x2 to one 3x2 array. Additionally, when I made the original design, I didn't think much about how I would power the solenoid array and thus didn't include any spots for batteries. I also wanted to make the holes for the solenoids on a separate press-fit cover on top of the main box. Finally, the original design doesn't include any internal parts to hold the solenoids in place.</p> <p>For my new design, I want to make the following key changes:</p> <ul> <li>3x2 solenoid array</li> <li>Internal beams to support solenoids</li> <li>Battery pack holders</li> <li>Press-fit cover</li> </ul> <p>Additionally, I want the box to look as nice as possible and ideally have all wiring contained within it.</p> <p>I first started out by creating the shell of the box. I hollowed out the innards because I want my electronics to be inside. I will end up adhering my PCB to the side of the box and having my MOSFET breakouts on the bottom of the hollowed inside.</p> <p> </p> <p>I then started working on the top cover. I started out with creating a sketch where all my holes would be, and a offset on the edges to match the shell. I then extruded the sketch to create the cover with holes that the solenoids will fill.</p> <p> </p> <p>Based on my previous sketch offset for the edges of the shell box, I created quarter-circles and extruded them to form the press-fit lid.</p> <p> </p> <p>Next, I designed the beams that hold the solenoids in place. I started by creating a sketch on the bottom of the shell box and extruded that to my desired height. I then created a sketch on the extruded rectangular prism to remove the bottom part of it and form it into a beam-like shape.</p> <p> </p> <p>I then started work on the battery holders. I created the bottom of the battery holder then extruded out the sides.</p> <p> </p> <p>Next, I created the dividers to firmly hold each battery pack in place. Each divided section has the same length and width dimensions as the actual battery pack that I will use, plus a little for tolerance.</p> <p> </p> <p>I then added a small hole on the side for the power, ground, and TX/RX cables for the ATTiny1614.</p> <p> </p> <p>Next, I extruded a small hole as a slot for the wires on the external battery packs to route into the main shell box, where it will be connected to the MOSFETs controlling the solenoids.</p> <p> </p> <p>Finally, I added fillets. Here is the final box design.</p> <p> </p>"},{"location":"conrad/#raspberry-pi-box-cad","title":"Raspberry Pi Box CAD","text":"<p>I first started off with a shelled box.</p> <p> </p> <p>I then added a lid, with a hole the same size as my 5 inch touchscreen where I would attach the screen.</p> <p> </p> <p>Next, I added screw holes on the lid. These screw holes allowed me to secure the screen to the 3D printed lid. Unfortunately, my screw holes actually ended up being a little small so I had to enlarge them after the print with the help of a soldering iron.</p> <p> </p> <p>I added legs to the lid to allow it to press-fit into the base.</p> <p> </p> <p>Then, I added holes for wires on the front and side of the Pi case.</p> <p> </p> <p>Finally, I added fillets all around.</p> <p> </p> <p>After printing that initial iteration, the screen fit and there was enough space inside to fit the Raspberry Pi. However, some of my cables didn't fit as they had long \"necks\" that had to remain straight. As such, I would have to significantly bend the HDMI and USB cords for the Raspberry Pi.</p> <p> </p> <p>As such, I adjusted the length of the box to give space for the USB cable necks. I also slightly decreased the height of the holes for the USB cables as they were larger than necessary and somewhat an eyesore.</p> <p> </p>"},{"location":"conrad/#electronics","title":"Electronics","text":"<p>Electronics were by far the worst part of this project, at least for me. The main issue was that I didn't understand transistors very well, and I ran into a bunch of problems with them. The two main problems I ran into were transistors not being able to handle the power and transistors having inconsistent pinouts and being backwards or jumbled around.</p> <p>In this section, I'll go through a few of the boards that didn't work then show my final board.</p> <p>This was my initial board design. I tested this board by plugging a solenoid into the top pin, and the resulting lack of transistor is visible. The transistor heated up after around 5 seconds and fell off the board, without powering the solenoid.</p> <p> </p> <p>This is my second iteration. I added what I thought were pull down resistors hidden under the left-hand side white female pin headers (they did not, in fact, function as pull down resistors) and a power indicator LED. Unfortunately, I forgot the capacitor, but that would not have affected this board's outcome of failure. When creating this board, I also ran into major issues with the ATTiny1614, which stuck me for a couple hours. Apparently, some of the ATTiny1614 chips in our lab just didn't work, so I needed to get the ATTiny1614s out of a specific drawer because those chips had a small dot indentation on one side. Only the chips with the dot indentation seemed to work well, in my experience. Upon testing, the transistor got really hot and I unplugged it before it melted off.</p> <p> </p> <p>In an attempt to simplify the amount of things that could cause the issue, I scaled down to one transistor, which in turn melted off.</p> <p> </p> <p>I tweaked the design with one transistor, and it melted again.</p> <p> </p> <p>At this point, I created a ATTiny1614 board for testing and debugging.</p> <p> </p> <p>I created another board with a pull down and headers to plug into external transistors. The hope was that this would allow me to test transistors without melting pads and traces.</p> <p> </p> <p>At this point, I was fairly certain the transistor had a problem with handling power. I switched to the Eugepae board which had a different transistor. I had made this board during a group project for embedded networking and communications, and it had previously handled 5V, so I was pretty confused when it failed to power my 5V solenoids. Unfortunately, this board also failed, in retrospect likely because the solenoids pulled too many amps.</p> <p> </p> <p>I then decided to switch to a through hole MOSFET. This board also failed, which I'm pretty confused about but will explain in the next paragraph.</p> <p> </p> <p>After all these boards failed, I found MOSFET drive modules in the Lab. These modules have six key inputs: VIN+, VIN-, VOUT+, VOUT-, TRIG, and GND. I ended up connecting the VIN+ and VIN- to the positive and ground terminals on the power input device (battery packs), the VOUT+ and VOUT- to the positive and ground of the load output device (solenoid), the TRIG pin to a GPIO on my ATTiny1614 board that would toggle the solenoid on and off, and the GND to ground. The architecture of the drive module (which worked) was really similar to some of my MOSFET attempts, so I am still a little unsure why this board worked when my own didn't. The only major discrepancy that I noticed was that this board had 2 transistors.</p> <p> </p> <p>Now that I decided to use the MOSFET breakout boards to toggle the solenoids, the MOSFETs are external to the main board and I am able to create a ATTiny1614 control board without transistors on it. My final board has headers for its power, ground, and data; a power indicator LED; a capacitor; pins for TX/RX serial communication with the Raspberry Pi, which will send text to be displayed as braille; 6 headers each corresponding to a GPIO pin on the ATTiny1614 which in turn, corresponds to controlling a single solenoid in the 3x2 array; and 6 headers for GND that will each connect to 1 MOSFET.</p> <p> </p>"},{"location":"conrad/#esp32cam-wireless-transmission","title":"ESP32CAM Wireless Transmission","text":"<p>WebSocket connections are initiated through HTTP protocol, using an upgrade request from HTTP to WebSocket. This begins with a client sending a standard HTTP request that includes an \"Upgrade: websocket\" header and a \"Connection: Upgrade\" header to the server. The server then responds with an HTTP 101 status code, indicating that the protocol will change, thus establishing the WebSocket connection.</p> <p>WebSocket, uses IP addresses to facilitate the initial connection before upgrading to the WebSocket protocol. Once the WebSocket connection is established, the IP addresses are used to maintain the connection over which data frames can be reliably transmitted back and forth.</p> <p>Once the WebSocket connection is established, data is transmitted in framed messages through backend data transmission ports, where each frame consists of an opcode to indicate the type of data being transmitted (e.g., text, binary, continuation frame, or control frames like close, ping, or pong). This structure allows the WebSocket protocol to be extremely versatile and efficient in handling different types of data seamlessly. The frames are small and allow for very efficient data transmission.</p> <p>The following program is uploaded onto the ESP32 CAM Board through Arduino IDE. This program is based off of the CameraWebServer example program from ESP32.</p> <pre><code>#include \"esp_camera.h\"\n#include \"WiFi.h\"\n#include \"WebSocketsServer.h\"\n\n#define CAMERA_MODEL_AI_THINKER // Has PSRAM\n#include \"camera_pins.h\"\n\nconst char* ssid = \"REDACTED\";\nconst char* password = \"REDACTED\";\n\nWebSocketsServer webSocket = WebSocketsServer(81);\n\nvoid startCameraServer();\nvoid setupLedFlash(int pin);\nvoid onWebSocketEvent(uint8_t client_num, WStype_t type, uint8_t *payload, size_t length);\n\nvoid setup() {\n  pinMode(2, OUTPUT);\n  Serial.begin(9600);\n  while (!Serial); // Wait for the serial connection to initialize\n  Serial.setDebugOutput(true);\n  Serial.println();\n\n  camera_config_t config;\n  config.ledc_channel = LEDC_CHANNEL_0;\n  config.ledc_timer = LEDC_TIMER_0;\n  config.pin_d0 = Y2_GPIO_NUM;\n  config.pin_d1 = Y3_GPIO_NUM;\n  config.pin_d2 = Y4_GPIO_NUM;\n  config.pin_d3 = Y5_GPIO_NUM;\n  config.pin_d4 = Y6_GPIO_NUM;\n  config.pin_d5 = Y7_GPIO_NUM;\n  config.pin_d6 = Y8_GPIO_NUM;\n  config.pin_d7 = Y9_GPIO_NUM;\n  config.pin_xclk = XCLK_GPIO_NUM;\n  config.pin_pclk = PCLK_GPIO_NUM;\n  config.pin_vsync = VSYNC_GPIO_NUM;\n  config.pin_href = HREF_GPIO_NUM;\n  config.pin_sscb_sda = SIOD_GPIO_NUM;\n  config.pin_sscb_scl = SIOC_GPIO_NUM;\n  config.pin_pwdn = PWDN_GPIO_NUM;\n  config.pin_reset = RESET_GPIO_NUM;\n  config.xclk_freq_hz = 20000000;\n  config.frame_size = FRAMESIZE_UXGA;\n  config.pixel_format = PIXFORMAT_JPEG; \n  config.grab_mode = CAMERA_GRAB_WHEN_EMPTY;\n  config.fb_location = CAMERA_FB_IN_PSRAM;\n  config.jpeg_quality = 12;\n  config.fb_count = 1;\n\n  if (psramFound()) {\n    config.jpeg_quality = 10;\n    config.fb_count = 2;\n    config.grab_mode = CAMERA_GRAB_LATEST;\n  }\n\n  esp_err_t err = esp_camera_init(&amp;config);\n  if (err != ESP_OK) {\n    Serial.printf(\"Camera init failed with error 0x%x\", err);\n    return;\n  }\n\n  sensor_t *s = esp_camera_sensor_get();\n  s-&gt;set_vflip(s, 1); // Flip it back\n  s-&gt;set_brightness(s, 1); // Up the brightness just a bit\n  s-&gt;set_saturation(s, -2); // Lower the saturation\n\n#if defined(LED_GPIO_NUM)\n  setupLedFlash(LED_GPIO_NUM);\n#endif\n\n  WiFi.begin(ssid, password);\n  WiFi.setSleep(false);\n  while (WiFi.status() != WL_CONNECTED) {\n    delay(500);\n    Serial.print(\".\");\n  }\n\n  Serial.println(\"\");\n  Serial.println(\"WiFi connected\");\n  webSocket.begin();\n  webSocket.onEvent(onWebSocketEvent);\n  startCameraServer();\n\n  Serial.print(\"Camera Ready! Use 'http://\");\n  Serial.print(WiFi.localIP());\n  Serial.println(\"' to connect\");\n}\n\nvoid loop() {\n  webSocket.loop();\n}\n\nvoid onWebSocketEvent(uint8_t client_num, WStype_t type, uint8_t *payload, size_t length) {\n  switch (type) {\n    case WStype_DISCONNECTED:\n      Serial.printf(\"[%u] Disconnected!\\n\", client_num);\n      break;\n    case WStype_CONNECTED:\n      {\n        IPAddress ip = webSocket.remoteIP(client_num);\n        Serial.printf(\"[%u] Connection from \", client_num);\n        Serial.println(ip.toString());\n      }\n      break;\n    case WStype_TEXT:\n      if (strcmp((char *)payload, \"capture\") == 0) {\n        camera_fb_t *fb = esp_camera_fb_get();\n        if (!fb) {\n          Serial.println(\"Camera capture failed\");\n        } else {\n          webSocket.sendBIN(client_num, fb-&gt;buf, fb-&gt;len);\n          esp_camera_fb_return(fb);\n        }\n      }\n      break;\n    case WStype_BIN:\n      Serial.printf(\"[%u] Get binary length: %u\\n\", client_num, length);\n      break;\n  }\n}\n\nvoid setupLedFlash(int pin) {\n  pinMode(pin, OUTPUT);\n  digitalWrite(pin, LOW);\n}\n</code></pre> <p>This program connects the ESP32CAM to a local WiFi network. It then sets up and initializes the camera, and sets up the local IP connection. It then continuously waits for a web socket connection. When a connection is created, it prints the IP address of the connecting device. If the device sends an input of \"capture\", the camera will take a picture and send it via the network web socket connection to the connecting Raspberry Pi.</p>"},{"location":"conrad/#camera-feed-ocr","title":"Camera Feed OCR","text":"<p>I had previously setup infrastructure to wirelessly transmit a command to capture an image from a Raspberry Pi to the ESP32CAM, along with sending the image data back over the network and saving it. I had created a WebSocket server to accept commands and then send the image data over HTTP back to the Raspberry Pi.</p> <p>However, I realized that I could fetch the image without needing a WebSocket handler by connecting to the ESP32CAM's capture image handler directly. The capture handler from the default CameraWebServer example project sets up a port that allows a direct download to what is currently on the camera feed.</p> <pre><code>static esp_err_t capture_handler(httpd_req_t *req)\n{\n    camera_fb_t *fb = NULL;\n    esp_err_t res = ESP_OK;\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n    int64_t fr_start = esp_timer_get_time();\n#endif\n\n#if CONFIG_LED_ILLUMINATOR_ENABLED\n    enable_led(true);\n    vTaskDelay(150 / portTICK_PERIOD_MS); // The LED needs to be turned on ~150ms before the call to esp_camera_fb_get()\n    fb = esp_camera_fb_get();             // or it won't be visible in the frame. A better way to do this is needed.\n    enable_led(false);\n#else\n    fb = esp_camera_fb_get();\n#endif\n\n    if (!fb)\n    {\n        log_e(\"Camera capture failed\");\n        httpd_resp_send_500(req);\n        return ESP_FAIL;\n    }\n\n    httpd_resp_set_type(req, \"image/jpeg\");\n    httpd_resp_set_hdr(req, \"Content-Disposition\", \"inline; filename=capture.jpg\");\n    httpd_resp_set_hdr(req, \"Access-Control-Allow-Origin\", \"*\");\n\n    char ts[32];\n    snprintf(ts, 32, \"%lld.%06ld\", fb-&gt;timestamp.tv_sec, fb-&gt;timestamp.tv_usec);\n    httpd_resp_set_hdr(req, \"X-Timestamp\", (const char *)ts);\n\n#if CONFIG_ESP_FACE_DETECT_ENABLED\n    size_t out_len, out_width, out_height;\n    uint8_t *out_buf;\n    bool s;\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n    bool detected = false;\n#endif\n    int face_id = 0;\n    if (!detection_enabled || fb-&gt;width &gt; 400)\n    {\n#endif\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n        size_t fb_len = 0;\n#endif\n        if (fb-&gt;format == PIXFORMAT_JPEG)\n        {\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n            fb_len = fb-&gt;len;\n#endif\n            res = httpd_resp_send(req, (const char *)fb-&gt;buf, fb-&gt;len);\n        }\n        else\n        {\n            jpg_chunking_t jchunk = {req, 0};\n            res = frame2jpg_cb(fb, 80, jpg_encode_stream, &amp;jchunk) ? ESP_OK : ESP_FAIL;\n            httpd_resp_send_chunk(req, NULL, 0);\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n            fb_len = jchunk.len;\n#endif\n        }\n        esp_camera_fb_return(fb);\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n        int64_t fr_end = esp_timer_get_time();\n#endif\n        log_i(\"JPG: %uB %ums\", (uint32_t)(fb_len), (uint32_t)((fr_end - fr_start) / 1000));\n        return res;\n#if CONFIG_ESP_FACE_DETECT_ENABLED\n    }\n\n    jpg_chunking_t jchunk = {req, 0};\n\n    if (fb-&gt;format == PIXFORMAT_RGB565\n#if CONFIG_ESP_FACE_RECOGNITION_ENABLED\n     &amp;&amp; !recognition_enabled\n#endif\n     ){\n#if TWO_STAGE\n        HumanFaceDetectMSR01 s1(0.1F, 0.5F, 10, 0.2F);\n        HumanFaceDetectMNP01 s2(0.5F, 0.3F, 5);\n        std::list&lt;dl::detect::result_t&gt; &amp;candidates = s1.infer((uint16_t *)fb-&gt;buf, {(int)fb-&gt;height, (int)fb-&gt;width, 3});\n        std::list&lt;dl::detect::result_t&gt; &amp;results = s2.infer((uint16_t *)fb-&gt;buf, {(int)fb-&gt;height, (int)fb-&gt;width, 3}, candidates);\n#else\n        HumanFaceDetectMSR01 s1(0.3F, 0.5F, 10, 0.2F);\n        std::list&lt;dl::detect::result_t&gt; &amp;results = s1.infer((uint16_t *)fb-&gt;buf, {(int)fb-&gt;height, (int)fb-&gt;width, 3});\n#endif\n        if (results.size() &gt; 0) {\n            fb_data_t rfb;\n            rfb.width = fb-&gt;width;\n            rfb.height = fb-&gt;height;\n            rfb.data = fb-&gt;buf;\n            rfb.bytes_per_pixel = 2;\n            rfb.format = FB_RGB565;\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n            detected = true;\n#endif\n            draw_face_boxes(&amp;rfb, &amp;results, face_id);\n        }\n        s = fmt2jpg_cb(fb-&gt;buf, fb-&gt;len, fb-&gt;width, fb-&gt;height, PIXFORMAT_RGB565, 90, jpg_encode_stream, &amp;jchunk);\n        esp_camera_fb_return(fb);\n    } else\n    {\n        out_len = fb-&gt;width * fb-&gt;height * 3;\n        out_width = fb-&gt;width;\n        out_height = fb-&gt;height;\n        out_buf = (uint8_t*)malloc(out_len);\n        if (!out_buf) {\n            log_e(\"out_buf malloc failed\");\n            httpd_resp_send_500(req);\n            return ESP_FAIL;\n        }\n        s = fmt2rgb888(fb-&gt;buf, fb-&gt;len, fb-&gt;format, out_buf);\n        esp_camera_fb_return(fb);\n        if (!s) {\n            free(out_buf);\n            log_e(\"To rgb888 failed\");\n            httpd_resp_send_500(req);\n            return ESP_FAIL;\n        }\n\n        fb_data_t rfb;\n        rfb.width = out_width;\n        rfb.height = out_height;\n        rfb.data = out_buf;\n        rfb.bytes_per_pixel = 3;\n        rfb.format = FB_BGR888;\n\n#if TWO_STAGE\n        HumanFaceDetectMSR01 s1(0.1F, 0.5F, 10, 0.2F);\n        HumanFaceDetectMNP01 s2(0.5F, 0.3F, 5);\n        std::list&lt;dl::detect::result_t&gt; &amp;candidates = s1.infer((uint8_t *)out_buf, {(int)out_height, (int)out_width, 3});\n        std::list&lt;dl::detect::result_t&gt; &amp;results = s2.infer((uint8_t *)out_buf, {(int)out_height, (int)out_width, 3}, candidates);\n#else\n        HumanFaceDetectMSR01 s1(0.3F, 0.5F, 10, 0.2F);\n        std::list&lt;dl::detect::result_t&gt; &amp;results = s1.infer((uint8_t *)out_buf, {(int)out_height, (int)out_width, 3});\n#endif\n\n        if (results.size() &gt; 0) {\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n            detected = true;\n#endif\n#if CONFIG_ESP_FACE_RECOGNITION_ENABLED\n            if (recognition_enabled) {\n                face_id = run_face_recognition(&amp;rfb, &amp;results);\n            }\n#endif\n            draw_face_boxes(&amp;rfb, &amp;results, face_id);\n        }\n\n        s = fmt2jpg_cb(out_buf, out_len, out_width, out_height, PIXFORMAT_RGB888, 90, jpg_encode_stream, &amp;jchunk);\n        free(out_buf);\n    }\n\n    if (!s) {\n        log_e(\"JPEG compression failed\");\n        httpd_resp_send_500(req);\n        return ESP_FAIL;\n    }\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n    int64_t fr_end = esp_timer_get_time();\n#endif\n    log_i(\"FACE: %uB %ums %s%d\", (uint32_t)(jchunk.len), (uint32_t)((fr_end - fr_start) / 1000), detected ? \"DETECTED \" : \"\", face_id);\n    return res;\n#endif\n}\n</code></pre> <p>I then set up the Raspberry Pi to receive an image from the ESP32CAM and perform OCR upon it.</p> <p>First, I created a directory to store this project.</p> <pre><code>cd Desktop\nmkdir ocr\n</code></pre> <p>Upon entering the new directory, I need to create a virtual environment to install the libraries I will be using for OCR.</p> <pre><code>python -m venv /virtual\n</code></pre> <p>However, running this command gave me an error. </p> <pre><code>Error: [Errno13] Permission denied: '/virtual'\n</code></pre> <p>For some reason, this command didn't have the permissions to create a new virtual environment, which was strange considering that the project directory was not protected in any way. Regardless, I attached the sudo prefix and successfully created the virtual environment.</p> <pre><code>sudo python -m venv /virtual\n</code></pre> <p>I then entered the virtual environment by activating it.</p> <pre><code>source bin/activate\n</code></pre> <p>The bin/activate is a relative path and would activate the venv as long as I am in the \"ocr\" folder. However, the venv could also be activated by supplying the absolute path of \"~/home/richard/Desktop/ocr/bin/activate\".</p>"},{"location":"conrad/#pytesseract","title":"PyTesseract","text":"<p>After activating the virtual environment, I can install all of my library dependencies.</p> <pre><code>sudo pip install pytesseract\nsudo pip install opencv-python\n</code></pre> <p>I then created the actual program that the Raspberry Pi would run.</p> <pre><code>import time\nimport cv2\nimport urllib.request\nimport numpy as np\nimport pytesseract\n\nurl = 'http://10.12.28.193/capture'\n\nimg_resp = urllib.request.urlopen(url)\nimgnp = np.array(bytearray(img_resp.read()), dtype=np.uint8)\nframe = cv2.imdecode(imgnp, -1)\n\ntext = pytesseract.image_to_string(frame, config='--psm 7')\n\nprint(\"Extracted Text:\", text)\ntime.sleep(1)\n</code></pre> <p>This script has the Raspberry Pi connect to the /capture handler of the ESP32CAM interface, which directly returns a capture of the current feed. It then decodes the image and parses it into the pytesseract OCR function. The PSM value of 6 tells the OCR model to scan the image for a single text block and extract text from that. A full list of PSM value options can be found by running <code>tesseract --help-psm</code> in the terminal.</p> <pre><code>  0    Orientation and script detection (OSD) only.\n  1    Automatic page segmentation with OSD.\n  2    Automatic page segmentation, but no OSD, or OCR. (not implemented)\n  3    Fully automatic page segmentation, but no OSD. (Default)\n  4    Assume a single column of text of variable sizes.\n  5    Assume a single uniform block of vertically aligned text.\n  6    Assume a single uniform block of text.\n  7    Treat the image as a single text line.\n  8    Treat the image as a single word.\n  9    Treat the image as a single word in a circle.\n 10    Treat the image as a single character.\n 11    Sparse text. Find as much text as possible in no particular order.\n 12    Sparse text with OSD.\n 13    Raw line. Treat the image as a single text line, bypassing hacks that are Tesseract-specific.\n</code></pre> <p>In my case, since I want the model to scan an image to find the line of text for \"Hello World!\", I will use psm-7.</p> <p>Here is a photo of my Raspberry Pi setup.</p> <p> </p> <p>The ESP32CAM is pointed towards a paper with the words \"Hello World!\". In the right side of the picture, the Raspberry Pi which is running the code is visible along with the display. Upon running the program on the Pi's terminal, the ESP32CAM takes a picture and transmits it to the Pi, which then uses tesseract to perform OCR on it and prints out the extracted text.</p> <p> <p></p>"},{"location":"conrad/#gpt4o","title":"GPT4o","text":"<p>At this point, I wanted to try to use as little computational power as possible, and thus decided to switch to processing my image in base64. Although switching to base64 ultimately failed to scale down the computing enough to run on a microcontroller, it still led me in an interesting direction: that I could use GPT4o's new multimodal capabilities as an OCR engine to extract text from the base64 image. GPT4o in general is much more accurate in OCR than pytesseract, hence the switch.</p> <p>To do this, I first created a new handler on the ESP32CAM that would have it return a base64 string of a capture of the camera feed when that handler is called.</p> <pre><code>static esp_err_t jpg_base64_handler(httpd_req_t *req) {\n    camera_fb_t *fb = esp_camera_fb_get();\n    if (!fb) {\n        Serial.println(\"Camera capture failed\");\n        httpd_resp_send_500(req);\n        return ESP_FAIL;\n    }\n\n    // Encode the frame in base64\n    String base64Image = base64::encode(fb-&gt;buf, fb-&gt;len);\n\n    // Send the base64 encoded image\n    httpd_resp_set_type(req, \"text/plain\");\n    esp_err_t res = httpd_resp_send(req, base64Image.c_str(), base64Image.length());\n\n    // Return the frame buffer\n    esp_camera_fb_return(fb);\n\n    return res;\n}\n\n[...]\n\nhttpd_uri_t base64_uri = {\n        .uri = \"/base64\",\n        .method = HTTP_GET,\n        .handler = jpg_base64_handler,\n        .user_ctx = NULL\n\n[...]\n\nhttpd_register_uri_handler(camera_httpd, &amp;base64_uri);\n</code></pre> <p>A base64 string containing the data of a single frame captured by the ESP32CAM looks something like this:</p> <pre><code>\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 \n</code></pre> <p>With some modifications to OpenAI's sample code and the help of GPT-4o itself, I created the following script that would grab the base64 data from the custom handler and parse it into the model. The prompt for the model is \"You are a image to text OCR engine. Output the text you see in this image, and nothing else.\"</p> <pre><code>import requests\n\n# OpenAI API Key\napi_key = \"REDACTED\"\n\n# Function to get the base64 encoded image from ESP32CAM\ndef get_base64_image(url):\n    response = requests.get(url)\n    return response.text\n\n# URL to the ESP32CAM base64 image endpoint\nesp32cam_url = \"http://10.12.28.193/base64\"\n\n# Getting the base64 string\nbase64_image = get_base64_image(esp32cam_url)\n\nheaders = {\n  \"Content-Type\": \"application/json\",\n  \"Authorization\": f\"Bearer {api_key}\"\n}\n\npayload = {\n  \"model\": \"gpt-4o\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"You are a image to text OCR engine. Output the text you see in this image, and nothing else.\"\n        },\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\n            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n          }\n        }\n      ]\n    }\n  ],\n  \"max_tokens\": 300\n}\n\nresponse = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n\nprint(response.json())\n</code></pre> <p>With the same setup as the Raspberry Pi, where the ESP32CAM is pointed towards a paper with the words \"Hello World!\", I ran the script on my computer to test it out.</p> <p>Upon running, the following is printed out:</p> <p> </p> <p>As is seen in the output, the prompt worked and the 4o model detected \"Hello World!\" as the text extract and stored it in 'content'. To just output the content, which is the actual result I want, I can modify the code a little.</p> <pre><code>response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n\ncontent_string = response.json()['choices'][0]['message']['content']\nprint(content_string)\n</code></pre> <p>Here is the code just printing out the extracted OCR text.</p> <p> </p> <p>This was essentially the bare-bones version of my final project Pi code. After integrating the serial communication functionality, and changing the wording of the prompt, I had the final code for the Raspberry Pi ready:</p> <pre><code>from serial import Serial\nimport time\nimport requests\n\n# OpenAI API Key\napi_key = \"REDACTED\"\n\n# Configure the serial port\nser = Serial('/dev/serial0', 9600, timeout = 1)\n\n# Function to get the base64 encoded image from ESP32CAM\ndef get_base64_image(url):\n    response = requests.get(url)\n    print(\"Camera connected\")\n    return response.text\n\n# URL to the ESP32CAM base64 image endpoint\nesp32cam_url = \"http://10.12.23.1/base64\"\n\n# Getting the base64 string\nbase64_image = get_base64_image(esp32cam_url)\n\n# Function to send a message\ndef send_message(message):\n    ser.write(message.encode())  # Convert the message to bytes and send\n    time.sleep(1)                # Wait for a second\n\n# Example message used for testing purposes\n# testMessage = \"Hello world\"\n\ndef ocr():\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {api_key}\"\n    }\n\n    # Define the information sent to GPT4o\n    payload = {\n        \"model\": \"gpt-4o\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"You are a image to text OCR engine. Output the text you see in this image, and nothing else. Only use lowercase letters and no punctuation.\"\n                    },\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n                        }\n                    }\n                ]\n            }\n        ],\n        \"max_tokens\": 300\n    }\n\n    # Query the engine\n    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n\n    # Extract its response\n    return response.json()['choices'][0]['message']['content']\n\ntry:\n    message = ocr()\n    send_message(message)\n    print(f\"Message sent: {message}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\nfinally:\n    ser.close()  # Close the serial port\n</code></pre>"},{"location":"conrad/#text-to-braille-mapping","title":"Text to Braille Mapping","text":"<p>The Raspberry Pi sends a byte-encoded text string to the ATTiny1614. From there, the ATTiny1614 is responsible for interpreting and converting the received text into braille dot arrays, which it then shows on the 3x2 array.</p> <p>The following program first sets up the configuration of solenodis and corresponding GPIOs, then creates the arrays of 0s and 1s (solenoid up or down) that forms a single braille character. It then maps each letter to its corresponding letter braille array.</p> <p>Upon the script's setup, each relevant pin is configured to be OUTPUT, and a serial connection with the Raspberry Pi is initialized. </p> <p>The script then continuously waits for an incoming string sent through Serial. Upon receiving the text, it is converted into an array of characters. The parse_input_string() method then does the heavy lifting and fetches the correct braille dot arrays. The method then iterates through each letter and represents that character's corresponding braille for one second on the physical solenoid array.</p> <p>The activate_solenoids() and deactivate_solenoids() methods were used for debugging purposes, turning all solenoids on or off, respectively.</p> <pre><code>/*\n  Solenoid arrangement:\n  0 1\n  2 3\n  4 5\n*/\n\nint sols[6] = {0, 1, 2, 3, 9, 8}; // Define the pins connected to the solenoids\n\n// Define the Braille arrays\nint a[6] = {0, 1, 1, 1, 1, 1};\nint b[6] = {0, 1, 0, 1, 1, 1};\nint c[6] = {0, 0, 1, 1, 1, 1};\nint d[6] = {0, 0, 1, 0, 1, 1};\nint e[6] = {0, 1, 1, 0, 1, 1};\nint f[6] = {0, 0, 0, 1, 1, 1};\nint g[6] = {0, 0, 0, 0, 1, 1};\nint h[6] = {0, 1, 0, 0, 1, 1};\nint i[6] = {1, 0, 0, 1, 1, 1};\nint j[6] = {1, 0, 0, 0, 1, 1};\nint k[6] = {0, 1, 1, 1, 0, 1};\nint l[6] = {0, 1, 0, 1, 0, 1};\nint m[6] = {0, 0, 1, 1, 0, 1};\nint n[6] = {0, 0, 1, 0, 0, 1};\nint o[6] = {0, 1, 1, 0, 0, 1};\nint p[6] = {0, 0, 0, 1, 0, 1};\nint q[6] = {0, 0, 0, 0, 0, 1};\nint r[6] = {0, 1, 0, 0, 0, 1};\nint s[6] = {1, 0, 0, 1, 0, 1};\nint t[6] = {1, 0, 0, 0, 0, 1};\nint u[6] = {0, 1, 1, 1, 0, 0};\nint v[6] = {0, 1, 0, 1, 0, 0};\nint w[6] = {1, 0, 0, 0, 1, 0};\nint x[6] = {0, 0, 1, 1, 0, 0};\nint y[6] = {0, 0, 1, 0, 0, 0};\nint z[6] = {0, 1, 1, 0, 0, 0};\n\nint space[6] = {1, 1, 1, 1, 1, 1};\n\nint number_sign[6] = {1, 0, 1, 0, 0, 0};\nint num_1[6] = {0, 1, 1, 1, 1, 1}; // Same as a\nint num_2[6] = {0, 1, 0, 1, 1, 1}; // Same as b\nint num_3[6] = {0, 0, 1, 1, 1, 1}; // Same as c\nint num_4[6] = {0, 0, 1, 0, 1, 1}; // Same as d\nint num_5[6] = {0, 1, 1, 0, 1, 1}; // Same as e\nint num_6[6] = {0, 0, 0, 1, 1, 1}; // Same as f\nint num_7[6] = {0, 0, 0, 0, 1, 1}; // Same as g\nint num_8[6] = {0, 1, 0, 0, 1, 1}; // Same as h\nint num_9[6] = {1, 0, 0, 1, 1, 1}; // Same as i\nint num_0[6] = {1, 0, 0, 0, 1, 1}; // Same as j\n\n// Define a structure to map characters to their Braille arrays\ntypedef struct {\n    char character;\n    int *braille_array;\n} BrailleMap;\n\n// Create the mapping\nBrailleMap braille_dictionary[] = {\n    {'a', a}, {'b', b}, {'c', c}, {'d', d}, {'e', e},\n    {'f', f}, {'g', g}, {'h', h}, {'i', i}, {'j', j},\n    {'k', k}, {'l', l}, {'m', m}, {'n', n}, {'o', o},\n    {'p', p}, {'q', q}, {'r', r}, {'s', s}, {'t', t},\n    {'u', u}, {'v', v}, {'w', w}, {'x', x}, {'y', y}, {'z', z}, {' ', space},\n    {'#', number_sign},\n    {'1', num_1}, {'2', num_2}, {'3', num_3}, {'4', num_4}, {'5', num_5},\n    {'6', num_6}, {'7', num_7}, {'8', num_8}, {'9', num_9}, {'0', num_0}\n};\n\nvoid setup() {\n    // Initialize the solenoid pins as output\n    for (int i = 0; i &lt; 6; i++) {\n        pinMode(sols[i], OUTPUT);\n    }\n  //  PORTMUX.CTRLB = 0x01;\n   Serial.begin(9600);\n\n   while(!Serial){\n\n   }\n}\n\nvoid activate_solenoids(int *braille_array) {\n    for (int i = 0; i &lt; 6; i++) {\n        digitalWrite(sols[i], braille_array[i]);\n    }\n}\n\n// Function to parse the input string and activate solenoids\nvoid parse_input_string(const char *input) {\n    int len = sizeof(braille_dictionary) / sizeof(BrailleMap);\n    for (int i = 0; i &lt; strlen(input); i++) {\n        for (int j = 0; j &lt; len; j++) {\n            if (braille_dictionary[j].character == input[i]) {\n                activate_solenoids(braille_dictionary[j].braille_array);\n                delay(1000); // Wait for a second before next character\n                break;\n            }\n        }\n    }\n    deactivate_solenoids();\n}\n\nvoid deactivate_solenoids() {\n    for (int i = 0; i &lt; 6; i++) {\n        digitalWrite(sols[i], LOW);\n    }\n}\n\nvoid loop() {\n  if (Serial.available() &gt; 0) {\n    // Read the incoming string\n\n    String input = Serial.readString();\n\n    // Convert the String to a C-style string (char array)\n    char inputArray[input.length() + 1];\n    input.toCharArray(inputArray, input.length() + 1);\n\n    // Call the parse_input_string function with the received string\n    parse_input_string(inputArray);\n\n    // Optional: Add a delay to avoid flooding the input\n    delay(5000); // Wait for 5 seconds before repeating\n  }\n}\n</code></pre>"},{"location":"conrad/#assembly","title":"Assembly","text":"<p>I first outlined the general setup of my final project. I secured each MOSFET to a corresponding battery pack and solenoid, and color-coded each MOSFET's trigger and GND wires. I organized them in such a way that toggling solenoid 1, 2, 3, 4, 5, then 6 would control each solenoid in a line.</p> <p> </p> <p>I then connected the color coded wire sets to the controller PCB, and tested turning all the solenoids on in the right order.</p> <p> <p></p> <p>Lastly, I secured everything in place and put the cover on. This involved using Nitto tape to attach the PCB to the side wall, and using hot glue to secure the MOSFETs to the bottom and the solenoids to their respective places. The PCB on the side wall is circled in red in the image below, as it is somewhat difficult to see. Each MOSFET drive module is connected to the controller PCB through a set of respective color coded wire pairs. The top-left solenoid corresponds with both white wires, the top-right solenoid with dark blue, the mid-left solenoid with green, the mid-right solenoid with orange, the bottom-left solenoid with blue, and the bottom right solenoid with yellow. The color coded scheme makes it easier for me to detach individual wires for debugging and knowing which solenoid a specific wire corresponds to.</p> <p> </p> <p>I then ran another test to ensure everything still worked.</p> <p> <p></p> <p>The assembly of the Raspberry Pi case is relatively simple. As the 3D print already has holes built in for USB wires, and a hole built in for the screen, all I really need to do is secure the Pi to the bottom and the screen to the top, then connect the wires. I used 4 M3 screws in the screw-holes that I designed to secure the Raspberry Pi screen to the top of the case. I then used hot glue to secure the Pi in place on the bottom of the case. The following image shows the case's top on the left, with the screwed-in screen connected to the Pi via the microHDMI port for data and the power cable. The Pi itself is shown on the right, with its power cable coming out of the left side box hole, and the ESP32CAM cable coming out of the bottom hole. The wires on the right side case hole are used for connecting the Pi's TX/RX, VCC, and GND with the BrailleBox's ATTiny1614 PCB.</p> <p> </p> <p>Here is the Pi case when closed.</p> <p> </p>"},{"location":"conrad/#evaluation","title":"Evaluation","text":"<p>My project is considered successful if it can:</p> <ul> <li>\u2611 Accurately extract text from a live image feed</li> <li>\u2611 Map the text to braille</li> <li>\u2611 Display the braille on the solenoid array</li> </ul>"},{"location":"conrad/#implications","title":"Implications","text":"<p>There is existing technologies on the market that can convert text to braille in real time, but those are often expensive and not readily available to the public. My hope with this project is to create a product that can be cheaply produced and reach a wide audience. </p>"},{"location":"conrad/#lessons-learned","title":"Lessons Learned","text":"<ul> <li>Some parts of a project will take longer while others will take shorter than expected.</li> <li>Double the planned allocation of time due to errors and debugging</li> <li>Working with lower-level hardware and software is more rewarding and often produces a more solid product</li> <li>There are many types of transistors, which can be a pain to sort through</li> </ul>"},{"location":"conrad/#final-product","title":"Final Product","text":""},{"location":"conrad/#poster","title":"Poster","text":""},{"location":"conrad/#video","title":"Video","text":"<p> <p></p>"},{"location":"conrad/#file-downloads","title":"File Downloads","text":"<p>My files can be downloaded here.</p>"},{"location":"edm2/","title":"Engineering Projects","text":""},{"location":"edm2/#automated-ouija-board","title":"Automated Ouija Board","text":""},{"location":"edm2/#engineering-i-and-ii-documentation","title":"Engineering I and II Documentation","text":""},{"location":"edm2/#deep-dive-into-fusion-360-pt-1","title":"Deep Dive into Fusion 360 Pt. 1","text":""},{"location":"edm2/#deep-dive-into-fusion-360-pt-2","title":"Deep Dive into Fusion 360 Pt. 2","text":""},{"location":"edm2/#milling-about","title":"Milling About","text":""},{"location":"edm2/#pressing-charges","title":"Pressing Charges","text":""},{"location":"edm2/#cutting-board","title":"Cutting Board","text":""},{"location":"coding/macro/","title":"Macro Maker","text":""},{"location":"coding/macro/#description","title":"Description","text":"<p>The Macro project was my final project for Java Data Structures (H) in 10th grade, of which the requirements can be found here. The final product allows the user to start and end a global recording of all inputs to the computer. The global recording will inputs including mouse movements; mouse dragging; mouse clicking, releasing, and holding; key presses, releases, and hold times; and any simultaneous combination of events. The actions are also logged in the logger.out file. After the user begins recording, (CTRL + SHIFT+ R), all henceforth input actions are recorded until the user stops recording (CTRL + SHIFT + S). Upon replaying, the program will execute the actions exactly as recorded. The press() methods also delays the action's execution as to match the delays between actions of the original recording. The program does not interfere with input actions or other applications in any way.</p>"},{"location":"coding/macro/#inspiration","title":"Inspiration","text":"<p>I've done a lot of scripting before, both static and dynamic through OCR. My goal with this project was to make a program that could record your actions and replay them with no need for hard coding or manual key press/mouse coordinate tracking. In the future, I plan to make make the project's logging function more useful, where the logging of actions into a seperate .txt document can function as a saved user macro sequence. Eventually I want to add a feature where the program is able to ingest a log file and replay the actions stored there, allowing users to save a specific macro multiple times past the closure of the JFrame.</p>"},{"location":"coding/macro/#method","title":"Method","text":"<p>The main library that enabled the creation of this project is JNativeHook, which allows the global tracking of all actions that are sent into a computer, as opposed to only being able to detect inputs inside a specific window. In this project, JNativeHook's GlobalScreen and adapters were used, but I have overriden the action listeners and adapters with my own code to suit the needs of this project, specifically storage and replaying of inputs. </p> <p>When the program is running and recording is true (the user has started recording), every input action triggers a specific handler method which is able to encapsulate the event as an object to store in the recordedActions stack. During recording, a global variable of time is used to keep track of delays between actions. Every time a new action is recorded, the variable is then updated to the time in milliseconds when the action was performed. When the next action is then recorded, subtracting the last recorded time from the current system time gives us the delay, which is stored as part of the action object. Then, the time variable is updated. Thus, we are able to track the delays between actions in order to replay them at the right times. </p> <p>The different action classes (ClickPress, ClickRelease, KeyPress, KeyRelease, MouseMove) allow the program to store every input as an individual object. All these classes implement the Action interface, giving them all a press() method which is called when replaying. Here is an example of the handler method creating a new object:</p> <pre><code>   @Override\n    public void nativeKeyPressed(NativeKeyEvent e) {    // This method is called when a key is pressed; other methods handle other events.\n        recordedActions.add(new KeyPress(e, Math.abs(System.currentTimeMillis()-time)));    // Creates a new KeyPress object in the stack, parsing in the NativeKeyEvent generated by JNativeHook/GlobalScreen and the time since last recorded action\n        time = System.currentTimeMillis();  // Updates variable time\n    } \n\n\n    // For context, the constructor for KeyPress is: \n    public KeyPress(NativeKeyEvent ke, long delay) {...}\n</code></pre> <p> The other adapter and conversion classes (SwingKeyAdapter, MouseCoordinateConverter, KeyLocationLookup, KeyCursorLookup) allow for conversion between different keycodes and screen sizes. The SwingKeyAdapter, KeyLocationLookup, and KeyCursorLookup classes convert VC keycodes used by NativeKeyEvent into VK keycodes used by KeyEvent. The MouseCoordinateConverter class allows for conversion between physical mouse cursor coordinate positions and scaled mouse cursor coordinate positions, allowing MouseMove to move the cursor to the correct point for all screen sizes on different computers. For example, the KeyPress class uses SwingKeyAdapter to translate a NativeKeyEvent keycode integer value to a KeyEvent keycode, which is what the robot from java.awt.Robot can interpret and use. </p> <pre><code>   @Override\n    public void press() {\n        robot.delay((int) delay); // Waits the millisecond amount as parsed from the constructor\n        robot.keyPress(ska.getJavaKeyEvent(e).getKeyCode()); // Uses SwingKeyAdapter to convert keycode values\n    }</code></pre> <p> During recording, all input actions are created as new objects of their respective class and are stored in a stack until the user stops. Upon replaying the stored actions, the program will iterate through the stack and call each object's press() method to execute the action exactly as performed. </p> <pre><code>   for(Action i : recordedActions) {\n        i.press(); /* The implementation of the Action inferface by all the input action classes \n                      allows me to call a generic .press() function */\n    }\n</code></pre> <p>  For more information, the full project can be found open-sourced on the Github repo. The downloadable jar file is not obfuscated and can be decompiled. </p> <p> </p>"},{"location":"coding/macro/#download","title":"Download","text":"<p>"},{"location":"coding/macro/#the-github-repo-can-be-found-here","title":"The github repo can be found here.","text":""},{"location":"coding/macro/#the-jar-executable-can-be-downloaded-here","title":"The jar executable can be downloaded here.","text":""},{"location":"coding/macro/#instructions-for-use","title":"Instructions for Use","text":"<p>Start Recording Keystrokes: CTRL + SHIFT + R</p> <p>Stop Recording Keystrokes and Save: CTRL + SHIFT + S</p> <p>Replay Saved Keystroke Sequence: CTRL + SHIFT + 1</p> <p> </p>"},{"location":"coding/macro/#initial-planning-chart","title":"Initial Planning Chart","text":"<p> <p> </p>"},{"location":"coding/macro/#key-elements","title":"Key Elements","text":"<ul> <li>Action interface implemented by all the action/event classes: ClickPress, ClickRelease, KeyPress, KeyRelease, MouseMove, etc.<ul> <li>Every action class follows the command archetype, allowing the encapsulation of a request as an object.</li> </ul> </li> <li>KeyLocationLookup <ul> <li>Singleton - single instance of the lookup object that is referenced every time a key conversion is needed</li> <li>Hashtable inside KeyLocationLookup to store NativeKeyEvent KeyLocations and corresponding KeyEvent integer values for conversion</li> </ul> </li> <li>Mouse and Keyboard Adapters to listen for actions within the global screen. Those adapters also function as observers for the GlobalScreen.</li> <li>Stack to store the recorded actions</li> <li>Custom MouseCoordinatesConverter component that will convert true x, y mouse coordinates to relative x, y mouse coordinates scaled to each computer's unique screen size. Also functions as an adapter.</li> <li>State machine under GUILogger handling action transitions and entries when a key/mouse is pressed</li> <li>TreeMap under KeyCursorLookup to store NativeKeyEvent VC values and corresponding KeyEvent VK values</li> </ul>"},{"location":"edm2/lessons/cuttingBoard/","title":"Cutting Board","text":"<p>For this project, I was assigned to create a custom cutting board through making indentations and holes with the Shopbot CNC machine and filling them in with resin.</p>"},{"location":"edm2/lessons/cuttingBoard/#jig-design","title":"Jig Design","text":"<p>Normally, when CNCing a piece of wood, I would simply attach it to the machine bed using brad nails or something similar. However, because the actual cutting board wood itself is being cut on, pushing nails through it wouldn't work out. As such, I needed to design a holder for the cutting board that ensures it remains stable during the CNC job. The holder can be nailed down to the bed.</p> <p>Mrs Morrow provided me with a DXF design of the jig that I was to cut.</p> <p> </p> <p>From there, I imported it into Aspire and began creating the toolpaths. Specifically, the internal rectangle would use the \"Inside\" cutting setting, as I needed to preserve the exact dimensions of the rectangular hole to fit the cutting board wood into. The outside edge and the six inlets all used the \"Outside\" cutting setting.</p> <p> </p> <p>However, as visible above, the inlets are not cutting as expected and are wider than they are supposed to be, since the tool we were using is too thick for doing a back and forth. We solved this issue by going back in the original DXF file and instead of creating inlet outlines, we created a single line which would represent the actual toolpath, and created a new toolpath on \"Center\" along that line.</p> <p>I then ran a test air cut for the jig.</p> <p> <p></p> <p>After verifying that everything seemed in order, I ran the cut.</p> <p> </p> <p>I then de-bradded the larger wood piece and took off the jig. The internal rectangle cutout has retained the correct dimensions to fit in a 13 inch by 11 inch cutting board, and each of the outer inlets are the correct thickness.</p> <p> </p>"},{"location":"edm2/lessons/cuttingBoard/#board-design","title":"Board Design","text":"<p>Once the jig was ready, I began to design my actual cutting board. I had previously created a 2D design in Cuttle, which I liked and decided to use for this project.</p> <p>I first designed a pentagon by using the Polygon tool.</p> <p> </p> <p>I then created a rectangle which I duplicated on all 5 edges of the pentagon using the Rotational Repeat tool.</p> <p> </p> <p>I then created a group with the Rotational Repeat attribute. This means that anything I design under this group would automatically be repeated across the pentagon. I then added a bezier curve and some circles.</p> <p> <p></p> <p>After doing some additional modifications, this was my final design:</p> <p> </p> <p>I then imported the design into Aspire and made some minor changes. I also set the canvas size to 13 by 11 inches, which was the size of the cutting board wood that I would work with. I then scaled up the design and removed the outer cutout and tabs. Lastly, I added a rectangle cutout on the left-hand side of the board to act as a handle on the finished cutting board.</p> <p> </p> <p>I then started the CAM process. </p> <p>The grip/handle was the only hole cutout in the design. I set its cutting depth equivalent to the height of the actual wood piece (0.82 inches) and used the inside cutting setting. Additionally, I added tabs to the cutout to prevent the removed internal wood from flying out when in contact with the spinning spindle. This toolpath uses a 1/4\" bit.</p> <p> </p> <p>For all other details, I wanted to engrave them onto the board and thus set the cutting depth to 0.25 inches, and also used the inside cutting setting. As the circles in my design vary quite significantly in size, I decided to take advantage of the tool changer that the Shopbot is equipped with, and I created each individual toolpath with a different size bit.</p> <p>I created the \"small\" toolpath, which cuts the outermost and innermost five small circles, along with cutting the outlines for the 5 leaves. This toolpath used the 1/8\" bit.</p> <p> </p> <p>The \"big\" toolpath is essentially responsible for everything that isn't too small for the 3/8\" bit. This toolpath cuts the first 3 circles on each 4-circle array, and cuts the first circle on each 3-circle array.</p> <p> </p> <p>The \"small2\" toolpath cuts the remaining circles with a 1/4\" bit.</p> <p> </p> <p>After previewing my entire design, I realized that there were small stubs in the middle of each large circle, as the 3/8\" bit size was smaller than the diameter of the circle. </p> <p> </p> <p>To solve this issue, I created a smaller concentric circle within each large circle that needed a second cut. I then created a \"correctional\" toolpath on these internal circles to remove the extra material. This toolpath uses the 3/8\" bit, as it essentially is just another internal pass after the first cut doesn't remove all the material in the large circle.</p> <p> </p> <p>Here is the completed preview of my board cut.</p> <p> </p>"},{"location":"edm2/lessons/cuttingBoard/#board-cutting","title":"Board Cutting","text":"<p>After running an air cut and verifying that everything looked good, I secured my cutting board wood to the previously made jig.</p> <p> </p> <p>I then ran the cut. Here is a timelapse of a short portion of the cut.</p> <p> <p></p> <p>Because I used multiple bits of different sizes, the Shopbot had to change the active bit during the cut. Here is a timelapse of the machine changing bits during the job.</p> <p> <p></p> <p>After I let the cut run through, this is what my board looked like.</p> <p> </p>"},{"location":"edm2/lessons/cuttingBoard/#resin","title":"Resin","text":"<p>Now that the board itself is ready, I need to apply resin to all of the indentations that I created. I first poured resin over the entire board to ensure that it got into every cut.</p> <p> </p> <p>Once the resin had solidified, I planed the top of the board remove the excess resin and sanded it to give the board a smooth finish.</p> <p> </p> <p>Finally, I applied oil to all sides of the board to give the board a richer color and prepare it for use.</p> <p> </p>"},{"location":"edm2/lessons/fusionpart1/","title":"Deep Dive into Fusion 360 (Part 1)","text":""},{"location":"edm2/lessons/fusionpart1/#self-evaluation-beforehand","title":"Self-Evaluation Beforehand","text":"<p>Before starting this project, I already had some prior experience using Fusion360. However, in the pas I always struggled with modifying geometry for a specific command and navigating different sketches. I was able to overcome this through practicing using the bar at the bottom. </p>"},{"location":"edm2/lessons/fusionpart1/#storage-box","title":"Storage Box","text":"<p>Before starting working with Fusion, I was required to make a storage box. I decided to reuse a box that I already had from making it during Fab Academy. My documentation for making the box can be found here, containing a description on how I made the box.</p>"},{"location":"edm2/lessons/fusionpart1/#navigating-the-fusion-360-user-interface","title":"Navigating the Fusion 360 User Interface","text":"<p> <p>  This is the application bar. This bar contains the data panel, file menu, save button, and undo/redo buttons. This is also where all open projects are displayed in a tabs format.</p> <p></p> <p>  This is the data panel. The data panel functions as the file explorer for Fusion files and directories, along with managing sharing settings.</p> <p></p> <p>  These are the Profile and Help settings. This area contains notifications, statuses, and the profile button which allows modification of preferences and profile settings.</p> <p></p> <p>  This is the toolbar. The toolbar contains all of the tools/actions. The tools in the toolbar are organized by category: solid, surface, mesh, sheet metal, plastic, and utilities. The categories can be selected at the top bar, and the tools themselves are in the main rectangular panel.</p> <p></p> <p>  This is the browser. The browser allows easy access and viewing of all objects within the design. The browser allows the user to change the visibility of objects and change file units. The browser is structured like a file explorer.</p> <p></p> <p>  This is the view cube. The view cube allows orbiting around the design, and snapping to a specific viewing angle. The home button will orbit to the default home viewing position.</p> <p></p> <p>  This is the canvas. The canvas is where all the designing takes place. Right clicking in the canvas will access the marking menu/right-click menu which allows quick access of specific tools and commands.</p> <p></p> <p>  These are the navigation bar and display settings. The navigation bar is used to move around the design more accurately than the view cube. The display settings control cosmetic appearances.</p> <p></p> <p>  This is the view of the canvas from multiple perspectives at once, through use of the Viewports -&gt; Multiple Views setting.</p> <p></p> <p></p>"},{"location":"edm2/lessons/fusionpart1/#interface-parts-list","title":"Interface Parts List","text":""},{"location":"edm2/lessons/fusionpart1/#pro-tricks","title":"Pro Tricks","text":"<ol> <li>Utilize sketch constraints to secure specific geometry.  </li> <li>Use the timeline to modify sketches that have other geometry defined on top of them.  </li> </ol>"},{"location":"edm2/lessons/fusionpart1/#paperclip","title":"Paperclip","text":"<p>My file for the paperclip design in Fusion360 can be downloaded here. The paperclip's design mainly used constraints and the sweep command. I learned how constraints can be used to ensure that geometry remains strictly vertical/horizontal/tangent and how the sweep tool uses a sweep path and selects an object to be swept.</p> <p> <p></p> <p></p>"},{"location":"edm2/lessons/fusionpart1/#glass-bottle","title":"Glass Bottle","text":"<p>My file for the glass bottle design in Fusion360 can be downloaded here. The glass bottle's design process mainly focused on inserting a reference image and using the revolve command. I learned how to insert an image to help as I designed and how to manually adjust objects by a set distance.  <p></p> <p></p>"},{"location":"edm2/lessons/fusionpart1/#problems-encountered","title":"Problems Encountered","text":"<p>The main problem I encountered while starting my exploration of Fusion 360 was a lack of up-to-date tutorials. The video series that I followed, Learn Fusion 360 in 30 Days for Complete Beginners, was made a year ago. As such, some of the tools and defaults were different from last year and now. Different placements of tools was not a large issue as I could simply navigate or search to find the correct tool. However, some commands had different effects on the model which I had to manually fix. For example, when inserting a reference image as a canvas, it would create a new origin point which was not the case in the videos. As such, I had to manually move the entire design up by 100mm in order to have it all start on the origin's z-plane.</p>"},{"location":"edm2/lessons/fusionpart2/","title":"Deep Dive into Fusion 360 (Part 2)","text":""},{"location":"edm2/lessons/fusionpart2/#flattened-cone-warmup","title":"Flattened Cone Warmup","text":"<p>As a warmup before starting class one day, I designed a flattened cone to the following specifications:</p> <p> </p> <p>I first made a sketch on the ZY-plane in Fusion360, where I created a quadrilateral to match the one shown in the requirements image.</p> <p> </p> <p>I then used the Revolve tool to revolve the entire quadrilateral sketch 360 degrees around the Z axis to create a full flattened cone:</p> <p> </p> <p>Finally, since the warmup asked me for a 2D drawing, I generated a 2D drawing based off the design.</p> <p> <p></p> <p></p>"},{"location":"edm2/lessons/fusionpart2/#bicycle-rack-design","title":"Bicycle Rack Design","text":"<p>As a review and challenge for my current Fusion360 knowledge, I was asked to design a bicycle rack to a set of specified dimensions. Here is the image with dimensions that I was designing off of:</p> <p> </p> <p>I first created a sketch on the ZY-plane and used the line and circle tools to design my sketch to match the specified dimensions:</p> <p> </p> <p>I then created a new sketch on the XY-plane and created a circle with a diameter of 4mm. To make the design 3D, I used the sweep tool to sweep the circle along the path of the bike rack.</p> <p> <p></p> <p></p>"},{"location":"edm2/lessons/fusionpart2/#generating-2d-drawings","title":"Generating 2D Drawings","text":"<p>I used Kevin Kennedy's Youtube Video to learn how to generate 2D drawings in Fusion. To make a 2D drawing, I first navigated to the design I wanted to make a drawing of. I then clicked on the File dropdown menu, and clicked New Drawing -&gt; From Design, making sure to select the object I wanted a drawing of. To practice making 2D drawings, I generated a drawing from the Fusion360 example design for a connector joint:</p> <p> <p>I then created a 2D drawing of my bicycle rack design:</p> <p> </p> <p></p>"},{"location":"edm2/lessons/fusionpart2/#lego-technic-brick-design","title":"Lego Technic Brick Design","text":"<p>For more practice designing objects in Fusion, I was asked to create a Lego Technic Brick from the following image to its dimensions:</p> <p> </p> <p>There were many possible approaches to designing the brick, but I started with a 3D rectangular prism to match the general specifications of the actual brick part of the Lego. From there, I created concentric circles on the sides and the top, and extruded them (both as a new body and as a hole function) to the dimensions in the image. To create the Lego connectors at the bottom, I created 3 circles centered at the midpoints between the radii of the circle connectors on the top of the Lego brick. I then shelled the entire rectangular prism to hollow it out. Here is my completed design:</p> <p> </p> <p></p>"},{"location":"edm2/lessons/fusionpart2/#custom-lego-design","title":"Custom Lego Design","text":"<p>I was then challenged to design a custom lego brick. I started designing the brick from the main Lego body as it was relatively simply and would be a good base to design the appendages from. I then sketched and extruded one appendage, mirroring it to all 4 sides of the Lego brick. Lastly, I touched up the brick, adding things like the Lego plus connector joint and smoothing out issues with the spline curve extrursion. The following image shows the brick that I was assigned to design.</p> <p> </p> <p>I first used digital calipers to measure every dimension of the Lego brick. I would continue to use these calipers as I went through the designing process to ensure my dimensions were correct.</p> <p>I started off with designing the body of the custom brick. I designed a sketch on the XY plane and extruded it to create the main body of the brick:</p> <p> </p> <p>Next, I created a sketch to design one of the four appendages coming out of the Lego brick's body. I then mirrored it to all four positions and extruded them.</p> <p> <p></p> <p>I then created and extruded a spline curve on the side of the Lego brick to mimic the curve of the real brick.</p> <p> <p></p> <p>To create the plus axle joint I created another sketch in the shape of the plus, which I then mirrored and extruded.</p> <p> <p></p> <p>Here was my final recreation of the Lego brick, shown in both 2D and 3D views:</p> <p> <p></p>"},{"location":"edm2/lessons/fusionpart2/#trading-lego-designs","title":"Trading Lego Designs","text":"<p>Next, I traded Lego designs with a partner. We sent each other our respective 2D views of our design with dimensions. I would then model his Lego brick based solely on the 2D design he sent me:</p> <p> </p> <p>I started the design by creating a sketch for the base of the Lego.</p> <p> </p> <p>I then extruded each of the base's parts to the proper length. I extruded the area between the concentric circles and the two outer rectangles as Lego connectors.</p> <p> </p> <p>I then created a sphere above the base, which I subsequently cut in half.</p> <p> <p></p> <p>Next, I shelled the semisphere to make it hollow.</p> <p> </p> <p>I then created 3 sketches on planes oriented 120 degrees from each other.</p> <p> </p> <p>I created a rectangle perpendicular to each of the three planes. I then used the sweep tool to sweep the rectangle on the path, creating a hole that fits to the spherical shape.</p> <p></p> <p>Here are the 3D and 2D views of the final design:</p> <p> <p> </p>"},{"location":"edm2/lessons/fusionpart2/#custom-object-design","title":"Custom Object Design","text":"<p>To further practice using Fusion, I decided to design a pen holder, of which the tutorial can be found here. My Fusion360 file of the design can be downloaded here. To make the design, I started off with shelling a large cylinder. I then created a polygon on the XZ plane and extruded it as a hole onto the surface of the cylinder. I then repeated this across the whole surface of the cylinder via the rectangular and circular pattern tools.</p> <p> <p></p> <p>To print this file, I first exported the file out of Fusion360 and opened it in PrusaSlicer. I then sliced the file with grid supports and a 15% infill, scaling the design down to finish printing under an hour. Since I was printing at home, my printer wasn't setup with wifi capabilities, so I saved the sliced file onto a flash drive which I then plugged into the printer.</p> <p>Because the hexagonal patterning was so thin, when I originally printed the design, it was heavily reliant on supports. Those supports obscured the actual design and when I tried to peel them away, the entire design crumbled.</p> <p> <p></p> <p>To solve this issue, I first upscaled the print which made the mesh more stable. I also changed the support type to be organic instead of grid, which minimized the points of contact between the supports and the actual print. This time, the supports were a lot easier to remove and did not damage the structure</p> <p> </p>"},{"location":"edm2/lessons/milling/","title":"Milling About","text":"<p>The purpose of this unit was to familiarize ourselves to the entire milling process. We discussed different types of milling machines, and their respective differences and use cases. We first created and milled a simple design on a dog tag to get used to the milling process. Then, we designed a model for a chocolate mold and milled it on a block of wax. Finally, we laser cut a box from cardstock to fit the Valentine's Day theme.</p>"},{"location":"edm2/lessons/milling/#workflow","title":"Workflow","text":"<ul> <li>Apply double-sided adhesive to your material and the bed. Position and place the material on the bed.</li> <li>Open Bantam Tools Software</li> <li>Under home, select install tool, and select the bit that you are installing</li> <li>Insert your tool into the spindle<ul> <li>Use the 2 wrenches by the milling machine and align them respectively to the top of the spindle area and by the middle bit area where their imprints are</li> <li>Have you or a partner hold the bit itself while you unscrew it. This is to prevent the bit from dropping onto the bed and potentially breaking.</li> <li>To loosen the bit, bring the 2 wrenches in towards each other</li> <li>If using multiple bits, select them all under File Setup. Start with the smallest tool and make your way to the largest.</li> </ul> </li> <li>Probe<ul> <li>Move the metal prong from the side of the bed to sit on top of it. Make sure that they touch so electricity can conduct.</li> <li>Click <code>Z Only Stick Probing</code> and use the Jog menu to move the spindle above your material. </li> <li>Once the spindle is positioned above your material, start the probing.</li> </ul> </li> <li>Under Material Setup, leave the <code>Material Offset Z</code> at 0.01mm</li> <li>Change the <code>Material Size</code> to the dimensions of your material.</li> <li>Import your file under File Setup</li> <li>Choose the bit(s) you will be using</li> <li>You can see the projection of the result on the right side of the screen. Use the <code>Plan Offset</code> x, y, and z setting to move the placement of the design around.</li> <li>Ensure that you have the correct bit selected and inserted into the machine.</li> <li>Run the job from the Summary/Run section. It is recommended to run the engraving job before the cutting job to ensure that your material does not move around.</li> <li>If you are using multiple bits, the machine will pause during the job and prompt you to change bits.</li> </ul>"},{"location":"edm2/lessons/milling/#dog-tag","title":"Dog Tag","text":"<p>Before milling the dog tag, I brainstormed an idea for what the tag would look like. I decided upon a simple design with just my name on the tag.</p> <p> </p> <p>I then created the design in CorelDraw. To do this, I first imported a template of the dogtag. This template functions as effective boundaries for my design to ensure that they can all be milled inside the actual dogtag. I then created a textbox inside the area of the dogtag and type my name into it. The following image image contains both the outline and the text, but when actually milling the file, I deleted the outline and only milled the text, as the outline is just a reference to guage size and there is no need to cut out the outline of the dog tag because it is already an individual material piece.</p> <p> </p> <p>I then sent the file to Bantam Software and milled it on the Bantam Desktop CNC Machine. I first set the dimensions of the material to the size of the dogtag, measured via calipers. Next, I equipped the 1/8\" bit and used Bantam Software's <code>Install Tool</code> function to ensure that the tool was homed and its length was calibrated correctly. I then used the <code>Z Only Stick Probing</code> function to calibrate the height offset of my material. I then ran the job.</p> <p> <p></p> <p>The main problem I encountered was the machine not milling deep enough to actually engrave the material, as the bit would either only barely touch the dog tag or start engraving in the air altogether. This problem was solved by rebooting both the machine and the software and redoing the tool length calibration and Z Material Offset calibration. Here is the final milled dogtag:</p> <p> </p> <p></p>"},{"location":"edm2/lessons/milling/#chocolate-mold","title":"Chocolate Mold","text":"<p>Before starting to design the mold, I created a quick sketch for my idea of the mold. Each member of our group created an individual design so that we could get different perspectives about the mold's design and aggregate them all into a final design.</p> <p> </p> <p>After discussing with my groupmates, we decided to use the following design for the mold.</p> <p> </p> <p>I then created the design in Fusion360. I first created a cube, then extruded a heart-shaped hole on the top face of the cube.</p> <p> </p> <p>I then converted the design to a toolpath. I changed workspaces from Design to Manufacturing. I then selected 3D Adaptive Clearing and selected the heart hole, then generated the design. I later realized that this actually caused the final design to be inverted, where the heart shaped hole was not cut out and everything else was cut out. If I were to redo the milling process, I would make sure to select the other components of the design as to make sure the heart indentation was being cutout. Unfortunately, I did not realize this mistake at the time and thus the wax mold would be inverted. However, the rest of the milling process is correct.</p> <p> </p> <p>Here is the simulation of the toolpaths. If I had realized to use the simulate function to check the actual milling toolpaths, I may have realized the error before milling. However, nobody in our group realized that the simulation function existed until after we finished milling.</p> <p> <p></p> <p>The wax mold was milled on a Bantam Tools Desktop CNC Milling Machine, as opposed to the dog tag which was milled on an Othermill Milling Machine. The process for milling was very similar as both machines used the Bantam Tools Desktop Milling Machine Software as a gateway for milling designs (the producers of the Othermill and Bantam machines are the same company).</p> <p> <p></p> <p>Here is the final mold after milling:</p> <p> </p>"},{"location":"edm2/lessons/milling/#chocolate-box","title":"Chocolate Box","text":"<p>To complement the chocolate mold and the Valentine's Day theme, I also designed and cut a chocolate box on special cardstock that had a gold infill. I designed the box in CorelDraw with tabs so that we could use small bits of double-sided tape to hold the box together while maintaining food safety, and designed a tab on the front to be able to close the box. I put raster lines where two faces met together so that we would be able to fold the box easier after the cut.</p> <p> </p> <p>We then sent the design to the laser cutter.</p> <p> <p></p> <p>Here is the box after putting the bits of tape on the tabs:</p> <p> </p> <p>For fun, we disassembled the box and engraved a hearts design on the top of the box. Here is the box (closed via the tab) with the hearts design on top.</p> <p> </p>"},{"location":"edm2/lessons/milling/#overall-problems","title":"Overall Problems","text":"<p>Most of this project went smoothly. The only area in which I encountered some difficulty was milling. At first, when running my dog tag job, the milling machine wouldn't probe correctly and would start cutting in the air, which was solved by restarting the machine and the software and re-running the 1 axis Z probing. When running the wax mold job, the heart symbol was inversed and was left uncut while the rest of the mold was cut. Unfortunately, this issue was not resolved before we cut the wax as we did not realize its presence, but in the future, this issue could be solved by running a simulation of the job before actually cutting.</p>"},{"location":"edm2/lessons/pressingCharges/","title":"Pressing Charges","text":"<p>For this project, I was given a soldering kit for a small handheld gaming console. The objective of this lesson was to teach us how to identify and understand the importance of each component of the console. We were able to practice through-hole soldering on a variety of components, and I also experimented with solder paste. I also learned how to operate the solder-sucker gun when I was fixing some issues I encountered during soldering.</p> <p>NB: All of the images I took got hypercompressed far more than usual. I'm not exactly sure why this is the case as they look fine on my end. However, this should not affect the documentation as all key features are still visible.</p> <p>I first took all of the parts out of the kit.</p> <p> </p> <p>I then further unboxed all of the electronics components to ensure that I had everything.</p> <p> <p></p> <p>Next, I identified all of my parts to prepare them for soldering.</p> <p> </p> <p>I then soldered on the 10uF capacitor, ceramic 104 capacitor, on/off tactile button switch, speaker, 1k ohm resistor, and triode.</p> <p> <p></p> <p>Next, I soldered on the 5 control buttons.</p> <p> <p></p> <p>I then soldered on the LED display and the USB power base.</p> <p> <p></p> <p>I soldered on the base that would hold the chip in place and connect its pins to the PCB traces. I was able to use solder paste for this component as it was a lot of solder holes all lined up together.</p> <p> <p></p> <p>I then soldered on the 2 LED dot matrices. This was the only major problem I ran into during this project, as I misread the polarity of the dot matrices and soldered both on backwards. However, due to this rather significant mistake, I was able to learn how to use the solder sucker gun, which was incredibly effective and even surprised me in that it allowed me to unsolder over 30 joints in just a few minutes. After correcting the polarity of the LED matrices, this is what my board looked like.</p> <p> <p></p> <p>I then pushed the chip into the chip holder. This took a bit of work to align each pin and I used tweezers to push in a couple of stubborn pins that refused to go into their spots. After pushing in the chip, these two images are the final images of the front and back of my board respectively without the installation of button caps.</p> <p> <p></p> <p>I then put on the button caps and assembled the acrylic case for the board.</p> <p> </p> <p>Here is a video of my completely assembled console while I play tetris on it.</p> <p> <p></p> <p>Overall, this unit was pretty straightforward and relatively simple. The only major issue I encountered along the way was misreading the polarity of the LED matrices and soldering them on backwards. However, this was a great opportunity to learn how to use the solder-sucker gun, which proved to be incredibly effective in allowing me to remove and resolder the matrices on. I was so impressed by the solder-sucking gun that I would end up using it on other projects such as the production of a SAMD11C PCB where I accidentially soldered a component on backwards.</p>"}]}