{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#richard-shan","title":"Richard Shan","text":"<p> <p> </p>"},{"location":"about/","title":"About","text":""},{"location":"about/#about-me","title":"About Me","text":""},{"location":"brailliant/","title":"Brailliant","text":"<p>WARNING: The project video and slide (poster) are out of date and provide incorrect information about licensing. The current EULA is here. By installing, accessing, or using the Product, you acknowledge that you have read this Agreement, understand it, and agree to be bound by its terms and conditions.</p>"},{"location":"brailliant/#overview","title":"Overview","text":"<p>I want to create a 3x2 solenoid array that can display braille characters by pushing solenoids up and down to create dots. This solenoid array will be connected to a Raspberry Pi, which in turn will be connected to an ESP32CAM. The camera will take a picture of a page of text, then perform OCR (optical character recognition) to extract a string of text from the image. That string of text will be converted to braille, which will be displayed on the solenoid array by flashing each character for 1 second at a time. This device will essentially allow for live-time conversion of any text into braille, which I hope will increase accessibility to books and the like.</p>"},{"location":"brailliant/#brainstorming-process","title":"Brainstorming Process","text":""},{"location":"brailliant/#initial-thoughts","title":"Initial Thoughts","text":"<p>My idea was to design a text to braille converter, which a blind person could use by moving the device over a page of text to convert it into braille. The braille translation of the English text would then be represented via a series of up/down pins which the user could use to interpret the information. The device was to be a rectangular box that would use an internal camera to interpret and OCR text, which could then be translated into braille and displayed via a series of servo motors pushing up metal rods on the top of the box. The pins would be in groups of six, each group representing a single braille character.</p> <p>However, I talked to Stuart Christhilf who had thought of a similar mechanism for his initial final project. He originally planned to create a dynamic clock to display the time using blocks of wood that acould be pushed out or pulled back via servos. However, when building his project, he realized that fitting so many servos into such a small space was completely unfeasible and warned me from doing the same. My initial design is shown in the following image:</p> <p> </p> <p>I then decided to use electromagnets for my pins, instead of a servo. The pins themselves would be a small magnetic rod sitting on top of an electromagnet. The small electromagnet could be powered on and off via a microcontroller. When the electromagnet was off, the pin would simply rest on top of the electromagnet, and the pin would be flush against the top of the board, forming the down position of the pin. If the pin needed to pop up, the microcontroller would power the electromagnet which would then emit a repelling magnetic charge. That magnetic force would then repel the pin slightly upwards, forming the up position of the pin. To represent a braille character, the microcontroller would push the specific pins into the up position that together would form the 6-dot pattern of the character.</p> <p>I also decided to move the camera out of the box. That would allow for more simple wiring and internal organization of the box, and allow the operator to more easily use the device. Moving the camera out means that the user would only need to move a small camera container across the page of text, instead of dragging the entire device. Here is my modified design:</p> <p> </p>"},{"location":"brailliant/#significant-changes","title":"Significant Changes","text":"<p>Although a large part of my project remains the same, I've changed some aspects of my project. Namely, I've decided to use a Raspberry Pi as a central controller and connect it to 5 separate ATTiny412 chips, which will each be responsible for controlling 6 electromagnets to represent 1 braille character. Each ATTiny412 and 6 electromagnet setup will be on its own PCB, and receive data from the controlling Raspberry Pi. Additionally, I decided to create an elevated case for the ESP32 camera so that the image would have a better angle and thus an easier time being processed for OCR, and so that more light could come into the camera lens from the unobstructed sides. Lastly, I decided I wanted to wirelessly transmit data from the ESP32 camera to the Raspberry Pi for processing. I worked with both serial communication and WiFi connectivity previously so I hope to sum it all together and wirelessly transmit data between these two controllers.</p> <p>Here is an updated system diagram which maps out all the parts of my project.</p> <p> </p>"},{"location":"brailliant/#feasibility","title":"Feasibility","text":"<p>However, after doing research, I realized that having 30 solenoids would be unfeasible. Instead, I decided to scale my project down to just having 6 solenoids, as this would still accomplish the mission of displaying braille for a reader. I would then flash each braille character for 1 second on the 6 solenoid array. This change allows me to worry less about power budget and ensures that I have a ready final project on my presentation date.</p>"},{"location":"brailliant/#bill-of-materials","title":"Bill of Materials","text":""},{"location":"brailliant/#components","title":"Components","text":""},{"location":"brailliant/#braille-box-cad","title":"Braille Box CAD","text":""},{"location":"brailliant/#initial-design","title":"Initial Design","text":"<p>I decided to first model my design in Fusion360, as I had prior experience working with Fusion and was pretty comfortable using it. When I started out with Autodesk Fusion, Kevin Kennedy's Fusion tutorials were a massive help.</p> <p>I first started off with a rectangular prism to act as the main body of the design.</p> <p> </p> <p>Next, I filleted the box to round out the edges.</p> <p> </p> <p>I then created a sketch on the top of the box, where I created six circles. These 6 circles represent the holes where I will put metal pins into that can pop up and down depending on what needs to be represented.</p> <p> </p> <p>I extruded the circles downward as holes. This creates the actual space where the pins will be placed.</p> <p> </p> <p>Finally, I used the pattern feature to repeat the sketch and extrusion across the top of the box. This created a total of 5 evenly spaced sets of 6 pins. With each set of 6 pins representing a single braille character, one iteration of pin setups can represent five letters.</p> <p> </p>"},{"location":"brailliant/#improved-design","title":"Improved Design","text":"<p>As I had made my initial design early on, it did not reflect the changes I had made to my final project, most notably scaling down the amount of solenoids from 5 arrays of 3x2 to one 3x2 array. Additionally, when I made the original design, I didn't think much about how I would power the solenoid array and thus didn't include any spots for batteries. I also wanted to make the holes for the solenoids on a separate press-fit cover on top of the main box. Finally, the original design doesn't include any internal parts to hold the solenoids in place.</p> <p>For my new design, I want to make the following key changes:</p> <ul> <li>3x2 solenoid array</li> <li>Internal beams to support solenoids</li> <li>Battery pack holders</li> <li>Press-fit cover</li> </ul> <p>Additionally, I want the box to look as nice as possible and ideally have all wiring contained within it.</p> <p>I first started out by creating the shell of the box. I hollowed out the innards because I want my electronics to be inside. I will end up adhering my PCB to the side of the box and having my MOSFET breakouts on the bottom of the hollowed inside.</p> <p> </p> <p>I then started working on the top cover. I started out with creating a sketch where all my holes would be, and a offset on the edges to match the shell. I then extruded the sketch to create the cover with holes that the solenoids will fill.</p> <p> </p> <p>Based on my previous sketch offset for the edges of the shell box, I created quarter-circles and extruded them to form the press-fit lid.</p> <p> </p> <p>Next, I designed the beams that hold the solenoids in place. I started by creating a sketch on the bottom of the shell box and extruded that to my desired height. I then created a sketch on the extruded rectangular prism to remove the bottom part of it and form it into a beam-like shape.</p> <p> </p> <p>I then started work on the battery holders. I created the bottom of the battery holder then extruded out the sides.</p> <p> </p> <p>Next, I created the dividers to firmly hold each battery pack in place. Each divided section has the same length and width dimensions as the actual battery pack that I will use, plus a little for tolerance.</p> <p> </p> <p>I then added a small hole on the side for the power, ground, and TX/RX cables for the ATTiny1614.</p> <p> </p> <p>Next, I extruded a small hole as a slot for the wires on the external battery packs to route into the main shell box, where it will be connected to the MOSFETs controlling the solenoids.</p> <p> </p> <p>Finally, I added fillets. Here is the final box design.</p> <p> </p>"},{"location":"brailliant/#raspberry-pi-box-cad","title":"Raspberry Pi Box CAD","text":"<p>I first started off with a shelled box.</p> <p> </p> <p>I then added a lid, with a hole the same size as my 5 inch touchscreen where I would attach the screen.</p> <p> </p> <p>Next, I added screw holes on the lid. These screw holes allowed me to secure the screen to the 3D printed lid. Unfortunately, my screw holes actually ended up being a little small so I had to enlarge them after the print with the help of a soldering iron.</p> <p> </p> <p>I added legs to the lid to allow it to press-fit into the base.</p> <p> </p> <p>Then, I added holes for wires on the front and side of the Pi case.</p> <p> </p> <p>Finally, I added fillets all around.</p> <p> </p> <p>After printing that initial iteration, the screen fit and there was enough space inside to fit the Raspberry Pi. However, some of my cables didn't fit as they had long \"necks\" that had to remain straight. As such, I would have to significantly bend the HDMI and USB cords for the Raspberry Pi.</p> <p> </p> <p>As such, I adjusted the length of the box to give space for the USB cable necks. I also slightly decreased the height of the holes for the USB cables as they were larger than necessary and somewhat an eyesore.</p> <p> </p>"},{"location":"brailliant/#electronics","title":"Electronics","text":"<p>Electronics were by far the worst part of this project, at least for me. The main issue was that I didn't understand transistors very well, and I ran into a bunch of problems with them. The two main problems I ran into were transistors not being able to handle the power and transistors having inconsistent pinouts and being backwards or jumbled around.</p> <p>In this section, I'll go through a few of the boards that didn't work then show my final board.</p> <p>This was my initial board design. I tested this board by plugging a solenoid into the top pin, and the resulting lack of transistor is visible. The transistor heated up after around 5 seconds and fell off the board, without powering the solenoid.</p> <p> </p> <p>This is my second iteration. I added what I thought were pull down resistors hidden under the left-hand side white female pin headers (they did not, in fact, function as pull down resistors) and a power indicator LED. Unfortunately, I forgot the capacitor, but that would not have affected this board's outcome of failure. When creating this board, I also ran into major issues with the ATTiny1614, which stuck me for a couple hours. Apparently, some of the ATTiny1614 chips in our lab just didn't work, so I needed to get the ATTiny1614s out of a specific drawer because those chips had a small dot indentation on one side. Only the chips with the dot indentation seemed to work well, in my experience. Upon testing, the transistor got really hot and I unplugged it before it melted off.</p> <p> </p> <p>In an attempt to simplify the amount of things that could cause the issue, I scaled down to one transistor, which in turn melted off.</p> <p> </p> <p>I tweaked the design with one transistor, and it melted again.</p> <p> </p> <p>At this point, I created a ATTiny1614 board for testing and debugging.</p> <p> </p> <p>I created another board with a pull down and headers to plug into external transistors. The hope was that this would allow me to test transistors without melting pads and traces.</p> <p> </p> <p>At this point, I was fairly certain the transistor had a problem with handling power. I switched to the Eugepae board which had a different transistor. I had made this board during a group project for embedded networking and communications, and it had previously handled 5V, so I was pretty confused when it failed to power my 5V solenoids. Unfortunately, this board also failed, in retrospect likely because the solenoids pulled too many amps.</p> <p> </p> <p>I then decided to switch to a through hole MOSFET. This board also failed, which I'm pretty confused about but will explain in the next paragraph.</p> <p> </p> <p>After all these boards failed, I found MOSFET drive modules in the Lab. These modules have six key inputs: VIN+, VIN-, VOUT+, VOUT-, TRIG, and GND. I ended up connecting the VIN+ and VIN- to the positive and ground terminals on the power input device (battery packs), the VOUT+ and VOUT- to the positive and ground of the load output device (solenoid), the TRIG pin to a GPIO on my ATTiny1614 board that would toggle the solenoid on and off, and the GND to ground. The architecture of the drive module (which worked) was really similar to some of my MOSFET attempts, so I am still a little unsure why this board worked when my own didn't. The only major discrepancy that I noticed was that this board had 2 transistors.</p> <p> </p> <p>Now that I decided to use the MOSFET breakout boards to toggle the solenoids, the MOSFETs are external to the main board and I am able to create a ATTiny1614 control board without transistors on it. My final board has headers for its power, ground, and data; a power indicator LED; a capacitor; pins for TX/RX serial communication with the Raspberry Pi, which will send text to be displayed as braille; 6 headers each corresponding to a GPIO pin on the ATTiny1614 which in turn, corresponds to controlling a single solenoid in the 3x2 array; and 6 headers for GND that will each connect to 1 MOSFET.</p> <p> </p>"},{"location":"brailliant/#esp32cam-wireless-transmission","title":"ESP32CAM Wireless Transmission","text":"<p>WebSocket connections are initiated through HTTP protocol, using an upgrade request from HTTP to WebSocket. This begins with a client sending a standard HTTP request that includes an \"Upgrade: websocket\" header and a \"Connection: Upgrade\" header to the server. The server then responds with an HTTP 101 status code, indicating that the protocol will change, thus establishing the WebSocket connection.</p> <p>WebSocket, uses IP addresses to facilitate the initial connection before upgrading to the WebSocket protocol. Once the WebSocket connection is established, the IP addresses are used to maintain the connection over which data frames can be reliably transmitted back and forth.</p> <p>Once the WebSocket connection is established, data is transmitted in framed messages through backend data transmission ports, where each frame consists of an opcode to indicate the type of data being transmitted (e.g., text, binary, continuation frame, or control frames like close, ping, or pong). This structure allows the WebSocket protocol to be extremely versatile and efficient in handling different types of data seamlessly. The frames are small and allow for very efficient data transmission.</p> <p>The following program is uploaded onto the ESP32 CAM Board through Arduino IDE. This program is based off of the CameraWebServer example program from ESP32.</p> <pre><code>#include \"esp_camera.h\"\n#include \"WiFi.h\"\n#include \"WebSocketsServer.h\"\n\n#define CAMERA_MODEL_AI_THINKER // Has PSRAM\n#include \"camera_pins.h\"\n\nconst char* ssid = \"REDACTED\";\nconst char* password = \"REDACTED\";\n\nWebSocketsServer webSocket = WebSocketsServer(81);\n\nvoid startCameraServer();\nvoid setupLedFlash(int pin);\nvoid onWebSocketEvent(uint8_t client_num, WStype_t type, uint8_t *payload, size_t length);\n\nvoid setup() {\n  pinMode(2, OUTPUT);\n  Serial.begin(9600);\n  while (!Serial); // Wait for the serial connection to initialize\n  Serial.setDebugOutput(true);\n  Serial.println();\n\n  camera_config_t config;\n  config.ledc_channel = LEDC_CHANNEL_0;\n  config.ledc_timer = LEDC_TIMER_0;\n  config.pin_d0 = Y2_GPIO_NUM;\n  config.pin_d1 = Y3_GPIO_NUM;\n  config.pin_d2 = Y4_GPIO_NUM;\n  config.pin_d3 = Y5_GPIO_NUM;\n  config.pin_d4 = Y6_GPIO_NUM;\n  config.pin_d5 = Y7_GPIO_NUM;\n  config.pin_d6 = Y8_GPIO_NUM;\n  config.pin_d7 = Y9_GPIO_NUM;\n  config.pin_xclk = XCLK_GPIO_NUM;\n  config.pin_pclk = PCLK_GPIO_NUM;\n  config.pin_vsync = VSYNC_GPIO_NUM;\n  config.pin_href = HREF_GPIO_NUM;\n  config.pin_sscb_sda = SIOD_GPIO_NUM;\n  config.pin_sscb_scl = SIOC_GPIO_NUM;\n  config.pin_pwdn = PWDN_GPIO_NUM;\n  config.pin_reset = RESET_GPIO_NUM;\n  config.xclk_freq_hz = 20000000;\n  config.frame_size = FRAMESIZE_UXGA;\n  config.pixel_format = PIXFORMAT_JPEG; \n  config.grab_mode = CAMERA_GRAB_WHEN_EMPTY;\n  config.fb_location = CAMERA_FB_IN_PSRAM;\n  config.jpeg_quality = 12;\n  config.fb_count = 1;\n\n  if (psramFound()) {\n    config.jpeg_quality = 10;\n    config.fb_count = 2;\n    config.grab_mode = CAMERA_GRAB_LATEST;\n  }\n\n  esp_err_t err = esp_camera_init(&amp;config);\n  if (err != ESP_OK) {\n    Serial.printf(\"Camera init failed with error 0x%x\", err);\n    return;\n  }\n\n  sensor_t *s = esp_camera_sensor_get();\n  s-&gt;set_vflip(s, 1); // Flip it back\n  s-&gt;set_brightness(s, 1); // Up the brightness just a bit\n  s-&gt;set_saturation(s, -2); // Lower the saturation\n\n#if defined(LED_GPIO_NUM)\n  setupLedFlash(LED_GPIO_NUM);\n#endif\n\n  WiFi.begin(ssid, password);\n  WiFi.setSleep(false);\n  while (WiFi.status() != WL_CONNECTED) {\n    delay(500);\n    Serial.print(\".\");\n  }\n\n  Serial.println(\"\");\n  Serial.println(\"WiFi connected\");\n  webSocket.begin();\n  webSocket.onEvent(onWebSocketEvent);\n  startCameraServer();\n\n  Serial.print(\"Camera Ready! Use 'http://\");\n  Serial.print(WiFi.localIP());\n  Serial.println(\"' to connect\");\n}\n\nvoid loop() {\n  webSocket.loop();\n}\n\nvoid onWebSocketEvent(uint8_t client_num, WStype_t type, uint8_t *payload, size_t length) {\n  switch (type) {\n    case WStype_DISCONNECTED:\n      Serial.printf(\"[%u] Disconnected!\\n\", client_num);\n      break;\n    case WStype_CONNECTED:\n      {\n        IPAddress ip = webSocket.remoteIP(client_num);\n        Serial.printf(\"[%u] Connection from \", client_num);\n        Serial.println(ip.toString());\n      }\n      break;\n    case WStype_TEXT:\n      if (strcmp((char *)payload, \"capture\") == 0) {\n        camera_fb_t *fb = esp_camera_fb_get();\n        if (!fb) {\n          Serial.println(\"Camera capture failed\");\n        } else {\n          webSocket.sendBIN(client_num, fb-&gt;buf, fb-&gt;len);\n          esp_camera_fb_return(fb);\n        }\n      }\n      break;\n    case WStype_BIN:\n      Serial.printf(\"[%u] Get binary length: %u\\n\", client_num, length);\n      break;\n  }\n}\n\nvoid setupLedFlash(int pin) {\n  pinMode(pin, OUTPUT);\n  digitalWrite(pin, LOW);\n}\n</code></pre> <p>This program connects the ESP32CAM to a local WiFi network. It then sets up and initializes the camera, and sets up the local IP connection. It then continuously waits for a web socket connection. When a connection is created, it prints the IP address of the connecting device. If the device sends an input of \"capture\", the camera will take a picture and send it via the network web socket connection to the connecting Raspberry Pi.</p>"},{"location":"brailliant/#camera-feed-ocr","title":"Camera Feed OCR","text":"<p>I had previously setup infrastructure to wirelessly transmit a command to capture an image from a Raspberry Pi to the ESP32CAM, along with sending the image data back over the network and saving it. I had created a WebSocket server to accept commands and then send the image data over HTTP back to the Raspberry Pi.</p> <p>However, I realized that I could fetch the image without needing a WebSocket handler by connecting to the ESP32CAM's capture image handler directly. The capture handler from the default CameraWebServer example project sets up a port that allows a direct download to what is currently on the camera feed.</p> <pre><code>static esp_err_t capture_handler(httpd_req_t *req)\n{\n    camera_fb_t *fb = NULL;\n    esp_err_t res = ESP_OK;\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n    int64_t fr_start = esp_timer_get_time();\n#endif\n\n#if CONFIG_LED_ILLUMINATOR_ENABLED\n    enable_led(true);\n    vTaskDelay(150 / portTICK_PERIOD_MS); // The LED needs to be turned on ~150ms before the call to esp_camera_fb_get()\n    fb = esp_camera_fb_get();             // or it won't be visible in the frame. A better way to do this is needed.\n    enable_led(false);\n#else\n    fb = esp_camera_fb_get();\n#endif\n\n    if (!fb)\n    {\n        log_e(\"Camera capture failed\");\n        httpd_resp_send_500(req);\n        return ESP_FAIL;\n    }\n\n    httpd_resp_set_type(req, \"image/jpeg\");\n    httpd_resp_set_hdr(req, \"Content-Disposition\", \"inline; filename=capture.jpg\");\n    httpd_resp_set_hdr(req, \"Access-Control-Allow-Origin\", \"*\");\n\n    char ts[32];\n    snprintf(ts, 32, \"%lld.%06ld\", fb-&gt;timestamp.tv_sec, fb-&gt;timestamp.tv_usec);\n    httpd_resp_set_hdr(req, \"X-Timestamp\", (const char *)ts);\n\n#if CONFIG_ESP_FACE_DETECT_ENABLED\n    size_t out_len, out_width, out_height;\n    uint8_t *out_buf;\n    bool s;\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n    bool detected = false;\n#endif\n    int face_id = 0;\n    if (!detection_enabled || fb-&gt;width &gt; 400)\n    {\n#endif\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n        size_t fb_len = 0;\n#endif\n        if (fb-&gt;format == PIXFORMAT_JPEG)\n        {\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n            fb_len = fb-&gt;len;\n#endif\n            res = httpd_resp_send(req, (const char *)fb-&gt;buf, fb-&gt;len);\n        }\n        else\n        {\n            jpg_chunking_t jchunk = {req, 0};\n            res = frame2jpg_cb(fb, 80, jpg_encode_stream, &amp;jchunk) ? ESP_OK : ESP_FAIL;\n            httpd_resp_send_chunk(req, NULL, 0);\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n            fb_len = jchunk.len;\n#endif\n        }\n        esp_camera_fb_return(fb);\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n        int64_t fr_end = esp_timer_get_time();\n#endif\n        log_i(\"JPG: %uB %ums\", (uint32_t)(fb_len), (uint32_t)((fr_end - fr_start) / 1000));\n        return res;\n#if CONFIG_ESP_FACE_DETECT_ENABLED\n    }\n\n    jpg_chunking_t jchunk = {req, 0};\n\n    if (fb-&gt;format == PIXFORMAT_RGB565\n#if CONFIG_ESP_FACE_RECOGNITION_ENABLED\n     &amp;&amp; !recognition_enabled\n#endif\n     ){\n#if TWO_STAGE\n        HumanFaceDetectMSR01 s1(0.1F, 0.5F, 10, 0.2F);\n        HumanFaceDetectMNP01 s2(0.5F, 0.3F, 5);\n        std::list&lt;dl::detect::result_t&gt; &amp;candidates = s1.infer((uint16_t *)fb-&gt;buf, {(int)fb-&gt;height, (int)fb-&gt;width, 3});\n        std::list&lt;dl::detect::result_t&gt; &amp;results = s2.infer((uint16_t *)fb-&gt;buf, {(int)fb-&gt;height, (int)fb-&gt;width, 3}, candidates);\n#else\n        HumanFaceDetectMSR01 s1(0.3F, 0.5F, 10, 0.2F);\n        std::list&lt;dl::detect::result_t&gt; &amp;results = s1.infer((uint16_t *)fb-&gt;buf, {(int)fb-&gt;height, (int)fb-&gt;width, 3});\n#endif\n        if (results.size() &gt; 0) {\n            fb_data_t rfb;\n            rfb.width = fb-&gt;width;\n            rfb.height = fb-&gt;height;\n            rfb.data = fb-&gt;buf;\n            rfb.bytes_per_pixel = 2;\n            rfb.format = FB_RGB565;\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n            detected = true;\n#endif\n            draw_face_boxes(&amp;rfb, &amp;results, face_id);\n        }\n        s = fmt2jpg_cb(fb-&gt;buf, fb-&gt;len, fb-&gt;width, fb-&gt;height, PIXFORMAT_RGB565, 90, jpg_encode_stream, &amp;jchunk);\n        esp_camera_fb_return(fb);\n    } else\n    {\n        out_len = fb-&gt;width * fb-&gt;height * 3;\n        out_width = fb-&gt;width;\n        out_height = fb-&gt;height;\n        out_buf = (uint8_t*)malloc(out_len);\n        if (!out_buf) {\n            log_e(\"out_buf malloc failed\");\n            httpd_resp_send_500(req);\n            return ESP_FAIL;\n        }\n        s = fmt2rgb888(fb-&gt;buf, fb-&gt;len, fb-&gt;format, out_buf);\n        esp_camera_fb_return(fb);\n        if (!s) {\n            free(out_buf);\n            log_e(\"To rgb888 failed\");\n            httpd_resp_send_500(req);\n            return ESP_FAIL;\n        }\n\n        fb_data_t rfb;\n        rfb.width = out_width;\n        rfb.height = out_height;\n        rfb.data = out_buf;\n        rfb.bytes_per_pixel = 3;\n        rfb.format = FB_BGR888;\n\n#if TWO_STAGE\n        HumanFaceDetectMSR01 s1(0.1F, 0.5F, 10, 0.2F);\n        HumanFaceDetectMNP01 s2(0.5F, 0.3F, 5);\n        std::list&lt;dl::detect::result_t&gt; &amp;candidates = s1.infer((uint8_t *)out_buf, {(int)out_height, (int)out_width, 3});\n        std::list&lt;dl::detect::result_t&gt; &amp;results = s2.infer((uint8_t *)out_buf, {(int)out_height, (int)out_width, 3}, candidates);\n#else\n        HumanFaceDetectMSR01 s1(0.3F, 0.5F, 10, 0.2F);\n        std::list&lt;dl::detect::result_t&gt; &amp;results = s1.infer((uint8_t *)out_buf, {(int)out_height, (int)out_width, 3});\n#endif\n\n        if (results.size() &gt; 0) {\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n            detected = true;\n#endif\n#if CONFIG_ESP_FACE_RECOGNITION_ENABLED\n            if (recognition_enabled) {\n                face_id = run_face_recognition(&amp;rfb, &amp;results);\n            }\n#endif\n            draw_face_boxes(&amp;rfb, &amp;results, face_id);\n        }\n\n        s = fmt2jpg_cb(out_buf, out_len, out_width, out_height, PIXFORMAT_RGB888, 90, jpg_encode_stream, &amp;jchunk);\n        free(out_buf);\n    }\n\n    if (!s) {\n        log_e(\"JPEG compression failed\");\n        httpd_resp_send_500(req);\n        return ESP_FAIL;\n    }\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n    int64_t fr_end = esp_timer_get_time();\n#endif\n    log_i(\"FACE: %uB %ums %s%d\", (uint32_t)(jchunk.len), (uint32_t)((fr_end - fr_start) / 1000), detected ? \"DETECTED \" : \"\", face_id);\n    return res;\n#endif\n}\n</code></pre> <p>I then set up the Raspberry Pi to receive an image from the ESP32CAM and perform OCR upon it.</p> <p>First, I created a directory to store this project.</p> <pre><code>cd Desktop\nmkdir ocr\n</code></pre> <p>Upon entering the new directory, I need to create a virtual environment to install the libraries I will be using for OCR.</p> <pre><code>python -m venv /virtual\n</code></pre> <p>However, running this command gave me an error. </p> <pre><code>Error: [Errno13] Permission denied: '/virtual'\n</code></pre> <p>For some reason, this command didn't have the permissions to create a new virtual environment, which was strange considering that the project directory was not protected in any way. Regardless, I attached the sudo prefix and successfully created the virtual environment.</p> <pre><code>sudo python -m venv /virtual\n</code></pre> <p>I then entered the virtual environment by activating it.</p> <pre><code>source bin/activate\n</code></pre> <p>The bin/activate is a relative path and would activate the venv as long as I am in the \"ocr\" folder. However, the venv could also be activated by supplying the absolute path of \"~/home/richard/Desktop/ocr/bin/activate\".</p>"},{"location":"brailliant/#pytesseract","title":"PyTesseract","text":"<p>After activating the virtual environment, I can install all of my library dependencies.</p> <pre><code>sudo pip install pytesseract\nsudo pip install opencv-python\n</code></pre> <p>I then created the actual program that the Raspberry Pi would run.</p> <pre><code>import time\nimport cv2\nimport urllib.request\nimport numpy as np\nimport pytesseract\n\nurl = 'http://10.12.28.193/capture'\n\nimg_resp = urllib.request.urlopen(url)\nimgnp = np.array(bytearray(img_resp.read()), dtype=np.uint8)\nframe = cv2.imdecode(imgnp, -1)\n\ntext = pytesseract.image_to_string(frame, config='--psm 7')\n\nprint(\"Extracted Text:\", text)\ntime.sleep(1)\n</code></pre> <p>This script has the Raspberry Pi connect to the /capture handler of the ESP32CAM interface, which directly returns a capture of the current feed. It then decodes the image and parses it into the pytesseract OCR function. The PSM value of 6 tells the OCR model to scan the image for a single text block and extract text from that. A full list of PSM value options can be found by running <code>tesseract --help-psm</code> in the terminal.</p> <pre><code>  0    Orientation and script detection (OSD) only.\n  1    Automatic page segmentation with OSD.\n  2    Automatic page segmentation, but no OSD, or OCR. (not implemented)\n  3    Fully automatic page segmentation, but no OSD. (Default)\n  4    Assume a single column of text of variable sizes.\n  5    Assume a single uniform block of vertically aligned text.\n  6    Assume a single uniform block of text.\n  7    Treat the image as a single text line.\n  8    Treat the image as a single word.\n  9    Treat the image as a single word in a circle.\n 10    Treat the image as a single character.\n 11    Sparse text. Find as much text as possible in no particular order.\n 12    Sparse text with OSD.\n 13    Raw line. Treat the image as a single text line, bypassing hacks that are Tesseract-specific.\n</code></pre> <p>In my case, since I want the model to scan an image to find the line of text for \"Hello World!\", I will use psm-7.</p> <p>Here is a photo of my Raspberry Pi setup.</p> <p> </p> <p>The ESP32CAM is pointed towards a paper with the words \"Hello World!\". In the right side of the picture, the Raspberry Pi which is running the code is visible along with the display. Upon running the program on the Pi's terminal, the ESP32CAM takes a picture and transmits it to the Pi, which then uses tesseract to perform OCR on it and prints out the extracted text.</p> <p> <p></p>"},{"location":"brailliant/#gpt4o","title":"GPT4o","text":"<p>At this point, I wanted to try to use as little computational power as possible, and thus decided to switch to processing my image in base64. Although switching to base64 ultimately failed to scale down the computing enough to run on a microcontroller, it still led me in an interesting direction: that I could use GPT4o's new multimodal capabilities as an OCR engine to extract text from the base64 image. GPT4o in general is much more accurate in OCR than pytesseract, hence the switch.</p> <p>To do this, I first created a new handler on the ESP32CAM that would have it return a base64 string of a capture of the camera feed when that handler is called.</p> <pre><code>static esp_err_t jpg_base64_handler(httpd_req_t *req) {\n    camera_fb_t *fb = esp_camera_fb_get();\n    if (!fb) {\n        Serial.println(\"Camera capture failed\");\n        httpd_resp_send_500(req);\n        return ESP_FAIL;\n    }\n\n    // Encode the frame in base64\n    String base64Image = base64::encode(fb-&gt;buf, fb-&gt;len);\n\n    // Send the base64 encoded image\n    httpd_resp_set_type(req, \"text/plain\");\n    esp_err_t res = httpd_resp_send(req, base64Image.c_str(), base64Image.length());\n\n    // Return the frame buffer\n    esp_camera_fb_return(fb);\n\n    return res;\n}\n\n[...]\n\nhttpd_uri_t base64_uri = {\n        .uri = \"/base64\",\n        .method = HTTP_GET,\n        .handler = jpg_base64_handler,\n        .user_ctx = NULL\n\n[...]\n\nhttpd_register_uri_handler(camera_httpd, &amp;base64_uri);\n</code></pre> <p>A base64 string containing the data of a single frame captured by the ESP32CAM looks something like this:</p> <pre><code>\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 \n</code></pre> <p>With some modifications to OpenAI's sample code and the help of GPT-4o itself, I created the following script that would grab the base64 data from the custom handler and parse it into the model. The prompt for the model is \"You are a image to text OCR engine. Output the text you see in this image, and nothing else.\"</p> <pre><code>import requests\n\n# OpenAI API Key\napi_key = \"REDACTED\"\n\n# Function to get the base64 encoded image from ESP32CAM\ndef get_base64_image(url):\n    response = requests.get(url)\n    return response.text\n\n# URL to the ESP32CAM base64 image endpoint\nesp32cam_url = \"http://10.12.28.193/base64\"\n\n# Getting the base64 string\nbase64_image = get_base64_image(esp32cam_url)\n\nheaders = {\n  \"Content-Type\": \"application/json\",\n  \"Authorization\": f\"Bearer {api_key}\"\n}\n\npayload = {\n  \"model\": \"gpt-4o\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"You are a image to text OCR engine. Output the text you see in this image, and nothing else.\"\n        },\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\n            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n          }\n        }\n      ]\n    }\n  ],\n  \"max_tokens\": 300\n}\n\nresponse = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n\nprint(response.json())\n</code></pre> <p>With the same setup as the Raspberry Pi, where the ESP32CAM is pointed towards a paper with the words \"Hello World!\", I ran the script on my computer to test it out.</p> <p>Upon running, the following is printed out:</p> <p> </p> <p>As is seen in the output, the prompt worked and the 4o model detected \"Hello World!\" as the text extract and stored it in 'content'. To just output the content, which is the actual result I want, I can modify the code a little.</p> <pre><code>response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n\ncontent_string = response.json()['choices'][0]['message']['content']\nprint(content_string)\n</code></pre> <p>Here is the code just printing out the extracted OCR text.</p> <p> </p> <p>This was essentially the bare-bones version of my final project Pi code. After integrating the serial communication functionality, and changing the wording of the prompt, I had the final code for the Raspberry Pi ready:</p> <pre><code>from serial import Serial\nimport time\nimport requests\n\n# OpenAI API Key\napi_key = \"REDACTED\"\n\n# Configure the serial port\nser = Serial('/dev/serial0', 9600, timeout = 1)\n\n# Function to get the base64 encoded image from ESP32CAM\ndef get_base64_image(url):\n    response = requests.get(url)\n    print(\"Camera connected\")\n    return response.text\n\n# URL to the ESP32CAM base64 image endpoint\nesp32cam_url = \"http://10.12.23.1/base64\"\n\n# Getting the base64 string\nbase64_image = get_base64_image(esp32cam_url)\n\n# Function to send a message\ndef send_message(message):\n    ser.write(message.encode())  # Convert the message to bytes and send\n    time.sleep(1)                # Wait for a second\n\n# Example message used for testing purposes\n# testMessage = \"Hello world\"\n\ndef ocr():\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {api_key}\"\n    }\n\n    # Define the information sent to GPT4o\n    payload = {\n        \"model\": \"gpt-4o\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"You are a image to text OCR engine. Output the text you see in this image, and nothing else. Only use lowercase letters and no punctuation.\"\n                    },\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n                        }\n                    }\n                ]\n            }\n        ],\n        \"max_tokens\": 300\n    }\n\n    # Query the engine\n    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n\n    # Extract its response\n    return response.json()['choices'][0]['message']['content']\n\ntry:\n    message = ocr()\n    send_message(message)\n    print(f\"Message sent: {message}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\nfinally:\n    ser.close()  # Close the serial port\n</code></pre>"},{"location":"brailliant/#text-to-braille-mapping","title":"Text to Braille Mapping","text":"<p>The Raspberry Pi sends a byte-encoded text string to the ATTiny1614. From there, the ATTiny1614 is responsible for interpreting and converting the received text into braille dot arrays, which it then shows on the 3x2 array.</p> <p>The following program first sets up the configuration of solenodis and corresponding GPIOs, then creates the arrays of 0s and 1s (solenoid up or down) that forms a single braille character. It then maps each letter to its corresponding letter braille array.</p> <p>Upon the script's setup, each relevant pin is configured to be OUTPUT, and a serial connection with the Raspberry Pi is initialized. </p> <p>The script then continuously waits for an incoming string sent through Serial. Upon receiving the text, it is converted into an array of characters. The parse_input_string() method then does the heavy lifting and fetches the correct braille dot arrays. The method then iterates through each letter and represents that character's corresponding braille for one second on the physical solenoid array.</p> <p>The activate_solenoids() and deactivate_solenoids() methods were used for debugging purposes, turning all solenoids on or off, respectively.</p> <pre><code>/*\n  Solenoid arrangement:\n  0 1\n  2 3\n  4 5\n*/\n\nint sols[6] = {0, 1, 2, 3, 9, 8}; // Define the pins connected to the solenoids\n\n// Define the Braille arrays\nint a[6] = {0, 1, 1, 1, 1, 1};\nint b[6] = {0, 1, 0, 1, 1, 1};\nint c[6] = {0, 0, 1, 1, 1, 1};\nint d[6] = {0, 0, 1, 0, 1, 1};\nint e[6] = {0, 1, 1, 0, 1, 1};\nint f[6] = {0, 0, 0, 1, 1, 1};\nint g[6] = {0, 0, 0, 0, 1, 1};\nint h[6] = {0, 1, 0, 0, 1, 1};\nint i[6] = {1, 0, 0, 1, 1, 1};\nint j[6] = {1, 0, 0, 0, 1, 1};\nint k[6] = {0, 1, 1, 1, 0, 1};\nint l[6] = {0, 1, 0, 1, 0, 1};\nint m[6] = {0, 0, 1, 1, 0, 1};\nint n[6] = {0, 0, 1, 0, 0, 1};\nint o[6] = {0, 1, 1, 0, 0, 1};\nint p[6] = {0, 0, 0, 1, 0, 1};\nint q[6] = {0, 0, 0, 0, 0, 1};\nint r[6] = {0, 1, 0, 0, 0, 1};\nint s[6] = {1, 0, 0, 1, 0, 1};\nint t[6] = {1, 0, 0, 0, 0, 1};\nint u[6] = {0, 1, 1, 1, 0, 0};\nint v[6] = {0, 1, 0, 1, 0, 0};\nint w[6] = {1, 0, 0, 0, 1, 0};\nint x[6] = {0, 0, 1, 1, 0, 0};\nint y[6] = {0, 0, 1, 0, 0, 0};\nint z[6] = {0, 1, 1, 0, 0, 0};\n\nint space[6] = {1, 1, 1, 1, 1, 1};\n\nint number_sign[6] = {1, 0, 1, 0, 0, 0};\nint num_1[6] = {0, 1, 1, 1, 1, 1}; // Same as a\nint num_2[6] = {0, 1, 0, 1, 1, 1}; // Same as b\nint num_3[6] = {0, 0, 1, 1, 1, 1}; // Same as c\nint num_4[6] = {0, 0, 1, 0, 1, 1}; // Same as d\nint num_5[6] = {0, 1, 1, 0, 1, 1}; // Same as e\nint num_6[6] = {0, 0, 0, 1, 1, 1}; // Same as f\nint num_7[6] = {0, 0, 0, 0, 1, 1}; // Same as g\nint num_8[6] = {0, 1, 0, 0, 1, 1}; // Same as h\nint num_9[6] = {1, 0, 0, 1, 1, 1}; // Same as i\nint num_0[6] = {1, 0, 0, 0, 1, 1}; // Same as j\n\n// Define a structure to map characters to their Braille arrays\ntypedef struct {\n    char character;\n    int *braille_array;\n} BrailleMap;\n\n// Create the mapping\nBrailleMap braille_dictionary[] = {\n    {'a', a}, {'b', b}, {'c', c}, {'d', d}, {'e', e},\n    {'f', f}, {'g', g}, {'h', h}, {'i', i}, {'j', j},\n    {'k', k}, {'l', l}, {'m', m}, {'n', n}, {'o', o},\n    {'p', p}, {'q', q}, {'r', r}, {'s', s}, {'t', t},\n    {'u', u}, {'v', v}, {'w', w}, {'x', x}, {'y', y}, {'z', z}, {' ', space},\n    {'#', number_sign},\n    {'1', num_1}, {'2', num_2}, {'3', num_3}, {'4', num_4}, {'5', num_5},\n    {'6', num_6}, {'7', num_7}, {'8', num_8}, {'9', num_9}, {'0', num_0}\n};\n\nvoid setup() {\n    // Initialize the solenoid pins as output\n    for (int i = 0; i &lt; 6; i++) {\n        pinMode(sols[i], OUTPUT);\n    }\n  //  PORTMUX.CTRLB = 0x01;\n   Serial.begin(9600);\n\n   while(!Serial){\n\n   }\n}\n\nvoid activate_solenoids(int *braille_array) {\n    for (int i = 0; i &lt; 6; i++) {\n        digitalWrite(sols[i], braille_array[i]);\n    }\n}\n\n// Function to parse the input string and activate solenoids\nvoid parse_input_string(const char *input) {\n    int len = sizeof(braille_dictionary) / sizeof(BrailleMap);\n    for (int i = 0; i &lt; strlen(input); i++) {\n        for (int j = 0; j &lt; len; j++) {\n            if (braille_dictionary[j].character == input[i]) {\n                activate_solenoids(braille_dictionary[j].braille_array);\n                delay(1000); // Wait for a second before next character\n                break;\n            }\n        }\n    }\n    deactivate_solenoids();\n}\n\nvoid deactivate_solenoids() {\n    for (int i = 0; i &lt; 6; i++) {\n        digitalWrite(sols[i], LOW);\n    }\n}\n\nvoid loop() {\n  if (Serial.available() &gt; 0) {\n    // Read the incoming string\n\n    String input = Serial.readString();\n\n    // Convert the String to a C-style string (char array)\n    char inputArray[input.length() + 1];\n    input.toCharArray(inputArray, input.length() + 1);\n\n    // Call the parse_input_string function with the received string\n    parse_input_string(inputArray);\n\n    // Optional: Add a delay to avoid flooding the input\n    delay(5000); // Wait for 5 seconds before repeating\n  }\n}\n</code></pre>"},{"location":"brailliant/#assembly","title":"Assembly","text":"<p>I first outlined the general setup of my final project. I secured each MOSFET to a corresponding battery pack and solenoid, and color-coded each MOSFET's trigger and GND wires. I organized them in such a way that toggling solenoid 1, 2, 3, 4, 5, then 6 would control each solenoid in a line.</p> <p> </p> <p>I then connected the color coded wire sets to the controller PCB, and tested turning all the solenoids on in the right order.</p> <p> <p></p> <p>Lastly, I secured everything in place and put the cover on. This involved using Nitto tape to attach the PCB to the side wall, and using hot glue to secure the MOSFETs to the bottom and the solenoids to their respective places. The PCB on the side wall is circled in red in the image below, as it is somewhat difficult to see. Each MOSFET drive module is connected to the controller PCB through a set of respective color coded wire pairs. The top-left solenoid corresponds with both white wires, the top-right solenoid with dark blue, the mid-left solenoid with green, the mid-right solenoid with orange, the bottom-left solenoid with blue, and the bottom right solenoid with yellow. The color coded scheme makes it easier for me to detach individual wires for debugging and knowing which solenoid a specific wire corresponds to.</p> <p> </p> <p>I then ran another test to ensure everything still worked.</p> <p> <p></p> <p>The assembly of the Raspberry Pi case is relatively simple. As the 3D print already has holes built in for USB wires, and a hole built in for the screen, all I really need to do is secure the Pi to the bottom and the screen to the top, then connect the wires. I used 4 M3 screws in the screw-holes that I designed to secure the Raspberry Pi screen to the top of the case. I then used hot glue to secure the Pi in place on the bottom of the case. The following image shows the case's top on the left, with the screwed-in screen connected to the Pi via the microHDMI port for data and the power cable. The Pi itself is shown on the right, with its power cable coming out of the left side box hole, and the ESP32CAM cable coming out of the bottom hole. The wires on the right side case hole are used for connecting the Pi's TX/RX, VCC, and GND with the BrailleBox's ATTiny1614 PCB.</p> <p> </p> <p>Here is the Pi case when closed.</p> <p> </p>"},{"location":"brailliant/#evaluation","title":"Evaluation","text":"<p>My project is considered successful if it can:</p> <ul> <li>\u2611 Accurately extract text from a live image feed</li> <li>\u2611 Map the text to braille</li> <li>\u2611 Display the braille on the solenoid array</li> </ul>"},{"location":"brailliant/#implications","title":"Implications","text":"<p>There is existing technologies on the market that can convert text to braille in real time, but those are often expensive and not readily available to the public. My hope with this project is to create a product that can be cheaply produced and reach a wide audience. </p>"},{"location":"brailliant/#lessons-learned","title":"Lessons Learned","text":"<ul> <li>Some parts of a project will take longer while others will take shorter than expected.</li> <li>Double the planned allocation of time due to errors and debugging</li> <li>Working with lower-level hardware and software is more rewarding and often produces a more solid product</li> <li>There are many types of transistors, which can be a pain to sort through</li> </ul>"},{"location":"brailliant/#final-product","title":"Final Product","text":"<p>WARNING: The project video and slide (poster) are out of date and provide incorrect information about licensing. The current EULA is here. By installing, accessing, or using the Product, you acknowledge that you have read this Agreement, understand it, and agree to be bound by its terms and conditions.</p>"},{"location":"brailliant/#poster","title":"Poster","text":""},{"location":"brailliant/#video","title":"Video","text":"<p> <p></p>"},{"location":"brailliant/#file-downloads","title":"File Downloads","text":"<p>My files can be downloaded here.</p>"},{"location":"coding/","title":"Coding","text":""},{"location":"coding/#here-are-some-of-my-coding-projects","title":"Here are some of my coding projects:","text":""},{"location":"coding/#macro-maker","title":"Macro Maker","text":""},{"location":"conrad/","title":"BrailleBox - Conrad Spirit of Innovation Challenge","text":"<p>WARNING: The project video and slide (poster) are out of date and provide incorrect information about licensing. The current EULA is here. By installing, accessing, or using the Product, you acknowledge that you have read this Agreement, understand it, and agree to be bound by its terms and conditions.</p>"},{"location":"conrad/#overview","title":"Overview","text":"<p>I want to create a 3x2 solenoid array that can display braille characters by pushing solenoids up and down to create dots. This solenoid array will be connected to a Raspberry Pi, which in turn will be connected to an ESP32CAM. The camera will take a picture of a page of text, then perform OCR (optical character recognition) to extract a string of text from the image. That string of text will be converted to braille, which will be displayed on the solenoid array by flashing each character for 1 second at a time. This device will essentially allow for live-time conversion of any text into braille, which I hope will increase accessibility to books and the like.</p>"},{"location":"conrad/#brainstorming-process","title":"Brainstorming Process","text":""},{"location":"conrad/#initial-thoughts","title":"Initial Thoughts","text":"<p>My idea was to design a text to braille converter, which a blind person could use by moving the device over a page of text to convert it into braille. The braille translation of the English text would then be represented via a series of up/down pins which the user could use to interpret the information. The device was to be a rectangular box that would use an internal camera to interpret and OCR text, which could then be translated into braille and displayed via a series of servo motors pushing up metal rods on the top of the box. The pins would be in groups of six, each group representing a single braille character.</p> <p>However, I talked to Stuart Christhilf who had thought of a similar mechanism for his initial final project. He originally planned to create a dynamic clock to display the time using blocks of wood that acould be pushed out or pulled back via servos. However, when building his project, he realized that fitting so many servos into such a small space was completely unfeasible and warned me from doing the same. My initial design is shown in the following image:</p> <p> </p> <p>I then decided to use electromagnets for my pins, instead of a servo. The pins themselves would be a small magnetic rod sitting on top of an electromagnet. The small electromagnet could be powered on and off via a microcontroller. When the electromagnet was off, the pin would simply rest on top of the electromagnet, and the pin would be flush against the top of the board, forming the down position of the pin. If the pin needed to pop up, the microcontroller would power the electromagnet which would then emit a repelling magnetic charge. That magnetic force would then repel the pin slightly upwards, forming the up position of the pin. To represent a braille character, the microcontroller would push the specific pins into the up position that together would form the 6-dot pattern of the character.</p> <p>I also decided to move the camera out of the box. That would allow for more simple wiring and internal organization of the box, and allow the operator to more easily use the device. Moving the camera out means that the user would only need to move a small camera container across the page of text, instead of dragging the entire device. Here is my modified design:</p> <p> </p>"},{"location":"conrad/#significant-changes","title":"Significant Changes","text":"<p>Although a large part of my project remains the same, I've changed some aspects of my project. Namely, I've decided to use a Raspberry Pi as a central controller and connect it to 5 separate ATTiny412 chips, which will each be responsible for controlling 6 electromagnets to represent 1 braille character. Each ATTiny412 and 6 electromagnet setup will be on its own PCB, and receive data from the controlling Raspberry Pi. Additionally, I decided to create an elevated case for the ESP32 camera so that the image would have a better angle and thus an easier time being processed for OCR, and so that more light could come into the camera lens from the unobstructed sides. Lastly, I decided I wanted to wirelessly transmit data from the ESP32 camera to the Raspberry Pi for processing. I worked with both serial communication and WiFi connectivity previously so I hope to sum it all together and wirelessly transmit data between these two controllers.</p> <p>Here is an updated system diagram which maps out all the parts of my project.</p> <p> </p>"},{"location":"conrad/#feasibility","title":"Feasibility","text":"<p>However, after doing research, I realized that having 30 solenoids would be unfeasible. Instead, I decided to scale my project down to just having 6 solenoids, as this would still accomplish the mission of displaying braille for a reader. I would then flash each braille character for 1 second on the 6 solenoid array. This change allows me to worry less about power budget and ensures that I have a ready final project on my presentation date.</p>"},{"location":"conrad/#bill-of-materials","title":"Bill of Materials","text":""},{"location":"conrad/#components","title":"Components","text":""},{"location":"conrad/#braille-box-cad","title":"Braille Box CAD","text":""},{"location":"conrad/#initial-design","title":"Initial Design","text":"<p>I decided to first model my design in Fusion360, as I had prior experience working with Fusion and was pretty comfortable using it. When I started out with Autodesk Fusion, Kevin Kennedy's Fusion tutorials were a massive help.</p> <p>I first started off with a rectangular prism to act as the main body of the design.</p> <p> </p> <p>Next, I filleted the box to round out the edges.</p> <p> </p> <p>I then created a sketch on the top of the box, where I created six circles. These 6 circles represent the holes where I will put metal pins into that can pop up and down depending on what needs to be represented.</p> <p> </p> <p>I extruded the circles downward as holes. This creates the actual space where the pins will be placed.</p> <p> </p> <p>Finally, I used the pattern feature to repeat the sketch and extrusion across the top of the box. This created a total of 5 evenly spaced sets of 6 pins. With each set of 6 pins representing a single braille character, one iteration of pin setups can represent five letters.</p> <p> </p>"},{"location":"conrad/#improved-design","title":"Improved Design","text":"<p>As I had made my initial design early on, it did not reflect the changes I had made to my final project, most notably scaling down the amount of solenoids from 5 arrays of 3x2 to one 3x2 array. Additionally, when I made the original design, I didn't think much about how I would power the solenoid array and thus didn't include any spots for batteries. I also wanted to make the holes for the solenoids on a separate press-fit cover on top of the main box. Finally, the original design doesn't include any internal parts to hold the solenoids in place.</p> <p>For my new design, I want to make the following key changes:</p> <ul> <li>3x2 solenoid array</li> <li>Internal beams to support solenoids</li> <li>Battery pack holders</li> <li>Press-fit cover</li> </ul> <p>Additionally, I want the box to look as nice as possible and ideally have all wiring contained within it.</p> <p>I first started out by creating the shell of the box. I hollowed out the innards because I want my electronics to be inside. I will end up adhering my PCB to the side of the box and having my MOSFET breakouts on the bottom of the hollowed inside.</p> <p> </p> <p>I then started working on the top cover. I started out with creating a sketch where all my holes would be, and a offset on the edges to match the shell. I then extruded the sketch to create the cover with holes that the solenoids will fill.</p> <p> </p> <p>Based on my previous sketch offset for the edges of the shell box, I created quarter-circles and extruded them to form the press-fit lid.</p> <p> </p> <p>Next, I designed the beams that hold the solenoids in place. I started by creating a sketch on the bottom of the shell box and extruded that to my desired height. I then created a sketch on the extruded rectangular prism to remove the bottom part of it and form it into a beam-like shape.</p> <p> </p> <p>I then started work on the battery holders. I created the bottom of the battery holder then extruded out the sides.</p> <p> </p> <p>Next, I created the dividers to firmly hold each battery pack in place. Each divided section has the same length and width dimensions as the actual battery pack that I will use, plus a little for tolerance.</p> <p> </p> <p>I then added a small hole on the side for the power, ground, and TX/RX cables for the ATTiny1614.</p> <p> </p> <p>Next, I extruded a small hole as a slot for the wires on the external battery packs to route into the main shell box, where it will be connected to the MOSFETs controlling the solenoids.</p> <p> </p> <p>Finally, I added fillets. Here is the final box design.</p> <p> </p>"},{"location":"conrad/#raspberry-pi-box-cad","title":"Raspberry Pi Box CAD","text":"<p>I first started off with a shelled box.</p> <p> </p> <p>I then added a lid, with a hole the same size as my 5 inch touchscreen where I would attach the screen.</p> <p> </p> <p>Next, I added screw holes on the lid. These screw holes allowed me to secure the screen to the 3D printed lid. Unfortunately, my screw holes actually ended up being a little small so I had to enlarge them after the print with the help of a soldering iron.</p> <p> </p> <p>I added legs to the lid to allow it to press-fit into the base.</p> <p> </p> <p>Then, I added holes for wires on the front and side of the Pi case.</p> <p> </p> <p>Finally, I added fillets all around.</p> <p> </p> <p>After printing that initial iteration, the screen fit and there was enough space inside to fit the Raspberry Pi. However, some of my cables didn't fit as they had long \"necks\" that had to remain straight. As such, I would have to significantly bend the HDMI and USB cords for the Raspberry Pi.</p> <p> </p> <p>As such, I adjusted the length of the box to give space for the USB cable necks. I also slightly decreased the height of the holes for the USB cables as they were larger than necessary and somewhat an eyesore.</p> <p> </p>"},{"location":"conrad/#electronics","title":"Electronics","text":"<p>Electronics were by far the worst part of this project, at least for me. The main issue was that I didn't understand transistors very well, and I ran into a bunch of problems with them. The two main problems I ran into were transistors not being able to handle the power and transistors having inconsistent pinouts and being backwards or jumbled around.</p> <p>In this section, I'll go through a few of the boards that didn't work then show my final board.</p> <p>This was my initial board design. I tested this board by plugging a solenoid into the top pin, and the resulting lack of transistor is visible. The transistor heated up after around 5 seconds and fell off the board, without powering the solenoid.</p> <p> </p> <p>This is my second iteration. I added what I thought were pull down resistors hidden under the left-hand side white female pin headers (they did not, in fact, function as pull down resistors) and a power indicator LED. Unfortunately, I forgot the capacitor, but that would not have affected this board's outcome of failure. When creating this board, I also ran into major issues with the ATTiny1614, which stuck me for a couple hours. Apparently, some of the ATTiny1614 chips in our lab just didn't work, so I needed to get the ATTiny1614s out of a specific drawer because those chips had a small dot indentation on one side. Only the chips with the dot indentation seemed to work well, in my experience. Upon testing, the transistor got really hot and I unplugged it before it melted off.</p> <p> </p> <p>In an attempt to simplify the amount of things that could cause the issue, I scaled down to one transistor, which in turn melted off.</p> <p> </p> <p>I tweaked the design with one transistor, and it melted again.</p> <p> </p> <p>At this point, I created a ATTiny1614 board for testing and debugging.</p> <p> </p> <p>I created another board with a pull down and headers to plug into external transistors. The hope was that this would allow me to test transistors without melting pads and traces.</p> <p> </p> <p>At this point, I was fairly certain the transistor had a problem with handling power. I switched to the Eugepae board which had a different transistor. I had made this board during a group project for embedded networking and communications, and it had previously handled 5V, so I was pretty confused when it failed to power my 5V solenoids. Unfortunately, this board also failed, in retrospect likely because the solenoids pulled too many amps.</p> <p> </p> <p>I then decided to switch to a through hole MOSFET. This board also failed, which I'm pretty confused about but will explain in the next paragraph.</p> <p> </p> <p>After all these boards failed, I found MOSFET drive modules in the Lab. These modules have six key inputs: VIN+, VIN-, VOUT+, VOUT-, TRIG, and GND. I ended up connecting the VIN+ and VIN- to the positive and ground terminals on the power input device (battery packs), the VOUT+ and VOUT- to the positive and ground of the load output device (solenoid), the TRIG pin to a GPIO on my ATTiny1614 board that would toggle the solenoid on and off, and the GND to ground. The architecture of the drive module (which worked) was really similar to some of my MOSFET attempts, so I am still a little unsure why this board worked when my own didn't. The only major discrepancy that I noticed was that this board had 2 transistors.</p> <p> </p> <p>Now that I decided to use the MOSFET breakout boards to toggle the solenoids, the MOSFETs are external to the main board and I am able to create a ATTiny1614 control board without transistors on it. My final board has headers for its power, ground, and data; a power indicator LED; a capacitor; pins for TX/RX serial communication with the Raspberry Pi, which will send text to be displayed as braille; 6 headers each corresponding to a GPIO pin on the ATTiny1614 which in turn, corresponds to controlling a single solenoid in the 3x2 array; and 6 headers for GND that will each connect to 1 MOSFET.</p> <p> </p>"},{"location":"conrad/#esp32cam-wireless-transmission","title":"ESP32CAM Wireless Transmission","text":"<p>WebSocket connections are initiated through HTTP protocol, using an upgrade request from HTTP to WebSocket. This begins with a client sending a standard HTTP request that includes an \"Upgrade: websocket\" header and a \"Connection: Upgrade\" header to the server. The server then responds with an HTTP 101 status code, indicating that the protocol will change, thus establishing the WebSocket connection.</p> <p>WebSocket, uses IP addresses to facilitate the initial connection before upgrading to the WebSocket protocol. Once the WebSocket connection is established, the IP addresses are used to maintain the connection over which data frames can be reliably transmitted back and forth.</p> <p>Once the WebSocket connection is established, data is transmitted in framed messages through backend data transmission ports, where each frame consists of an opcode to indicate the type of data being transmitted (e.g., text, binary, continuation frame, or control frames like close, ping, or pong). This structure allows the WebSocket protocol to be extremely versatile and efficient in handling different types of data seamlessly. The frames are small and allow for very efficient data transmission.</p> <p>The following program is uploaded onto the ESP32 CAM Board through Arduino IDE. This program is based off of the CameraWebServer example program from ESP32.</p> <pre><code>#include \"esp_camera.h\"\n#include \"WiFi.h\"\n#include \"WebSocketsServer.h\"\n\n#define CAMERA_MODEL_AI_THINKER // Has PSRAM\n#include \"camera_pins.h\"\n\nconst char* ssid = \"REDACTED\";\nconst char* password = \"REDACTED\";\n\nWebSocketsServer webSocket = WebSocketsServer(81);\n\nvoid startCameraServer();\nvoid setupLedFlash(int pin);\nvoid onWebSocketEvent(uint8_t client_num, WStype_t type, uint8_t *payload, size_t length);\n\nvoid setup() {\n  pinMode(2, OUTPUT);\n  Serial.begin(9600);\n  while (!Serial); // Wait for the serial connection to initialize\n  Serial.setDebugOutput(true);\n  Serial.println();\n\n  camera_config_t config;\n  config.ledc_channel = LEDC_CHANNEL_0;\n  config.ledc_timer = LEDC_TIMER_0;\n  config.pin_d0 = Y2_GPIO_NUM;\n  config.pin_d1 = Y3_GPIO_NUM;\n  config.pin_d2 = Y4_GPIO_NUM;\n  config.pin_d3 = Y5_GPIO_NUM;\n  config.pin_d4 = Y6_GPIO_NUM;\n  config.pin_d5 = Y7_GPIO_NUM;\n  config.pin_d6 = Y8_GPIO_NUM;\n  config.pin_d7 = Y9_GPIO_NUM;\n  config.pin_xclk = XCLK_GPIO_NUM;\n  config.pin_pclk = PCLK_GPIO_NUM;\n  config.pin_vsync = VSYNC_GPIO_NUM;\n  config.pin_href = HREF_GPIO_NUM;\n  config.pin_sscb_sda = SIOD_GPIO_NUM;\n  config.pin_sscb_scl = SIOC_GPIO_NUM;\n  config.pin_pwdn = PWDN_GPIO_NUM;\n  config.pin_reset = RESET_GPIO_NUM;\n  config.xclk_freq_hz = 20000000;\n  config.frame_size = FRAMESIZE_UXGA;\n  config.pixel_format = PIXFORMAT_JPEG; \n  config.grab_mode = CAMERA_GRAB_WHEN_EMPTY;\n  config.fb_location = CAMERA_FB_IN_PSRAM;\n  config.jpeg_quality = 12;\n  config.fb_count = 1;\n\n  if (psramFound()) {\n    config.jpeg_quality = 10;\n    config.fb_count = 2;\n    config.grab_mode = CAMERA_GRAB_LATEST;\n  }\n\n  esp_err_t err = esp_camera_init(&amp;config);\n  if (err != ESP_OK) {\n    Serial.printf(\"Camera init failed with error 0x%x\", err);\n    return;\n  }\n\n  sensor_t *s = esp_camera_sensor_get();\n  s-&gt;set_vflip(s, 1); // Flip it back\n  s-&gt;set_brightness(s, 1); // Up the brightness just a bit\n  s-&gt;set_saturation(s, -2); // Lower the saturation\n\n#if defined(LED_GPIO_NUM)\n  setupLedFlash(LED_GPIO_NUM);\n#endif\n\n  WiFi.begin(ssid, password);\n  WiFi.setSleep(false);\n  while (WiFi.status() != WL_CONNECTED) {\n    delay(500);\n    Serial.print(\".\");\n  }\n\n  Serial.println(\"\");\n  Serial.println(\"WiFi connected\");\n  webSocket.begin();\n  webSocket.onEvent(onWebSocketEvent);\n  startCameraServer();\n\n  Serial.print(\"Camera Ready! Use 'http://\");\n  Serial.print(WiFi.localIP());\n  Serial.println(\"' to connect\");\n}\n\nvoid loop() {\n  webSocket.loop();\n}\n\nvoid onWebSocketEvent(uint8_t client_num, WStype_t type, uint8_t *payload, size_t length) {\n  switch (type) {\n    case WStype_DISCONNECTED:\n      Serial.printf(\"[%u] Disconnected!\\n\", client_num);\n      break;\n    case WStype_CONNECTED:\n      {\n        IPAddress ip = webSocket.remoteIP(client_num);\n        Serial.printf(\"[%u] Connection from \", client_num);\n        Serial.println(ip.toString());\n      }\n      break;\n    case WStype_TEXT:\n      if (strcmp((char *)payload, \"capture\") == 0) {\n        camera_fb_t *fb = esp_camera_fb_get();\n        if (!fb) {\n          Serial.println(\"Camera capture failed\");\n        } else {\n          webSocket.sendBIN(client_num, fb-&gt;buf, fb-&gt;len);\n          esp_camera_fb_return(fb);\n        }\n      }\n      break;\n    case WStype_BIN:\n      Serial.printf(\"[%u] Get binary length: %u\\n\", client_num, length);\n      break;\n  }\n}\n\nvoid setupLedFlash(int pin) {\n  pinMode(pin, OUTPUT);\n  digitalWrite(pin, LOW);\n}\n</code></pre> <p>This program connects the ESP32CAM to a local WiFi network. It then sets up and initializes the camera, and sets up the local IP connection. It then continuously waits for a web socket connection. When a connection is created, it prints the IP address of the connecting device. If the device sends an input of \"capture\", the camera will take a picture and send it via the network web socket connection to the connecting Raspberry Pi.</p>"},{"location":"conrad/#camera-feed-ocr","title":"Camera Feed OCR","text":"<p>I had previously setup infrastructure to wirelessly transmit a command to capture an image from a Raspberry Pi to the ESP32CAM, along with sending the image data back over the network and saving it. I had created a WebSocket server to accept commands and then send the image data over HTTP back to the Raspberry Pi.</p> <p>However, I realized that I could fetch the image without needing a WebSocket handler by connecting to the ESP32CAM's capture image handler directly. The capture handler from the default CameraWebServer example project sets up a port that allows a direct download to what is currently on the camera feed.</p> <pre><code>static esp_err_t capture_handler(httpd_req_t *req)\n{\n    camera_fb_t *fb = NULL;\n    esp_err_t res = ESP_OK;\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n    int64_t fr_start = esp_timer_get_time();\n#endif\n\n#if CONFIG_LED_ILLUMINATOR_ENABLED\n    enable_led(true);\n    vTaskDelay(150 / portTICK_PERIOD_MS); // The LED needs to be turned on ~150ms before the call to esp_camera_fb_get()\n    fb = esp_camera_fb_get();             // or it won't be visible in the frame. A better way to do this is needed.\n    enable_led(false);\n#else\n    fb = esp_camera_fb_get();\n#endif\n\n    if (!fb)\n    {\n        log_e(\"Camera capture failed\");\n        httpd_resp_send_500(req);\n        return ESP_FAIL;\n    }\n\n    httpd_resp_set_type(req, \"image/jpeg\");\n    httpd_resp_set_hdr(req, \"Content-Disposition\", \"inline; filename=capture.jpg\");\n    httpd_resp_set_hdr(req, \"Access-Control-Allow-Origin\", \"*\");\n\n    char ts[32];\n    snprintf(ts, 32, \"%lld.%06ld\", fb-&gt;timestamp.tv_sec, fb-&gt;timestamp.tv_usec);\n    httpd_resp_set_hdr(req, \"X-Timestamp\", (const char *)ts);\n\n#if CONFIG_ESP_FACE_DETECT_ENABLED\n    size_t out_len, out_width, out_height;\n    uint8_t *out_buf;\n    bool s;\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n    bool detected = false;\n#endif\n    int face_id = 0;\n    if (!detection_enabled || fb-&gt;width &gt; 400)\n    {\n#endif\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n        size_t fb_len = 0;\n#endif\n        if (fb-&gt;format == PIXFORMAT_JPEG)\n        {\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n            fb_len = fb-&gt;len;\n#endif\n            res = httpd_resp_send(req, (const char *)fb-&gt;buf, fb-&gt;len);\n        }\n        else\n        {\n            jpg_chunking_t jchunk = {req, 0};\n            res = frame2jpg_cb(fb, 80, jpg_encode_stream, &amp;jchunk) ? ESP_OK : ESP_FAIL;\n            httpd_resp_send_chunk(req, NULL, 0);\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n            fb_len = jchunk.len;\n#endif\n        }\n        esp_camera_fb_return(fb);\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n        int64_t fr_end = esp_timer_get_time();\n#endif\n        log_i(\"JPG: %uB %ums\", (uint32_t)(fb_len), (uint32_t)((fr_end - fr_start) / 1000));\n        return res;\n#if CONFIG_ESP_FACE_DETECT_ENABLED\n    }\n\n    jpg_chunking_t jchunk = {req, 0};\n\n    if (fb-&gt;format == PIXFORMAT_RGB565\n#if CONFIG_ESP_FACE_RECOGNITION_ENABLED\n     &amp;&amp; !recognition_enabled\n#endif\n     ){\n#if TWO_STAGE\n        HumanFaceDetectMSR01 s1(0.1F, 0.5F, 10, 0.2F);\n        HumanFaceDetectMNP01 s2(0.5F, 0.3F, 5);\n        std::list&lt;dl::detect::result_t&gt; &amp;candidates = s1.infer((uint16_t *)fb-&gt;buf, {(int)fb-&gt;height, (int)fb-&gt;width, 3});\n        std::list&lt;dl::detect::result_t&gt; &amp;results = s2.infer((uint16_t *)fb-&gt;buf, {(int)fb-&gt;height, (int)fb-&gt;width, 3}, candidates);\n#else\n        HumanFaceDetectMSR01 s1(0.3F, 0.5F, 10, 0.2F);\n        std::list&lt;dl::detect::result_t&gt; &amp;results = s1.infer((uint16_t *)fb-&gt;buf, {(int)fb-&gt;height, (int)fb-&gt;width, 3});\n#endif\n        if (results.size() &gt; 0) {\n            fb_data_t rfb;\n            rfb.width = fb-&gt;width;\n            rfb.height = fb-&gt;height;\n            rfb.data = fb-&gt;buf;\n            rfb.bytes_per_pixel = 2;\n            rfb.format = FB_RGB565;\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n            detected = true;\n#endif\n            draw_face_boxes(&amp;rfb, &amp;results, face_id);\n        }\n        s = fmt2jpg_cb(fb-&gt;buf, fb-&gt;len, fb-&gt;width, fb-&gt;height, PIXFORMAT_RGB565, 90, jpg_encode_stream, &amp;jchunk);\n        esp_camera_fb_return(fb);\n    } else\n    {\n        out_len = fb-&gt;width * fb-&gt;height * 3;\n        out_width = fb-&gt;width;\n        out_height = fb-&gt;height;\n        out_buf = (uint8_t*)malloc(out_len);\n        if (!out_buf) {\n            log_e(\"out_buf malloc failed\");\n            httpd_resp_send_500(req);\n            return ESP_FAIL;\n        }\n        s = fmt2rgb888(fb-&gt;buf, fb-&gt;len, fb-&gt;format, out_buf);\n        esp_camera_fb_return(fb);\n        if (!s) {\n            free(out_buf);\n            log_e(\"To rgb888 failed\");\n            httpd_resp_send_500(req);\n            return ESP_FAIL;\n        }\n\n        fb_data_t rfb;\n        rfb.width = out_width;\n        rfb.height = out_height;\n        rfb.data = out_buf;\n        rfb.bytes_per_pixel = 3;\n        rfb.format = FB_BGR888;\n\n#if TWO_STAGE\n        HumanFaceDetectMSR01 s1(0.1F, 0.5F, 10, 0.2F);\n        HumanFaceDetectMNP01 s2(0.5F, 0.3F, 5);\n        std::list&lt;dl::detect::result_t&gt; &amp;candidates = s1.infer((uint8_t *)out_buf, {(int)out_height, (int)out_width, 3});\n        std::list&lt;dl::detect::result_t&gt; &amp;results = s2.infer((uint8_t *)out_buf, {(int)out_height, (int)out_width, 3}, candidates);\n#else\n        HumanFaceDetectMSR01 s1(0.3F, 0.5F, 10, 0.2F);\n        std::list&lt;dl::detect::result_t&gt; &amp;results = s1.infer((uint8_t *)out_buf, {(int)out_height, (int)out_width, 3});\n#endif\n\n        if (results.size() &gt; 0) {\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n            detected = true;\n#endif\n#if CONFIG_ESP_FACE_RECOGNITION_ENABLED\n            if (recognition_enabled) {\n                face_id = run_face_recognition(&amp;rfb, &amp;results);\n            }\n#endif\n            draw_face_boxes(&amp;rfb, &amp;results, face_id);\n        }\n\n        s = fmt2jpg_cb(out_buf, out_len, out_width, out_height, PIXFORMAT_RGB888, 90, jpg_encode_stream, &amp;jchunk);\n        free(out_buf);\n    }\n\n    if (!s) {\n        log_e(\"JPEG compression failed\");\n        httpd_resp_send_500(req);\n        return ESP_FAIL;\n    }\n#if ARDUHAL_LOG_LEVEL &gt;= ARDUHAL_LOG_LEVEL_INFO\n    int64_t fr_end = esp_timer_get_time();\n#endif\n    log_i(\"FACE: %uB %ums %s%d\", (uint32_t)(jchunk.len), (uint32_t)((fr_end - fr_start) / 1000), detected ? \"DETECTED \" : \"\", face_id);\n    return res;\n#endif\n}\n</code></pre> <p>I then set up the Raspberry Pi to receive an image from the ESP32CAM and perform OCR upon it.</p> <p>First, I created a directory to store this project.</p> <pre><code>cd Desktop\nmkdir ocr\n</code></pre> <p>Upon entering the new directory, I need to create a virtual environment to install the libraries I will be using for OCR.</p> <pre><code>python -m venv /virtual\n</code></pre> <p>However, running this command gave me an error. </p> <pre><code>Error: [Errno13] Permission denied: '/virtual'\n</code></pre> <p>For some reason, this command didn't have the permissions to create a new virtual environment, which was strange considering that the project directory was not protected in any way. Regardless, I attached the sudo prefix and successfully created the virtual environment.</p> <pre><code>sudo python -m venv /virtual\n</code></pre> <p>I then entered the virtual environment by activating it.</p> <pre><code>source bin/activate\n</code></pre> <p>The bin/activate is a relative path and would activate the venv as long as I am in the \"ocr\" folder. However, the venv could also be activated by supplying the absolute path of \"~/home/richard/Desktop/ocr/bin/activate\".</p>"},{"location":"conrad/#pytesseract","title":"PyTesseract","text":"<p>After activating the virtual environment, I can install all of my library dependencies.</p> <pre><code>sudo pip install pytesseract\nsudo pip install opencv-python\n</code></pre> <p>I then created the actual program that the Raspberry Pi would run.</p> <pre><code>import time\nimport cv2\nimport urllib.request\nimport numpy as np\nimport pytesseract\n\nurl = 'http://10.12.28.193/capture'\n\nimg_resp = urllib.request.urlopen(url)\nimgnp = np.array(bytearray(img_resp.read()), dtype=np.uint8)\nframe = cv2.imdecode(imgnp, -1)\n\ntext = pytesseract.image_to_string(frame, config='--psm 7')\n\nprint(\"Extracted Text:\", text)\ntime.sleep(1)\n</code></pre> <p>This script has the Raspberry Pi connect to the /capture handler of the ESP32CAM interface, which directly returns a capture of the current feed. It then decodes the image and parses it into the pytesseract OCR function. The PSM value of 6 tells the OCR model to scan the image for a single text block and extract text from that. A full list of PSM value options can be found by running <code>tesseract --help-psm</code> in the terminal.</p> <pre><code>  0    Orientation and script detection (OSD) only.\n  1    Automatic page segmentation with OSD.\n  2    Automatic page segmentation, but no OSD, or OCR. (not implemented)\n  3    Fully automatic page segmentation, but no OSD. (Default)\n  4    Assume a single column of text of variable sizes.\n  5    Assume a single uniform block of vertically aligned text.\n  6    Assume a single uniform block of text.\n  7    Treat the image as a single text line.\n  8    Treat the image as a single word.\n  9    Treat the image as a single word in a circle.\n 10    Treat the image as a single character.\n 11    Sparse text. Find as much text as possible in no particular order.\n 12    Sparse text with OSD.\n 13    Raw line. Treat the image as a single text line, bypassing hacks that are Tesseract-specific.\n</code></pre> <p>In my case, since I want the model to scan an image to find the line of text for \"Hello World!\", I will use psm-7.</p> <p>Here is a photo of my Raspberry Pi setup.</p> <p> </p> <p>The ESP32CAM is pointed towards a paper with the words \"Hello World!\". In the right side of the picture, the Raspberry Pi which is running the code is visible along with the display. Upon running the program on the Pi's terminal, the ESP32CAM takes a picture and transmits it to the Pi, which then uses tesseract to perform OCR on it and prints out the extracted text.</p> <p> <p></p>"},{"location":"conrad/#gpt4o","title":"GPT4o","text":"<p>At this point, I wanted to try to use as little computational power as possible, and thus decided to switch to processing my image in base64. Although switching to base64 ultimately failed to scale down the computing enough to run on a microcontroller, it still led me in an interesting direction: that I could use GPT4o's new multimodal capabilities as an OCR engine to extract text from the base64 image. GPT4o in general is much more accurate in OCR than pytesseract, hence the switch.</p> <p>To do this, I first created a new handler on the ESP32CAM that would have it return a base64 string of a capture of the camera feed when that handler is called.</p> <pre><code>static esp_err_t jpg_base64_handler(httpd_req_t *req) {\n    camera_fb_t *fb = esp_camera_fb_get();\n    if (!fb) {\n        Serial.println(\"Camera capture failed\");\n        httpd_resp_send_500(req);\n        return ESP_FAIL;\n    }\n\n    // Encode the frame in base64\n    String base64Image = base64::encode(fb-&gt;buf, fb-&gt;len);\n\n    // Send the base64 encoded image\n    httpd_resp_set_type(req, \"text/plain\");\n    esp_err_t res = httpd_resp_send(req, base64Image.c_str(), base64Image.length());\n\n    // Return the frame buffer\n    esp_camera_fb_return(fb);\n\n    return res;\n}\n\n[...]\n\nhttpd_uri_t base64_uri = {\n        .uri = \"/base64\",\n        .method = HTTP_GET,\n        .handler = jpg_base64_handler,\n        .user_ctx = NULL\n\n[...]\n\nhttpd_register_uri_handler(camera_httpd, &amp;base64_uri);\n</code></pre> <p>A base64 string containing the data of a single frame captured by the ESP32CAM looks something like this:</p> <pre><code>\n/9j/4AAQSkZJRgABAQEAAAAAAAD/2wBDAAoHCAkIBgoJCAkLCwoMDxkQDw4ODx8WFxIZJCAmJiQgIyIoLToxKCs2KyIjMkQzNjs9QEFAJzBHTEY/Szo/QD7/2wBDAQsLCw8NDx0QEB0+KSMpPj4+Pj4+Pj4+Pj4+Pj4+Pj4+Pj4+Pj4+Pj4+Pj4+Pj4+Pj4+Pj4+Pj4+Pj4+Pj4+Pj7/xAAfAAABBQEBAQEBAQAAAAAAAAAAAQIDBAUGBwgJCgv/xAC1EAACAQMDAgQDBQUEBAAAAX0BAgMABBEFEiExQQYTUWEHInEUMoGRoQgjQrHBFVLR8CQzYnKCCQoWFxgZGiUmJygpKjQ1Njc4OTpDREVGR0hJSlNUVVZXWFlaY2RlZmdoaWpzdHV2d3h5eoOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4eLj5OXm5+jp6vHy8/T19vf4+fr/xAAfAQADAQEBAQEBAQEBAAAAAAAAAQIDBAUGBwgJCgv/xAC1EQACAQIEBAMEBwUEBAABAncAAQIDEQQFITEGEkFRB2FxEyIygQgUQpGhscEJIzNS8BVictEKFiQ04SXxFxgZGiYnKCkqNTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqCg4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2dri4+Tl5ufo6ery8/T19vf4+fr/wAARCADwAUADASEAAhEBAxEB/9oADAMBAAIRAxEAPwDF8mMfwCk8iP8AuD8aHYzshPJi/wCea0vlJ02Ci49AEMX/ADzWnGKL/nmlG4WFMUZ/gWk+zxf3BQtCbDvKj/55rSeTH/cWkXy6C+VH/cWk8qP+4tOw+UZ5Uf8AzzWjy4s/6paGTbUAkXaNaXyo/wDnmtIu1xdkf/PMUwxR5+4KdhWGNHH/AHBUZjT+4KVhqBGY1z0FRtGv90U7ILEYjTHKioCqf3aA5RhCf3RUWwZ6UCY3yx2FNKD0FJ2HZETIOajYZ5p+YWGkU3C55ot1DzEKik2iqC5GdtMOKncLCU3vVIAxxRtFPoSeinrTTmi5ImDSiluhhRS2AfS0DAilpIYmKKYDSvekxQAY/WlAqbh0FxSbeKbHbqMYYqIjvTERsKhIpD5tSJ6iIo8iiJqiJ5p9CWJUT0twZEaYRQSMJpueKYxGpmaoRE5pKQxM+lFDFYD0pMU+g4nop69KTbUy0JE707vRfQoXFLQA7Bo24pIlbhilxTHcdikxU6jvcaRTaYahilpdRhtpcUwGFKgkGKLAyNgahZTQKzIWU5qNgaLjISmajMfGaYCbahdDSW4EO00myiwxmymlaYkMNRGhaMBpBzUeD3qmxWFFOpAO209Y81V7AegN14pNtJi3YY4x3p2OagYu2lC07CHYpduTSaGhdlASmHoLspfLpIA8uk8qhjDyqPKqQHiKneVQBGYqieGquBGYajaKpT7gQtFUEkIoe4xnlComi9KV9RiNHiqzJTFuQMlRkU76DGlRUTUdCblc9aZV9AuIajYUCALU6pQwJ0hqwkFFgZ25j5pPLpXFqxRFTvLpXGO2U4R0XEP8ujy6Qx3l0bKBhspdlIA20m2hAGKMUxi4pcVIhMVDJQBCaiamMhaq0lIERN1qFielOw2RvwKqtVCIWNRtQLqRFsdagc0CZHTMUwsGKcEz2qgLEVux7VcitfWpb1Hctpb8VKsNUZnWlaTbUSLF207FIAxS0wFp1ABSUhhS0wDFIRSATFGKLgOxS4qRjSKhejYRXbiompjIGqBqQyOoD1zQtxEMzVTLVVgIi3NRmgCB2qImhMQ2kq9AJoYcnpV+CzJpPUGaEVpjirCW9NGZL5W2kK0XQmdDRUmiFo70CHUUDHYp1IAxR1oQwxS0wEpKkBKKAJBRikA1hxUDijcZXcVAQaBkLKahZDSDyGeUaiKcU2h+RVljqqyUXJIWUVXkYVcQKxptOwh6oWNXbayzQ9BGrbWHc9Kvx22KZLZN5eKCtCYiM1GcUehJ0G2jFJmguKdikAuKXFAx2KXbSHYXbRtoAXZRsoANlJspAHl04JUgO2U7ZQMQqKqyAUAV2xUTYxTGRNioH60DI81WlbANBJnSyHNVXehbgivI9Vjk1omIZVmC1eU8CgDZstMI54rWiswMGkZvUtCICjFMCNqYaAI6jaqEdFijFItDgtLsqRjttO2UCHhadtoKFwKMUALRSASkoYDaKkYtLQA1hVZ1pjIGSoylSIjMdQstAETLxVK44U01sBmSVVkIqkK5Tc5zQkbMQFGasDVsdKeRgWBFdBbaaqDmpEy6sIWnYp2IIzTDQBG1MNMCI1G1NCOoxQBUmg4UuKQC4pwFAxQtP20MQu2jZSAXbRsoGJtpu2kAYpNtJDDFPUUAMcYqo+aGxkLK1NKnFK4xhWoXFFxEDCs67Uf3sU09BGXLxVGVstVxES2djLdSDYPl9a6ew0OOHl1Bak7gayW6oOKXFMkaaaaCSM0xqAI2qNqaAiNRNTEdbS4qSxfwpQKBi4p22hgPC07bSGO2UuygB2yl2ikAuxaTaKkBhApMUDG7aeg9qAI5lOOlZk24GkBWZ2qJpmxQUQNOarvOfWmBH52Wxmql/wDfNNEmRKctgc/StfS/D8kpWa44XqBVXEdTDaQ267YkCipDQSMNMNMBhplAhhqM0CIzUZpgRN0qJqYjsaWkaC06kwFp1IY5afigBcUUAPpM+1IB1FIY2kpAN5p6ZzQAy53YrIuFkz1oQFJ0f+9VZoGP8VAELWvqzfgage1pXsURC02tmobu0aaVQmN7U79RM2NI0CO1xNdAST9vQVt4qiRKYaYiM1GaYhppho0AjNMoAjaoiKaJIzUTCmDOypaRQtOpDFxTqQDlFSYoGLtp+KAFApdtSA4CkxSGNIpuKEBG7hetJDcwvL5aSBn9KBXK2sanZaY8K38zRGZSyYTdnFc5P4m00/c+1MfTycU7Me5RbxHbNIES2uDnvlasQyaldRCW10i4eNuj7xinZWu2P1JBaa23/MMVfd5xS/2drJPMFmv/AG1qNAEfS9T8tj59nux90I1WLFoHuf3O1iIxlxzzjmgk1QMCirENphpgRGmGmIZSEUhEZFMNMZGxqJjTEQtTM56UEncG3kz0H50otpPQfnQUO+yy+g/Oni0f2pDF+yN/eFO+y/7VIZIlqM8tVhbFO7vRqBILOHHO8/8AAqeLWIfw/maY7i+RF/zzWlEa/wB1fyqbALsHoKQigRA9V3FAGddjg1nWLCK/SV+FAOaTGiLxLapr2oWckBcRwQNGfUktmi08KrJaqkVqZPLILMalyuhpWNJfD8ydI8e22rMdu0NqkZ7VAxPJzSGCqEItruccGsG00yaxum8xo8OM4X61Yrmlil2n0oEMNRGmFyN+O1RlvaqFcauWBqIk5oEQu4HVgPqageeNfvTIPxpgRfaYf+fiOq7Xtv3k/JadhEDXkZ/vUqXsH8W4H6UhnqZFApgPpakYYooGOXrVkUCHUtAwpKBi4pjVAIheqVxNHD988noKYdTPVLi/kCwJ1rWh8O29sqzapcpEPRj1qHqXsQa9r+m+GUthb6ZJePcbtrfdXiucm+JGomC7VNMtI5JeI3EhOytYwXUh6lO6+JPiR/8AUx6bAPURsxqrF8QdbZMXdtpt0/8AfZCtJwQrit471V/uW1lD/uRbqjfxnrp/5fMf7sSCnGPKDsynP4q1toZC2qXf3T0kArZ8UX95af2V9luWTzrNWY4BJOBzT059ECVjCOsaoB/yErj/AMdqM6tqjHnUrn86VtBaCjVtTGf9PmOf72DUTX98T/x/T/gaYWQxr26b791M31eomnlb70sh/wCBUra3Aj3t/eNMY+taANzR5nNSSR+bz1o87jrRYY3zs96b5vvTSYHuZ60oqAHUtAxaKAHL96pxQMdS0ALQaQC1GxABJOAKBmRPfPNKIbFST/fq7ZaCqJ9o1J9q/wC0ah9gJLzVfs8flaVEsQ/56sOa5uPdNqUc07vNJuHzOc0dAQ3x9Bv0G0uf4oLnZ+DA1wEnBrSF7CK0p+Wq6Hk1QWJUqQZIoAjuF/0eT/dNdL4smBTQ1H3v7PX/ANlqOoMwGyq/NxUfmj+8KaYMYZ/92mGc0PUViMzN/e/Smec/96n1Cwxp2Heozcvn/wCvTHYT7Q1MMzUh2Gea/rR5jdzTkKw3efWkLnHWnewH0LSipJFp1IApaBjkqakMdS0AOpaQyK4mjgi3ynA/nWQgu9bm2R7ktwc4qWCN6KG10hAETzbj+VULiWW4k3zvk9h2FNIDPuFzVS0hPn5piLPiS0a88L3kS/eQeev/AAHNeXScqH7GtIbAn0KrfcqBPv49qYiQdamHSpKGycxsPatzXX3WPh9x0fT+f/HapESMO8/5Z/8AXMCq3GKkoYMZ5oyO1HQQw0wmlYojaoaoQ2imMbQMmkIKbQB9D06kyRwpaQxRS0AOWpaRQtOpgKKgvb2Ozh3Py/8ACvrSAz7SwudaufOuPlj+nSt/zI7SH7PZcdmep3AosKhxQIgkTNNt48PTA04wu5N6goW2sDXj2p2B0+5urEjb9mlZB9M8VcGSZbj5arR5872xSbLRKetTLytDAaRkVr6rltL8PH/pxYfqtWnoTYxbo9B7VWqGPYZ3pKAG9aQ0xEZ5qHNJDG0maYCUtFwEpKBH0MacKbEPpakYopaAHLUlAx1LSAqX9+lmuPvSn7q1X0vTJb6X7Vevx9KTGmb0jqsXlQjagqqaQEbVFimA3bT4UoETuOE9pE/9CFcD4+sfK8S+eeVv4t2fQrgGiHxA+hxbA/Mp61UHEtbAS45qRetQMcea07/5tA0Fv+mMi/rSBIxbnt9KrGmIYabQAGm0CGGoD1otcY3vRirQB0pDUiUgo96LBc+hTSikIeKWkMWlpiHLT6RQ6qF/qIg/dQ/NL/KpAZpemGZzc3h4Jzz3rceUEbIxtQdqBjKY1MCM0ygQYqaMUgFuT5duz/3cH9awviZbL/ZlvddDDc7P+AtmmlqFzzGdf3hNUCmLirAe3WlBpDsPNat5/wAilo7ekkq/+PGol0AxLr+D/dqoapPoA00ymxCGkpCGmoG+9TGNopIYlBp2EJRRfUZ9C96WkQOpwoAUU7NAxwp2cDJ6CgDKutRaRvKsT/20q3pmlJCBPc9eoFHkM0nk3ey0gosMdSGkBGabSAUVKlAiHVv+QReY/wCeLVP4ng/tHRbq0TH+lREJ/vYyKfRCPGGBZemCvB+tUZ0w27FUFhCMdaZRYY8VsXn/ACJGme15IP8A0KpnuilsYFx/D9KqnrVdBXGGkNK4mJSUCsNqBuDTKG0lMQhooAQ0dKYz39LmCT7k0Z/4FU1S9zMfSigY7NIzqgzIwUe5oC5Tl1WFOIB5x9R0quq3moyfPwnoOlIqxsWdnBZrwAW+lWGbNJAFLTAWlqQGUlAwqVKLiGX43abdj/pg/wDKr0HzWtszf881P/jtSxnIeNfC/n79W01P3ygm4gX/AJa/7VedSIDWkWQVpBUVO40FbE3Pga2/2b1v60t2VcwLjotVjQIbTTQFhppM073ASo3qrICOjFSwDaaXYaY2G2k2U+Yk9kk0s1ELCaM/IzD6GoYXHbNRX7txNj/ep3/Ey/5+JsfWi4EiwX7/AHp5j/wKpk0dicyH86QzSt9Phj5bk1fUhRhRimSLRQUOFOFIY6lxQAuKaaQhtSLQMdMN1tMPWNh+lTQkLp9uWPSJP5VHUf2SYvjFeU6lYnVtc1ptHtXMdswZ1Qf3q1iiDmmAYfKwaoSnrTAjK81q5z4Ib/YvhTiN7GFc/wANVj7UgEpppdAGUnamAU3vSEGKMUygpfpTEJRikhHvFLTELTxSAeDS5pDFFPoYC08UALTqBjhTxSAdUbUARxZkyVUkA4qVaQEn8J+lMlcf2ZEnqqj8BS8xtkOsXn2XSbi6/upkVzvgGIw+G59QJPm313n8F4qugrDvEfhy21CUzW5Frc9eB8j1xepaFqNjHJNNa5hjG5pIzuXFPmEYMknzfKM1owsT4MvQw6XsdWhMxrj+Gq9TYY2mmgY2ikAlNNMBaSgNgpaAYUnegD3g0CmyR1LUiHU6gBwp1Axwp4oGOFOFIB1PFAATWRrV99mtyqH96/AoGYmjWzSXPm7m8qN8n5z8zV1wNJ6gSr1rPu9x0qVg3zeT5cfsTmmhGD8QNQEOhLBn743H6LWlotq+n+F9ItZciUxedIM/xNzQUXbhs1RnijuYHt5xmKZdj/Q1BL2PL762lsbya2uUw8TY+o7GpoG3eF9Qx/DdRfyNaNjS6mHN2/GoaYiM0lLqUJig09xCU00AA4paXUYlLQAlJVEnu560ChkjgacKQC0+gY6nCkMdTqBDqcKBjxS5pAQXNwsELSOeBXJSPLqF5/tvwvsKfQDdhjSCJY4/urWmpqRkit8wqnjzY1j5/dUxHH68g1nxvp2mHmPzAj8/w/eauz1CbzLxqGCK0h+UVWZqlgjmvF2mC4tv7RhU+bAuJQv8aetc5bD/AIprVMHpIhqnsVEw5eoqA1ZAw0lSUJTaACkNMApQpPSkG5MsHrSm2PY0cwEboVqOrEe6mikyRadmkMdThQA6nUgHU6gBwNKDQMfmms+BQBy2r332ifaP9VHVzS7cwxebIMPIPyFDAu5q8p4FSA/NQ+asKyOe3zH8KBHF+C91/wCOLu/P3beCTOfViAK6mRy0jE9zTluUDt+7FVWakIZu5rk9R03+zNI10R/NBN5csXqvzcigFucfL1FV+9WAlNpdRCdaQ1RQ2ikAZq3Gu1RSfYaJM0oagbFIDg5FUpY9jUeoj3CiqZmLS0gHCnCkAuadQMdT80ALmnUABasXWb/aPIiPzH71AGZp0H2ifJH7qLr7n0rdzUlCK26ryHKL9KBD81jeIrn7Jotw+fv/ALv86aEyr4Fg+x+E5Lk/evpS+eeVX5Vq+x4pFCk/6Ov0qsxpCGFqy/EXPh6//wCuX9RTGedzdqgPWr6iGmm0DEpM0hCUlV0C4qffq4eAKlgJSbqTKuP302b51B9KegHtBopmYU6gBadmgBc06kAtOBpjH5ozSEU9QuxbwE5+btXM7nuJ/V3NBSN6CNYIRGnQU6R8LUgNhPWtGI/uxTAeTXG+OpXuGs9Ng3ebM3AA7ngUCOsu0jsrSGyg/wBXCoiH/ARVFj8p+lIoXP7gVWY0AQNPGDguoP1qhrTrJ4d1AocjyDQiTz+aq9W3qMYTSUAIaSmMKSixNx0f3xVlqm2pRGabmmxhk1MpyKVhHsoanUzMWikMdS0ALS5oGLTxTELmoppQiFmOKAOXvLo3M27Py9qu6XDiPzmHLfd+lJjNGqjSb3oGTQH5jWlEfkqQHGuZ063/ALS+IV3eSKHg0pAqkrx5uOKoRuXz/vQPQVTkbC1JSHA/6OKrOaYHOa1FNJqAESsV8lgcNjntUxDr4Onjm4kW1cNzmmScVLUFPqUNPWm0rkiUlBQlFUBJD9+p261PUdyJqZQMWnIaQj//2QAAAAAAAAAAAAAAAAAAAAAAAAAAAL3pCKQDaQ0DCkqrjJIjiVauPUMRXamUDsFANID/2QAACYwzL3p2tIU8GmJwA0Zi6exovYNzi5ahqxDaZTsMKQ0hXEpaQAPvCrp5ApMogbrSUrAgoFAH/9kAAAAAAAAAAAEpKAuFXVP7mlIZARSYouMMUdqTA//ZAAAAAAAAAAAAAGKSpAb60tAy1D/qqifrSGR9aSgBe1OTpQI//9kAAACrpqImjmwavwXFZtDNSG4+WrFpc/vbkZ6MuPypEEzzbsGmmQ460dRgj/PT5pOaLhfUhL0m6gZzXis/6dZf9cH/APQhXPt1qxDabQPYSkoATvRQwCkNCAKSmB//2Q== \n</code></pre> <p>With some modifications to OpenAI's sample code and the help of GPT-4o itself, I created the following script that would grab the base64 data from the custom handler and parse it into the model. The prompt for the model is \"You are a image to text OCR engine. Output the text you see in this image, and nothing else.\"</p> <pre><code>import requests\n\n# OpenAI API Key\napi_key = \"REDACTED\"\n\n# Function to get the base64 encoded image from ESP32CAM\ndef get_base64_image(url):\n    response = requests.get(url)\n    return response.text\n\n# URL to the ESP32CAM base64 image endpoint\nesp32cam_url = \"http://10.12.28.193/base64\"\n\n# Getting the base64 string\nbase64_image = get_base64_image(esp32cam_url)\n\nheaders = {\n  \"Content-Type\": \"application/json\",\n  \"Authorization\": f\"Bearer {api_key}\"\n}\n\npayload = {\n  \"model\": \"gpt-4o\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"You are a image to text OCR engine. Output the text you see in this image, and nothing else.\"\n        },\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\n            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n          }\n        }\n      ]\n    }\n  ],\n  \"max_tokens\": 300\n}\n\nresponse = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n\nprint(response.json())\n</code></pre> <p>With the same setup as the Raspberry Pi, where the ESP32CAM is pointed towards a paper with the words \"Hello World!\", I ran the script on my computer to test it out.</p> <p>Upon running, the following is printed out:</p> <p> </p> <p>As is seen in the output, the prompt worked and the 4o model detected \"Hello World!\" as the text extract and stored it in 'content'. To just output the content, which is the actual result I want, I can modify the code a little.</p> <pre><code>response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n\ncontent_string = response.json()['choices'][0]['message']['content']\nprint(content_string)\n</code></pre> <p>Here is the code just printing out the extracted OCR text.</p> <p> </p> <p>This was essentially the bare-bones version of my final project Pi code. After integrating the serial communication functionality, and changing the wording of the prompt, I had the final code for the Raspberry Pi ready:</p> <pre><code>from serial import Serial\nimport time\nimport requests\n\n# OpenAI API Key\napi_key = \"REDACTED\"\n\n# Configure the serial port\nser = Serial('/dev/serial0', 9600, timeout = 1)\n\n# Function to get the base64 encoded image from ESP32CAM\ndef get_base64_image(url):\n    response = requests.get(url)\n    print(\"Camera connected\")\n    return response.text\n\n# URL to the ESP32CAM base64 image endpoint\nesp32cam_url = \"http://10.12.23.1/base64\"\n\n# Getting the base64 string\nbase64_image = get_base64_image(esp32cam_url)\n\n# Function to send a message\ndef send_message(message):\n    ser.write(message.encode())  # Convert the message to bytes and send\n    time.sleep(1)                # Wait for a second\n\n# Example message used for testing purposes\n# testMessage = \"Hello world\"\n\ndef ocr():\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {api_key}\"\n    }\n\n    # Define the information sent to GPT4o\n    payload = {\n        \"model\": \"gpt-4o\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"You are a image to text OCR engine. Output the text you see in this image, and nothing else. Only use lowercase letters and no punctuation.\"\n                    },\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n                        }\n                    }\n                ]\n            }\n        ],\n        \"max_tokens\": 300\n    }\n\n    # Query the engine\n    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n\n    # Extract its response\n    return response.json()['choices'][0]['message']['content']\n\ntry:\n    message = ocr()\n    send_message(message)\n    print(f\"Message sent: {message}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\nfinally:\n    ser.close()  # Close the serial port\n</code></pre>"},{"location":"conrad/#text-to-braille-mapping","title":"Text to Braille Mapping","text":"<p>The Raspberry Pi sends a byte-encoded text string to the ATTiny1614. From there, the ATTiny1614 is responsible for interpreting and converting the received text into braille dot arrays, which it then shows on the 3x2 array.</p> <p>The following program first sets up the configuration of solenodis and corresponding GPIOs, then creates the arrays of 0s and 1s (solenoid up or down) that forms a single braille character. It then maps each letter to its corresponding letter braille array.</p> <p>Upon the script's setup, each relevant pin is configured to be OUTPUT, and a serial connection with the Raspberry Pi is initialized. </p> <p>The script then continuously waits for an incoming string sent through Serial. Upon receiving the text, it is converted into an array of characters. The parse_input_string() method then does the heavy lifting and fetches the correct braille dot arrays. The method then iterates through each letter and represents that character's corresponding braille for one second on the physical solenoid array.</p> <p>The activate_solenoids() and deactivate_solenoids() methods were used for debugging purposes, turning all solenoids on or off, respectively.</p> <pre><code>/*\n  Solenoid arrangement:\n  0 1\n  2 3\n  4 5\n*/\n\nint sols[6] = {0, 1, 2, 3, 9, 8}; // Define the pins connected to the solenoids\n\n// Define the Braille arrays\nint a[6] = {0, 1, 1, 1, 1, 1};\nint b[6] = {0, 1, 0, 1, 1, 1};\nint c[6] = {0, 0, 1, 1, 1, 1};\nint d[6] = {0, 0, 1, 0, 1, 1};\nint e[6] = {0, 1, 1, 0, 1, 1};\nint f[6] = {0, 0, 0, 1, 1, 1};\nint g[6] = {0, 0, 0, 0, 1, 1};\nint h[6] = {0, 1, 0, 0, 1, 1};\nint i[6] = {1, 0, 0, 1, 1, 1};\nint j[6] = {1, 0, 0, 0, 1, 1};\nint k[6] = {0, 1, 1, 1, 0, 1};\nint l[6] = {0, 1, 0, 1, 0, 1};\nint m[6] = {0, 0, 1, 1, 0, 1};\nint n[6] = {0, 0, 1, 0, 0, 1};\nint o[6] = {0, 1, 1, 0, 0, 1};\nint p[6] = {0, 0, 0, 1, 0, 1};\nint q[6] = {0, 0, 0, 0, 0, 1};\nint r[6] = {0, 1, 0, 0, 0, 1};\nint s[6] = {1, 0, 0, 1, 0, 1};\nint t[6] = {1, 0, 0, 0, 0, 1};\nint u[6] = {0, 1, 1, 1, 0, 0};\nint v[6] = {0, 1, 0, 1, 0, 0};\nint w[6] = {1, 0, 0, 0, 1, 0};\nint x[6] = {0, 0, 1, 1, 0, 0};\nint y[6] = {0, 0, 1, 0, 0, 0};\nint z[6] = {0, 1, 1, 0, 0, 0};\n\nint space[6] = {1, 1, 1, 1, 1, 1};\n\nint number_sign[6] = {1, 0, 1, 0, 0, 0};\nint num_1[6] = {0, 1, 1, 1, 1, 1}; // Same as a\nint num_2[6] = {0, 1, 0, 1, 1, 1}; // Same as b\nint num_3[6] = {0, 0, 1, 1, 1, 1}; // Same as c\nint num_4[6] = {0, 0, 1, 0, 1, 1}; // Same as d\nint num_5[6] = {0, 1, 1, 0, 1, 1}; // Same as e\nint num_6[6] = {0, 0, 0, 1, 1, 1}; // Same as f\nint num_7[6] = {0, 0, 0, 0, 1, 1}; // Same as g\nint num_8[6] = {0, 1, 0, 0, 1, 1}; // Same as h\nint num_9[6] = {1, 0, 0, 1, 1, 1}; // Same as i\nint num_0[6] = {1, 0, 0, 0, 1, 1}; // Same as j\n\n// Define a structure to map characters to their Braille arrays\ntypedef struct {\n    char character;\n    int *braille_array;\n} BrailleMap;\n\n// Create the mapping\nBrailleMap braille_dictionary[] = {\n    {'a', a}, {'b', b}, {'c', c}, {'d', d}, {'e', e},\n    {'f', f}, {'g', g}, {'h', h}, {'i', i}, {'j', j},\n    {'k', k}, {'l', l}, {'m', m}, {'n', n}, {'o', o},\n    {'p', p}, {'q', q}, {'r', r}, {'s', s}, {'t', t},\n    {'u', u}, {'v', v}, {'w', w}, {'x', x}, {'y', y}, {'z', z}, {' ', space},\n    {'#', number_sign},\n    {'1', num_1}, {'2', num_2}, {'3', num_3}, {'4', num_4}, {'5', num_5},\n    {'6', num_6}, {'7', num_7}, {'8', num_8}, {'9', num_9}, {'0', num_0}\n};\n\nvoid setup() {\n    // Initialize the solenoid pins as output\n    for (int i = 0; i &lt; 6; i++) {\n        pinMode(sols[i], OUTPUT);\n    }\n  //  PORTMUX.CTRLB = 0x01;\n   Serial.begin(9600);\n\n   while(!Serial){\n\n   }\n}\n\nvoid activate_solenoids(int *braille_array) {\n    for (int i = 0; i &lt; 6; i++) {\n        digitalWrite(sols[i], braille_array[i]);\n    }\n}\n\n// Function to parse the input string and activate solenoids\nvoid parse_input_string(const char *input) {\n    int len = sizeof(braille_dictionary) / sizeof(BrailleMap);\n    for (int i = 0; i &lt; strlen(input); i++) {\n        for (int j = 0; j &lt; len; j++) {\n            if (braille_dictionary[j].character == input[i]) {\n                activate_solenoids(braille_dictionary[j].braille_array);\n                delay(1000); // Wait for a second before next character\n                break;\n            }\n        }\n    }\n    deactivate_solenoids();\n}\n\nvoid deactivate_solenoids() {\n    for (int i = 0; i &lt; 6; i++) {\n        digitalWrite(sols[i], LOW);\n    }\n}\n\nvoid loop() {\n  if (Serial.available() &gt; 0) {\n    // Read the incoming string\n\n    String input = Serial.readString();\n\n    // Convert the String to a C-style string (char array)\n    char inputArray[input.length() + 1];\n    input.toCharArray(inputArray, input.length() + 1);\n\n    // Call the parse_input_string function with the received string\n    parse_input_string(inputArray);\n\n    // Optional: Add a delay to avoid flooding the input\n    delay(5000); // Wait for 5 seconds before repeating\n  }\n}\n</code></pre>"},{"location":"conrad/#assembly","title":"Assembly","text":"<p>I first outlined the general setup of my final project. I secured each MOSFET to a corresponding battery pack and solenoid, and color-coded each MOSFET's trigger and GND wires. I organized them in such a way that toggling solenoid 1, 2, 3, 4, 5, then 6 would control each solenoid in a line.</p> <p> </p> <p>I then connected the color coded wire sets to the controller PCB, and tested turning all the solenoids on in the right order.</p> <p> <p></p> <p>Lastly, I secured everything in place and put the cover on. This involved using Nitto tape to attach the PCB to the side wall, and using hot glue to secure the MOSFETs to the bottom and the solenoids to their respective places. The PCB on the side wall is circled in red in the image below, as it is somewhat difficult to see. Each MOSFET drive module is connected to the controller PCB through a set of respective color coded wire pairs. The top-left solenoid corresponds with both white wires, the top-right solenoid with dark blue, the mid-left solenoid with green, the mid-right solenoid with orange, the bottom-left solenoid with blue, and the bottom right solenoid with yellow. The color coded scheme makes it easier for me to detach individual wires for debugging and knowing which solenoid a specific wire corresponds to.</p> <p> </p> <p>I then ran another test to ensure everything still worked.</p> <p> <p></p> <p>The assembly of the Raspberry Pi case is relatively simple. As the 3D print already has holes built in for USB wires, and a hole built in for the screen, all I really need to do is secure the Pi to the bottom and the screen to the top, then connect the wires. I used 4 M3 screws in the screw-holes that I designed to secure the Raspberry Pi screen to the top of the case. I then used hot glue to secure the Pi in place on the bottom of the case. The following image shows the case's top on the left, with the screwed-in screen connected to the Pi via the microHDMI port for data and the power cable. The Pi itself is shown on the right, with its power cable coming out of the left side box hole, and the ESP32CAM cable coming out of the bottom hole. The wires on the right side case hole are used for connecting the Pi's TX/RX, VCC, and GND with the BrailleBox's ATTiny1614 PCB.</p> <p> </p> <p>Here is the Pi case when closed.</p> <p> </p>"},{"location":"conrad/#evaluation","title":"Evaluation","text":"<p>My project is considered successful if it can:</p> <ul> <li>\u2611 Accurately extract text from a live image feed</li> <li>\u2611 Map the text to braille</li> <li>\u2611 Display the braille on the solenoid array</li> </ul>"},{"location":"conrad/#implications","title":"Implications","text":"<p>There is existing technologies on the market that can convert text to braille in real time, but those are often expensive and not readily available to the public. My hope with this project is to create a product that can be cheaply produced and reach a wide audience. </p>"},{"location":"conrad/#lessons-learned","title":"Lessons Learned","text":"<ul> <li>Some parts of a project will take longer while others will take shorter than expected.</li> <li>Double the planned allocation of time due to errors and debugging</li> <li>Working with lower-level hardware and software is more rewarding and often produces a more solid product</li> <li>There are many types of transistors, which can be a pain to sort through</li> </ul>"},{"location":"conrad/#final-product","title":"Final Product","text":"<p>WARNING: The project video and slide (poster) are out of date and provide incorrect information about licensing. The current EULA is here. By installing, accessing, or using the Product, you acknowledge that you have read this Agreement, understand it, and agree to be bound by its terms and conditions.</p>"},{"location":"conrad/#poster","title":"Poster","text":""},{"location":"conrad/#video","title":"Video","text":"<p> <p></p>"},{"location":"conrad/#file-downloads","title":"File Downloads","text":"<p>My files can be downloaded here.</p>"},{"location":"edm2/","title":"Engineering Projects","text":""},{"location":"edm2/#automated-ouija-board","title":"Automated Ouija Board","text":""},{"location":"edm2/#engineering-i-and-ii-documentation","title":"Engineering I and II Documentation","text":""},{"location":"edm2/#deep-dive-into-fusion-360-pt-1","title":"Deep Dive into Fusion 360 Pt. 1","text":""},{"location":"edm2/#deep-dive-into-fusion-360-pt-2","title":"Deep Dive into Fusion 360 Pt. 2","text":""},{"location":"edm2/#milling-about","title":"Milling About","text":""},{"location":"edm2/#pressing-charges","title":"Pressing Charges","text":""},{"location":"edm2/#cutting-board","title":"Cutting Board","text":""},{"location":"coding/macro/","title":"Macro Maker","text":""},{"location":"coding/macro/#description","title":"Description","text":"<p>The Macro project was my final project for Java Data Structures (H) in 10th grade, of which the requirements can be found here. The final product allows the user to start and end a global recording of all inputs to the computer. The global recording will inputs including mouse movements; mouse dragging; mouse clicking, releasing, and holding; key presses, releases, and hold times; and any simultaneous combination of events. The actions are also logged in the logger.out file. After the user begins recording, (CTRL + SHIFT+ R), all henceforth input actions are recorded until the user stops recording (CTRL + SHIFT + S). Upon replaying, the program will execute the actions exactly as recorded. The press() methods also delays the action's execution as to match the delays between actions of the original recording. The program does not interfere with input actions or other applications in any way.</p>"},{"location":"coding/macro/#demonstration","title":"Demonstration","text":"<p>This is a macro made to play the video game Bloons Tower Defense 6 by infinitely beating one of the levels to farm as much in-game currency and in-game player experience as desired. This macro uses a slightly modified version of the Macro Maker source code that allows specification of how much in-game currency or experience the user wants.</p> <p> <p></p>"},{"location":"coding/macro/#inspiration","title":"Inspiration","text":"<p>I've done a lot of scripting before, both static and dynamic through OCR. My goal with this project was to make a program that could record your actions and replay them with no need for hard coding or manual key press/mouse coordinate tracking. In the future, I plan to make make the project's logging function more useful, where the logging of actions into a seperate .txt document can function as a saved user macro sequence. Eventually I want to add a feature where the program is able to ingest a log file and replay the actions stored there, allowing users to save a specific macro multiple times past the closure of the JFrame.</p>"},{"location":"coding/macro/#method","title":"Method","text":"<p>The main library that enabled the creation of this project is JNativeHook, which allows the global tracking of all actions that are sent into a computer, as opposed to only being able to detect inputs inside a specific window. In this project, JNativeHook's GlobalScreen and adapters were used, but I have overriden the action listeners and adapters with my own code to suit the needs of this project, specifically storage and replaying of inputs. </p> <p>When the program is running and recording is true (the user has started recording), every input action triggers a specific handler method which is able to encapsulate the event as an object to store in the recordedActions stack. During recording, a global variable of time is used to keep track of delays between actions. Every time a new action is recorded, the variable is then updated to the time in milliseconds when the action was performed. When the next action is then recorded, subtracting the last recorded time from the current system time gives us the delay, which is stored as part of the action object. Then, the time variable is updated. Thus, we are able to track the delays between actions in order to replay them at the right times. </p> <p>The different action classes (ClickPress, ClickRelease, KeyPress, KeyRelease, MouseMove) allow the program to store every input as an individual object. All these classes implement the Action interface, giving them all a press() method which is called when replaying. Here is an example of the handler method creating a new object:</p> <pre><code>   @Override\n    public void nativeKeyPressed(NativeKeyEvent e) {    // This method is called when a key is pressed; other methods handle other events.\n        recordedActions.add(new KeyPress(e, Math.abs(System.currentTimeMillis()-time)));    // Creates a new KeyPress object in the stack, parsing in the NativeKeyEvent generated by JNativeHook/GlobalScreen and the time since last recorded action\n        time = System.currentTimeMillis();  // Updates variable time\n    } \n\n\n    // For context, the constructor for KeyPress is: \n    public KeyPress(NativeKeyEvent ke, long delay) {...}\n</code></pre> <p> The other adapter and conversion classes (SwingKeyAdapter, MouseCoordinateConverter, KeyLocationLookup, KeyCursorLookup) allow for conversion between different keycodes and screen sizes. The SwingKeyAdapter, KeyLocationLookup, and KeyCursorLookup classes convert VC keycodes used by NativeKeyEvent into VK keycodes used by KeyEvent. The MouseCoordinateConverter class allows for conversion between physical mouse cursor coordinate positions and scaled mouse cursor coordinate positions, allowing MouseMove to move the cursor to the correct point for all screen sizes on different computers. For example, the KeyPress class uses SwingKeyAdapter to translate a NativeKeyEvent keycode integer value to a KeyEvent keycode, which is what the robot from java.awt.Robot can interpret and use. </p> <pre><code>   @Override\n    public void press() {\n        robot.delay((int) delay); // Waits the millisecond amount as parsed from the constructor\n        robot.keyPress(ska.getJavaKeyEvent(e).getKeyCode()); // Uses SwingKeyAdapter to convert keycode values\n    }</code></pre> <p> During recording, all input actions are created as new objects of their respective class and are stored in a stack until the user stops. Upon replaying the stored actions, the program will iterate through the stack and call each object's press() method to execute the action exactly as performed. </p> <pre><code>   for(Action i : recordedActions) {\n        i.press(); /* The implementation of the Action inferface by all the input action classes \n                      allows me to call a generic .press() function */\n    }\n</code></pre> <p>  For more information, the full project can be found open-sourced on the Github repo. The downloadable jar file is not obfuscated and can be decompiled. </p> <p> </p>"},{"location":"coding/macro/#download","title":"Download","text":"<p>"},{"location":"coding/macro/#the-github-repo-can-be-found-here","title":"The github repo can be found here.","text":""},{"location":"coding/macro/#the-jar-executable-can-be-downloaded-here","title":"The jar executable can be downloaded here.","text":"<p>It appears as of November 2024 that an update to the JVM has caused an issue with the JNativeHook library, likely due to increased keylogger malware protections. I will debug this issue if I find the time.</p> <p></p>"},{"location":"coding/macro/#instructions-for-use","title":"Instructions for Use","text":"<p>Start Recording Keystrokes: CTRL + SHIFT + R</p> <p>Stop Recording Keystrokes and Save: CTRL + SHIFT + S</p> <p>Replay Saved Keystroke Sequence: CTRL + SHIFT + 1</p> <p> </p>"},{"location":"coding/macro/#initial-planning-chart","title":"Initial Planning Chart","text":"<p> <p> </p>"},{"location":"coding/macro/#key-elements","title":"Key Elements","text":"<ul> <li>Action interface implemented by all the action/event classes: ClickPress, ClickRelease, KeyPress, KeyRelease, MouseMove, etc.<ul> <li>Every action class follows the command archetype, allowing the encapsulation of a request as an object.</li> </ul> </li> <li>KeyLocationLookup <ul> <li>Singleton - single instance of the lookup object that is referenced every time a key conversion is needed</li> <li>Hashtable inside KeyLocationLookup to store NativeKeyEvent KeyLocations and corresponding KeyEvent integer values for conversion</li> </ul> </li> <li>Mouse and Keyboard Adapters to listen for actions within the global screen. Those adapters also function as observers for the GlobalScreen.</li> <li>Stack to store the recorded actions</li> <li>Custom MouseCoordinatesConverter component that will convert true x, y mouse coordinates to relative x, y mouse coordinates scaled to each computer's unique screen size. Also functions as an adapter.</li> <li>State machine under GUILogger handling action transitions and entries when a key/mouse is pressed</li> <li>TreeMap under KeyCursorLookup to store NativeKeyEvent VC values and corresponding KeyEvent VK values</li> </ul>"},{"location":"edm2/lessons/cuttingBoard/","title":"Cutting Board","text":"<p>For this project, I was assigned to create a custom cutting board through making indentations and holes with the Shopbot CNC machine and filling them in with resin.</p>"},{"location":"edm2/lessons/cuttingBoard/#jig-design","title":"Jig Design","text":"<p>Normally, when CNCing a piece of wood, I would simply attach it to the machine bed using brad nails or something similar. However, because the actual cutting board wood itself is being cut on, pushing nails through it wouldn't work out. As such, I needed to design a holder for the cutting board that ensures it remains stable during the CNC job. The holder can be nailed down to the bed.</p> <p>Mrs Morrow provided me with a DXF design of the jig that I was to cut.</p> <p> </p> <p>From there, I imported it into Aspire and began creating the toolpaths. Specifically, the internal rectangle would use the \"Inside\" cutting setting, as I needed to preserve the exact dimensions of the rectangular hole to fit the cutting board wood into. The outside edge and the six inlets all used the \"Outside\" cutting setting.</p> <p> </p> <p>However, as visible above, the inlets are not cutting as expected and are wider than they are supposed to be, since the tool we were using is too thick for doing a back and forth. We solved this issue by going back in the original DXF file and instead of creating inlet outlines, we created a single line which would represent the actual toolpath, and created a new toolpath on \"Center\" along that line.</p> <p>I then ran a test air cut for the jig.</p> <p> <p></p> <p>After verifying that everything seemed in order, I ran the cut.</p> <p> </p> <p>I then de-bradded the larger wood piece and took off the jig. The internal rectangle cutout has retained the correct dimensions to fit in a 13 inch by 11 inch cutting board, and each of the outer inlets are the correct thickness.</p> <p> </p>"},{"location":"edm2/lessons/cuttingBoard/#board-design","title":"Board Design","text":"<p>Once the jig was ready, I began to design my actual cutting board. I had previously created a 2D design in Cuttle, which I liked and decided to use for this project.</p> <p>I first designed a pentagon by using the Polygon tool.</p> <p> </p> <p>I then created a rectangle which I duplicated on all 5 edges of the pentagon using the Rotational Repeat tool.</p> <p> </p> <p>I then created a group with the Rotational Repeat attribute. This means that anything I design under this group would automatically be repeated across the pentagon. I then added a bezier curve and some circles.</p> <p> <p></p> <p>After doing some additional modifications, this was my final design:</p> <p> </p> <p>I then imported the design into Aspire and made some minor changes. I also set the canvas size to 13 by 11 inches, which was the size of the cutting board wood that I would work with. I then scaled up the design and removed the outer cutout and tabs. Lastly, I added a rectangle cutout on the left-hand side of the board to act as a handle on the finished cutting board.</p> <p> </p> <p>I then started the CAM process. </p> <p>The grip/handle was the only hole cutout in the design. I set its cutting depth equivalent to the height of the actual wood piece (0.82 inches) and used the inside cutting setting. Additionally, I added tabs to the cutout to prevent the removed internal wood from flying out when in contact with the spinning spindle. This toolpath uses a 1/4\" bit.</p> <p> </p> <p>For all other details, I wanted to engrave them onto the board and thus set the cutting depth to 0.25 inches, and also used the inside cutting setting. As the circles in my design vary quite significantly in size, I decided to take advantage of the tool changer that the Shopbot is equipped with, and I created each individual toolpath with a different size bit.</p> <p>I created the \"small\" toolpath, which cuts the outermost and innermost five small circles, along with cutting the outlines for the 5 leaves. This toolpath used the 1/8\" bit.</p> <p> </p> <p>The \"big\" toolpath is essentially responsible for everything that isn't too small for the 3/8\" bit. This toolpath cuts the first 3 circles on each 4-circle array, and cuts the first circle on each 3-circle array.</p> <p> </p> <p>The \"small2\" toolpath cuts the remaining circles with a 1/4\" bit.</p> <p> </p> <p>After previewing my entire design, I realized that there were small stubs in the middle of each large circle, as the 3/8\" bit size was smaller than the diameter of the circle. </p> <p> </p> <p>To solve this issue, I created a smaller concentric circle within each large circle that needed a second cut. I then created a \"correctional\" toolpath on these internal circles to remove the extra material. This toolpath uses the 3/8\" bit, as it essentially is just another internal pass after the first cut doesn't remove all the material in the large circle.</p> <p> </p> <p>Here is the completed preview of my board cut.</p> <p> </p>"},{"location":"edm2/lessons/cuttingBoard/#board-cutting","title":"Board Cutting","text":"<p>After running an air cut and verifying that everything looked good, I secured my cutting board wood to the previously made jig.</p> <p> </p> <p>I then ran the cut. Here is a timelapse of a short portion of the cut.</p> <p> <p></p> <p>Because I used multiple bits of different sizes, the Shopbot had to change the active bit during the cut. Here is a timelapse of the machine changing bits during the job.</p> <p> <p></p> <p>After I let the cut run through, this is what my board looked like.</p> <p> </p>"},{"location":"edm2/lessons/cuttingBoard/#resin","title":"Resin","text":"<p>Now that the board itself is ready, I need to apply resin to all of the indentations that I created. I first poured resin over the entire board to ensure that it got into every cut.</p> <p> </p> <p>Once the resin had solidified, I planed the top of the board remove the excess resin and sanded it to give the board a smooth finish.</p> <p> </p> <p>Finally, I applied oil to all sides of the board to give the board a richer color and prepare it for use.</p> <p> </p>"},{"location":"edm2/lessons/fusionpart1/","title":"Deep Dive into Fusion 360 (Part 1)","text":""},{"location":"edm2/lessons/fusionpart1/#self-evaluation-beforehand","title":"Self-Evaluation Beforehand","text":"<p>Before starting this project, I already had some prior experience using Fusion360. However, in the pas I always struggled with modifying geometry for a specific command and navigating different sketches. I was able to overcome this through practicing using the bar at the bottom. </p>"},{"location":"edm2/lessons/fusionpart1/#storage-box","title":"Storage Box","text":"<p>Before starting working with Fusion, I was required to make a storage box. I decided to reuse a box that I already had from making it during Fab Academy. My documentation for making the box can be found here, containing a description on how I made the box.</p>"},{"location":"edm2/lessons/fusionpart1/#navigating-the-fusion-360-user-interface","title":"Navigating the Fusion 360 User Interface","text":"<p> <p>  This is the application bar. This bar contains the data panel, file menu, save button, and undo/redo buttons. This is also where all open projects are displayed in a tabs format.</p> <p></p> <p>  This is the data panel. The data panel functions as the file explorer for Fusion files and directories, along with managing sharing settings.</p> <p></p> <p>  These are the Profile and Help settings. This area contains notifications, statuses, and the profile button which allows modification of preferences and profile settings.</p> <p></p> <p>  This is the toolbar. The toolbar contains all of the tools/actions. The tools in the toolbar are organized by category: solid, surface, mesh, sheet metal, plastic, and utilities. The categories can be selected at the top bar, and the tools themselves are in the main rectangular panel.</p> <p></p> <p>  This is the browser. The browser allows easy access and viewing of all objects within the design. The browser allows the user to change the visibility of objects and change file units. The browser is structured like a file explorer.</p> <p></p> <p>  This is the view cube. The view cube allows orbiting around the design, and snapping to a specific viewing angle. The home button will orbit to the default home viewing position.</p> <p></p> <p>  This is the canvas. The canvas is where all the designing takes place. Right clicking in the canvas will access the marking menu/right-click menu which allows quick access of specific tools and commands.</p> <p></p> <p>  These are the navigation bar and display settings. The navigation bar is used to move around the design more accurately than the view cube. The display settings control cosmetic appearances.</p> <p></p> <p>  This is the view of the canvas from multiple perspectives at once, through use of the Viewports -&gt; Multiple Views setting.</p> <p></p> <p></p>"},{"location":"edm2/lessons/fusionpart1/#interface-parts-list","title":"Interface Parts List","text":""},{"location":"edm2/lessons/fusionpart1/#pro-tricks","title":"Pro Tricks","text":"<ol> <li>Utilize sketch constraints to secure specific geometry.  </li> <li>Use the timeline to modify sketches that have other geometry defined on top of them.  </li> </ol>"},{"location":"edm2/lessons/fusionpart1/#paperclip","title":"Paperclip","text":"<p>My file for the paperclip design in Fusion360 can be downloaded here. The paperclip's design mainly used constraints and the sweep command. I learned how constraints can be used to ensure that geometry remains strictly vertical/horizontal/tangent and how the sweep tool uses a sweep path and selects an object to be swept.</p> <p> <p></p> <p></p>"},{"location":"edm2/lessons/fusionpart1/#glass-bottle","title":"Glass Bottle","text":"<p>My file for the glass bottle design in Fusion360 can be downloaded here. The glass bottle's design process mainly focused on inserting a reference image and using the revolve command. I learned how to insert an image to help as I designed and how to manually adjust objects by a set distance.  <p></p> <p></p>"},{"location":"edm2/lessons/fusionpart1/#problems-encountered","title":"Problems Encountered","text":"<p>The main problem I encountered while starting my exploration of Fusion 360 was a lack of up-to-date tutorials. The video series that I followed, Learn Fusion 360 in 30 Days for Complete Beginners, was made a year ago. As such, some of the tools and defaults were different from last year and now. Different placements of tools was not a large issue as I could simply navigate or search to find the correct tool. However, some commands had different effects on the model which I had to manually fix. For example, when inserting a reference image as a canvas, it would create a new origin point which was not the case in the videos. As such, I had to manually move the entire design up by 100mm in order to have it all start on the origin's z-plane.</p>"},{"location":"edm2/lessons/fusionpart2/","title":"Deep Dive into Fusion 360 (Part 2)","text":""},{"location":"edm2/lessons/fusionpart2/#flattened-cone-warmup","title":"Flattened Cone Warmup","text":"<p>As a warmup before starting class one day, I designed a flattened cone to the following specifications:</p> <p> </p> <p>I first made a sketch on the ZY-plane in Fusion360, where I created a quadrilateral to match the one shown in the requirements image.</p> <p> </p> <p>I then used the Revolve tool to revolve the entire quadrilateral sketch 360 degrees around the Z axis to create a full flattened cone:</p> <p> </p> <p>Finally, since the warmup asked me for a 2D drawing, I generated a 2D drawing based off the design.</p> <p> <p></p> <p></p>"},{"location":"edm2/lessons/fusionpart2/#bicycle-rack-design","title":"Bicycle Rack Design","text":"<p>As a review and challenge for my current Fusion360 knowledge, I was asked to design a bicycle rack to a set of specified dimensions. Here is the image with dimensions that I was designing off of:</p> <p> </p> <p>I first created a sketch on the ZY-plane and used the line and circle tools to design my sketch to match the specified dimensions:</p> <p> </p> <p>I then created a new sketch on the XY-plane and created a circle with a diameter of 4mm. To make the design 3D, I used the sweep tool to sweep the circle along the path of the bike rack.</p> <p> <p></p> <p></p>"},{"location":"edm2/lessons/fusionpart2/#generating-2d-drawings","title":"Generating 2D Drawings","text":"<p>I used Kevin Kennedy's Youtube Video to learn how to generate 2D drawings in Fusion. To make a 2D drawing, I first navigated to the design I wanted to make a drawing of. I then clicked on the File dropdown menu, and clicked New Drawing -&gt; From Design, making sure to select the object I wanted a drawing of. To practice making 2D drawings, I generated a drawing from the Fusion360 example design for a connector joint:</p> <p> <p>I then created a 2D drawing of my bicycle rack design:</p> <p> </p> <p></p>"},{"location":"edm2/lessons/fusionpart2/#lego-technic-brick-design","title":"Lego Technic Brick Design","text":"<p>For more practice designing objects in Fusion, I was asked to create a Lego Technic Brick from the following image to its dimensions:</p> <p> </p> <p>There were many possible approaches to designing the brick, but I started with a 3D rectangular prism to match the general specifications of the actual brick part of the Lego. From there, I created concentric circles on the sides and the top, and extruded them (both as a new body and as a hole function) to the dimensions in the image. To create the Lego connectors at the bottom, I created 3 circles centered at the midpoints between the radii of the circle connectors on the top of the Lego brick. I then shelled the entire rectangular prism to hollow it out. Here is my completed design:</p> <p> </p> <p></p>"},{"location":"edm2/lessons/fusionpart2/#custom-lego-design","title":"Custom Lego Design","text":"<p>I was then challenged to design a custom lego brick. I started designing the brick from the main Lego body as it was relatively simply and would be a good base to design the appendages from. I then sketched and extruded one appendage, mirroring it to all 4 sides of the Lego brick. Lastly, I touched up the brick, adding things like the Lego plus connector joint and smoothing out issues with the spline curve extrursion. The following image shows the brick that I was assigned to design.</p> <p> </p> <p>I first used digital calipers to measure every dimension of the Lego brick. I would continue to use these calipers as I went through the designing process to ensure my dimensions were correct.</p> <p>I started off with designing the body of the custom brick. I designed a sketch on the XY plane and extruded it to create the main body of the brick:</p> <p> </p> <p>Next, I created a sketch to design one of the four appendages coming out of the Lego brick's body. I then mirrored it to all four positions and extruded them.</p> <p> <p></p> <p>I then created and extruded a spline curve on the side of the Lego brick to mimic the curve of the real brick.</p> <p> <p></p> <p>To create the plus axle joint I created another sketch in the shape of the plus, which I then mirrored and extruded.</p> <p> <p></p> <p>Here was my final recreation of the Lego brick, shown in both 2D and 3D views:</p> <p> <p></p>"},{"location":"edm2/lessons/fusionpart2/#trading-lego-designs","title":"Trading Lego Designs","text":"<p>Next, I traded Lego designs with a partner. We sent each other our respective 2D views of our design with dimensions. I would then model his Lego brick based solely on the 2D design he sent me:</p> <p> </p> <p>I started the design by creating a sketch for the base of the Lego.</p> <p> </p> <p>I then extruded each of the base's parts to the proper length. I extruded the area between the concentric circles and the two outer rectangles as Lego connectors.</p> <p> </p> <p>I then created a sphere above the base, which I subsequently cut in half.</p> <p> <p></p> <p>Next, I shelled the semisphere to make it hollow.</p> <p> </p> <p>I then created 3 sketches on planes oriented 120 degrees from each other.</p> <p> </p> <p>I created a rectangle perpendicular to each of the three planes. I then used the sweep tool to sweep the rectangle on the path, creating a hole that fits to the spherical shape.</p> <p></p> <p>Here are the 3D and 2D views of the final design:</p> <p> <p> </p>"},{"location":"edm2/lessons/fusionpart2/#custom-object-design","title":"Custom Object Design","text":"<p>To further practice using Fusion, I decided to design a pen holder, of which the tutorial can be found here. My Fusion360 file of the design can be downloaded here. To make the design, I started off with shelling a large cylinder. I then created a polygon on the XZ plane and extruded it as a hole onto the surface of the cylinder. I then repeated this across the whole surface of the cylinder via the rectangular and circular pattern tools.</p> <p> <p></p> <p>To print this file, I first exported the file out of Fusion360 and opened it in PrusaSlicer. I then sliced the file with grid supports and a 15% infill, scaling the design down to finish printing under an hour. Since I was printing at home, my printer wasn't setup with wifi capabilities, so I saved the sliced file onto a flash drive which I then plugged into the printer.</p> <p>Because the hexagonal patterning was so thin, when I originally printed the design, it was heavily reliant on supports. Those supports obscured the actual design and when I tried to peel them away, the entire design crumbled.</p> <p> <p></p> <p>To solve this issue, I first upscaled the print which made the mesh more stable. I also changed the support type to be organic instead of grid, which minimized the points of contact between the supports and the actual print. This time, the supports were a lot easier to remove and did not damage the structure</p> <p> </p>"},{"location":"edm2/lessons/milling/","title":"Milling About","text":"<p>The purpose of this unit was to familiarize ourselves to the entire milling process. We discussed different types of milling machines, and their respective differences and use cases. We first created and milled a simple design on a dog tag to get used to the milling process. Then, we designed a model for a chocolate mold and milled it on a block of wax. Finally, we laser cut a box from cardstock to fit the Valentine's Day theme.</p>"},{"location":"edm2/lessons/milling/#workflow","title":"Workflow","text":"<ul> <li>Apply double-sided adhesive to your material and the bed. Position and place the material on the bed.</li> <li>Open Bantam Tools Software</li> <li>Under home, select install tool, and select the bit that you are installing</li> <li>Insert your tool into the spindle<ul> <li>Use the 2 wrenches by the milling machine and align them respectively to the top of the spindle area and by the middle bit area where their imprints are</li> <li>Have you or a partner hold the bit itself while you unscrew it. This is to prevent the bit from dropping onto the bed and potentially breaking.</li> <li>To loosen the bit, bring the 2 wrenches in towards each other</li> <li>If using multiple bits, select them all under File Setup. Start with the smallest tool and make your way to the largest.</li> </ul> </li> <li>Probe<ul> <li>Move the metal prong from the side of the bed to sit on top of it. Make sure that they touch so electricity can conduct.</li> <li>Click <code>Z Only Stick Probing</code> and use the Jog menu to move the spindle above your material. </li> <li>Once the spindle is positioned above your material, start the probing.</li> </ul> </li> <li>Under Material Setup, leave the <code>Material Offset Z</code> at 0.01mm</li> <li>Change the <code>Material Size</code> to the dimensions of your material.</li> <li>Import your file under File Setup</li> <li>Choose the bit(s) you will be using</li> <li>You can see the projection of the result on the right side of the screen. Use the <code>Plan Offset</code> x, y, and z setting to move the placement of the design around.</li> <li>Ensure that you have the correct bit selected and inserted into the machine.</li> <li>Run the job from the Summary/Run section. It is recommended to run the engraving job before the cutting job to ensure that your material does not move around.</li> <li>If you are using multiple bits, the machine will pause during the job and prompt you to change bits.</li> </ul>"},{"location":"edm2/lessons/milling/#dog-tag","title":"Dog Tag","text":"<p>Before milling the dog tag, I brainstormed an idea for what the tag would look like. I decided upon a simple design with just my name on the tag.</p> <p> </p> <p>I then created the design in CorelDraw. To do this, I first imported a template of the dogtag. This template functions as effective boundaries for my design to ensure that they can all be milled inside the actual dogtag. I then created a textbox inside the area of the dogtag and type my name into it. The following image image contains both the outline and the text, but when actually milling the file, I deleted the outline and only milled the text, as the outline is just a reference to guage size and there is no need to cut out the outline of the dog tag because it is already an individual material piece.</p> <p> </p> <p>I then sent the file to Bantam Software and milled it on the Bantam Desktop CNC Machine. I first set the dimensions of the material to the size of the dogtag, measured via calipers. Next, I equipped the 1/8\" bit and used Bantam Software's <code>Install Tool</code> function to ensure that the tool was homed and its length was calibrated correctly. I then used the <code>Z Only Stick Probing</code> function to calibrate the height offset of my material. I then ran the job.</p> <p> <p></p> <p>The main problem I encountered was the machine not milling deep enough to actually engrave the material, as the bit would either only barely touch the dog tag or start engraving in the air altogether. This problem was solved by rebooting both the machine and the software and redoing the tool length calibration and Z Material Offset calibration. Here is the final milled dogtag:</p> <p> </p> <p></p>"},{"location":"edm2/lessons/milling/#chocolate-mold","title":"Chocolate Mold","text":"<p>Before starting to design the mold, I created a quick sketch for my idea of the mold. Each member of our group created an individual design so that we could get different perspectives about the mold's design and aggregate them all into a final design.</p> <p> </p> <p>After discussing with my groupmates, we decided to use the following design for the mold.</p> <p> </p> <p>I then created the design in Fusion360. I first created a cube, then extruded a heart-shaped hole on the top face of the cube.</p> <p> </p> <p>I then converted the design to a toolpath. I changed workspaces from Design to Manufacturing. I then selected 3D Adaptive Clearing and selected the heart hole, then generated the design. I later realized that this actually caused the final design to be inverted, where the heart shaped hole was not cut out and everything else was cut out. If I were to redo the milling process, I would make sure to select the other components of the design as to make sure the heart indentation was being cutout. Unfortunately, I did not realize this mistake at the time and thus the wax mold would be inverted. However, the rest of the milling process is correct.</p> <p> </p> <p>Here is the simulation of the toolpaths. If I had realized to use the simulate function to check the actual milling toolpaths, I may have realized the error before milling. However, nobody in our group realized that the simulation function existed until after we finished milling.</p> <p> <p></p> <p>The wax mold was milled on a Bantam Tools Desktop CNC Milling Machine, as opposed to the dog tag which was milled on an Othermill Milling Machine. The process for milling was very similar as both machines used the Bantam Tools Desktop Milling Machine Software as a gateway for milling designs (the producers of the Othermill and Bantam machines are the same company).</p> <p> <p></p> <p>Here is the final mold after milling:</p> <p> </p>"},{"location":"edm2/lessons/milling/#chocolate-box","title":"Chocolate Box","text":"<p>To complement the chocolate mold and the Valentine's Day theme, I also designed and cut a chocolate box on special cardstock that had a gold infill. I designed the box in CorelDraw with tabs so that we could use small bits of double-sided tape to hold the box together while maintaining food safety, and designed a tab on the front to be able to close the box. I put raster lines where two faces met together so that we would be able to fold the box easier after the cut.</p> <p> </p> <p>We then sent the design to the laser cutter.</p> <p> <p></p> <p>Here is the box after putting the bits of tape on the tabs:</p> <p> </p> <p>For fun, we disassembled the box and engraved a hearts design on the top of the box. Here is the box (closed via the tab) with the hearts design on top.</p> <p> </p>"},{"location":"edm2/lessons/milling/#overall-problems","title":"Overall Problems","text":"<p>Most of this project went smoothly. The only area in which I encountered some difficulty was milling. At first, when running my dog tag job, the milling machine wouldn't probe correctly and would start cutting in the air, which was solved by restarting the machine and the software and re-running the 1 axis Z probing. When running the wax mold job, the heart symbol was inversed and was left uncut while the rest of the mold was cut. Unfortunately, this issue was not resolved before we cut the wax as we did not realize its presence, but in the future, this issue could be solved by running a simulation of the job before actually cutting.</p>"},{"location":"edm2/lessons/pressingCharges/","title":"Pressing Charges","text":"<p>For this project, I was given a soldering kit for a small handheld gaming console. The objective of this lesson was to teach us how to identify and understand the importance of each component of the console. We were able to practice through-hole soldering on a variety of components, and I also experimented with solder paste. I also learned how to operate the solder-sucker gun when I was fixing some issues I encountered during soldering.</p> <p>NB: All of the images I took got hypercompressed far more than usual. I'm not exactly sure why this is the case as they look fine on my end. However, this should not affect the documentation as all key features are still visible.</p> <p>I first took all of the parts out of the kit.</p> <p> </p> <p>I then further unboxed all of the electronics components to ensure that I had everything.</p> <p> <p></p> <p>Next, I identified all of my parts to prepare them for soldering.</p> <p> </p> <p>I then soldered on the 10uF capacitor, ceramic 104 capacitor, on/off tactile button switch, speaker, 1k ohm resistor, and triode.</p> <p> <p></p> <p>Next, I soldered on the 5 control buttons.</p> <p> <p></p> <p>I then soldered on the LED display and the USB power base.</p> <p> <p></p> <p>I soldered on the base that would hold the chip in place and connect its pins to the PCB traces. I was able to use solder paste for this component as it was a lot of solder holes all lined up together.</p> <p> <p></p> <p>I then soldered on the 2 LED dot matrices. This was the only major problem I ran into during this project, as I misread the polarity of the dot matrices and soldered both on backwards. However, due to this rather significant mistake, I was able to learn how to use the solder sucker gun, which was incredibly effective and even surprised me in that it allowed me to unsolder over 30 joints in just a few minutes. After correcting the polarity of the LED matrices, this is what my board looked like.</p> <p> <p></p> <p>I then pushed the chip into the chip holder. This took a bit of work to align each pin and I used tweezers to push in a couple of stubborn pins that refused to go into their spots. After pushing in the chip, these two images are the final images of the front and back of my board respectively without the installation of button caps.</p> <p> <p></p> <p>I then put on the button caps and assembled the acrylic case for the board.</p> <p> </p> <p>Here is a video of my completely assembled console while I play tetris on it.</p> <p> <p></p> <p>Overall, this unit was pretty straightforward and relatively simple. The only major issue I encountered along the way was misreading the polarity of the LED matrices and soldering them on backwards. However, this was a great opportunity to learn how to use the solder-sucker gun, which proved to be incredibly effective in allowing me to remove and resolder the matrices on. I was so impressed by the solder-sucking gun that I would end up using it on other projects such as the production of a SAMD11C PCB where I accidentially soldered a component on backwards.</p>"},{"location":"smathhacks/smathhacks2025/","title":"SMathHacks2025","text":"<p>This page includes documentation for ARES-M: Atmospheric Remote Environmental System for Mars, submitted for the Hardware track at the  SMathHacks 2025 hackathon. Our team members include Richard Shan, Aaditya Sah, Trevor Bedson, and Josh Chilukuri. ARES is a remote autonomous sensor suite for Mars, monitoring temperature, humidity, seismic activity, UV/IR, and soil moisture in real time to advance planetary science and environmental research. SMathHacks 2025 was hosted 2/8/2025 to 2/9/2025.</p> <p> </p> <p>ARES is an integrated hardware-software solution featuring a remote sensor suite to detect temperature, humidity, seismic activity, and soil moisture monitoring on Mars. The nature of ARES' scalable mesh network deployment enables real-time environmental data acquisition across a distributed sensor array on multiple varied geographic locations simultaneously. A geospatially interactive interface allows users to access and analyze live telemetry along with a visualization of recently recorded data.</p>"},{"location":"smathhacks/smathhacks2025/#inspiration","title":"Inspiration","text":"<p>The idea for ARES emerged from the need for a more efficient and scalable approach to environmental monitoring on Mars. Existing planetary weather stations such as NASA\u2019s InSight lander and Curiosity\u2019s REMS provide valuable atmospheric and seismic data but are limited in coverage and flexibility. InSight, for example, offers stationary readings from a single location, while Curiosity\u2019s sensors can only gather data wherever the rover happens to be. These limitations pose challenges for understanding global weather patterns, seismic activity, and potential water presence on Mars. Given the planet\u2019s extreme conditions\u2014dust storms, drastic temperature shifts, and geological activity\u2014there is a critical need for a distributed, autonomous sensor system capable of continuous real-time data collection across multiple locations.</p> <p> </p> <p>ARES was designed to address these challenges through a scalable mesh network of remote sensor pods that can be deployed across varied Martian terrain. These pods continuously monitor temperature, humidity, seismic activity, and soil moisture, providing high-resolution environmental data to researchers. AI integration allows for real-time processing, adaptive calibration, and anomaly detection, enhancing data accuracy and reliability in Mars\u2019 harsh conditions. The system is paired with a geospatial visualization platform, enabling users to interact with a live map of Mars, click on sensor locations, and access real-time environmental data streams. Inspired by terrestrial meteorological and seismic networks, ARES brings a planetary-scale sensing solution to Mars, laying the groundwork for future habitat planning, climate research, and exploration missions.</p>"},{"location":"smathhacks/smathhacks2025/#ideation","title":"Ideation","text":"<p>Our initial vision for ARES was centered on creating a comprehensive atmospheric and environmental monitoring system, with a strong focus on air quality analysis. Originally, we planned to incorporate sensors for volatile organic compounds (VOC), carbon dioxide (CO\u2082), and particulate matter (PM2.5/PM10) to better understand Mars\u2019 atmospheric composition and potential hazards. These sensors would have allowed us to analyze dust composition, detect trace gases, and assess air quality, providing insights into both habitability and long-term climate trends on the planet. However, we faced a lot of logistical challenges considering our timeframe was only 2 days long. Our ordered sensors did not arrive in time, and forced us to adapt our approach while maintaining the core functionality of the system.</p> <p>To ensure that ARES remained a robust and scientifically valuable solution, we pivoted to include sunlight sensors (UV/IR) and seismic monitoring instead. This adjustment allowed us to capture even more important environmental data despite our sensors not being shipped properly. Specifically, ultraviolet and infrared radiation monitoring is particularly relevant for Martian exploration since it helps assess surface conditions, radiation exposure risks for future missions, and potential atmospheric dust interactions. The shift also streamlined our hardware integration as it allowed us to develop and assemble the entire device without having any uncertainty about parts. While our final implementation has slightly different measurements than my original plan, our central idea remains the same and our changes have only reinforced the robustness of ARES.</p>"},{"location":"smathhacks/smathhacks2025/#cad","title":"CAD","text":"<p>Since we developed the entire device in only two days, we created a simple yet highly effective and theme-matching CAD model. We created a hexagonal case for our electronics, with spots on the top for the sunlight and temperature/humidity sensor, as those needed access to light or the air respectively. We created an opening on the bottom of the hexagonal case for the soil moisture sensor to be deployed from. Spikes were added on the bottom as a way for the device to mount into extraterrestrial soil when dropped from a decent height, and serves to secure the device in place. We notate the ARES name and Pod number on the top of each individual pod.</p> <p> </p> <p>Internally, we designed mounts for the temperature/humidity sensor, sunlight sensor, and the MPU6050. The temperature/humidity sensor mount is shown on the left side of this image, and has a mesh on top of it so that the sensor is not completely exposed to the elements whilst also being able to access air for measuring humidity and temperature. The sunlight sensor mount is on the right side of this internal image, and faces directly upward into the sky to access the light. In production, we put a section of clear tape over the hole so that the sensor is protected while also being able to access its necessary measurements. The accelerometer and gyroscope sensor is mounted to the print directly, and is aided in contact area by the mount located in the upper right extrusion in this image. The servo is mounted in production directly to the print.</p> <p> </p> <p>The locations for the temperature/humidity sensor and sunlight sensor are clearly visible in the top view of the design. The mesh underneath the Pod 1 label houses the temperature/humidity sensor. The small cutout above the ARES insignia is where the sunlight sensor is mounted, and protected by a tape-over.</p> <p> </p> <p>Finally, we added spikes on the bottom to act as stakes to secure the ARES Pod to the ground, and protect it in dangerous Martian conditions such as storms and earthquakes. It is important  that ARES is able to survive these events and continue transmitting data, as gathering data about these abnormalities is a key application of ARES.</p> <p> </p>"},{"location":"smathhacks/smathhacks2025/#electrical-engineering","title":"Electrical Engineering","text":"<p>Before beginning any electrical engineering, we first researched the pinouts of all the sensors that we were to use. We then assembled the entire circuit in a basic schematic to plan out everything. Unfortunately, during these two days we were unable to access a desktop CNC machine to mill a PCB, so we never ended up creating a board design and instead simply referred to the schematic during design and soldering.</p> <p> </p> <p>Before we assembled the finalized soldered circuit, we first tested using a test ESP32 Feather V2 board on a breadboard circuit. We used the same wiring that would be our final design, excluding the servo that was added in a later iteration.</p> <p> </p> <p>As the ESP32 Feather V2 board was NCSSM property, we switched over to a spare Xiao Seeed ESP32S3 chip that Richard had. We chose the ESP32 architecture due to its native Wi-Fi capabilities, as we knew we would send data over a wireless network to the frontend. This involved minor changes in our code. Here is what our final circuit looks like when it is integrated into the system.</p> <p> </p> <p>We implemented a servo on the bottom to allow the soil moisture sensor to be deployed into the soil without being harmed by the initial ground impact.</p> <p> <p></p> <p>We used the following sensors: MPU6050 gyroscope/accelerometer, SI1145 sunlight sensor, DHT11 temperature/humidity sensor, and a capacitive soil moisture sensor. The following is a picture of our final product.</p> <p> </p>"},{"location":"smathhacks/smathhacks2025/#backend-software","title":"Backend Software","text":"<p>Our following ESP32-based embedded system script establishes a Wi-Fi connection to facilitate remote environmental and motion data acquisition, processing, and transmission. It integrates multiple sensor modalities, including an MPU6050 inertial measurement unit (IMU) for three-axis acceleration and angular velocity, a DHT11 for temperature and humidity sensing, an SI1145 for ultraviolet (UV) and infrared (IR) irradiance measurement, and a capacitive soil moisture sensor for substrate hydration assessment. A servo motor is utilized for lowering the soil moisture sensor into the ground for more accurate readings.</p> <p>The system utilizes I2C communication protocol to acquire real-time sensor readings, which are encapsulated within a structured JSON object via the ArduinoJson library. This JSON payload is transmitted via an HTTP POST request to a remote server hosting the frontend API endpoint and is secured through authentication tokens. The pod transmits data through the mesh network to the endpoint every 5 seconds to ensure live-time data measurements. The system concurrently logs network status and signal strength to the serial interface for diagnostic purposes, since we experienced many issues with Wi-Fi during development.</p> <pre><code>#include &lt;WiFi.h&gt;\n#include &lt;HTTPClient.h&gt;\n#include &lt;ArduinoJson.h&gt;\n#include &lt;Wire.h&gt;\n#include &lt;Adafruit_MPU6050.h&gt;\n#include &lt;Adafruit_Sensor.h&gt;\n#include &lt;dht11.h&gt;\n#include \"SI114X.h\"\n#include &lt;ESP32Servo.h&gt;\n\n// Wi-Fi Credentials\nchar ssid[] = \"redacted\";\nchar pass[] = \"redacted\";\n\n\n// API Endpoint and Auth\nconst char* serverURL = \"https://mars.prorickey.xyz/api\";\nconst char* authToken = \"CYcagejUv3tJwTfEQMWxrF2ALnkVm87RP5hNybHZBDXdGu9zsK\";\n\n// MPU6050 (Gyroscope/Accelerometer)\nAdafruit_MPU6050 mpu;\n\n// DHT11 (Temperature/Humidity)\n#define DHT11PIN 3\ndht11 DHT11;\n\n// SI1145 (Sunlight sensor)\nSI114X SI1145 = SI114X();\n\n#define SERVO_PIN 1 \n\nServo myServo;\n\n// Soil Moisture sensor (analog pin A3)\nconst int moisturePin = 4;\n\nvoid setup() {\n  // ----- Serial &amp; Wi-Fi Setup -----\n  myServo.attach(SERVO_PIN);\n  Serial.println(\"Servo initialized successfully!\");\n  myServo.write(0);\n\n  Serial.begin(115200);\n  while (!Serial) { }\n\n  Serial.print(\"Attempting to connect to SSID: \");\n  Serial.println(ssid);\n\n  //WiFi.useStaticBuffers(true);\n  WiFi.mode(WIFI_STA);\n  WiFi.begin(ssid, pass);\n  while (WiFi.status() != WL_CONNECTED) {\n    delay(500);\n    Serial.print(\".\");\n  }\n\n  delay(500);\n\n  Serial.println(\"\");\n  Serial.println(\"Connected to WiFi\");\n  printWifiStatus();\n\n  myServo.write(90);\n  Serial.println(\"Moving to Down (90\u00b0)\");\n  delay(1000);  // Hold for 1 second\n\n  // ----- I2C Initialization -----\n  Wire.begin();\n\n  // ----- Sensor Initialization -----\n  Serial.println(\"Initializing sensors...\");\n\n  // MPU6050 Initialization\n  if (!mpu.begin()) {\n    Serial.println(\"Failed to find MPU6050 chip\");\n    while (1) { delay(10); }\n  }\n  Serial.println(\"MPU6050 Found!\");\n\n  // Set MPU6050 ranges and bandwidth\n  mpu.setAccelerometerRange(MPU6050_RANGE_8_G);\n  mpu.setGyroRange(MPU6050_RANGE_500_DEG);\n  mpu.setFilterBandwidth(MPU6050_BAND_5_HZ);\n\n  // SI1145 (Sunlight Sensor) Initialization\n  Serial.println(\"Initializing SI1145 (Sunlight Sensor)...\");\n  while (!SI1145.Begin()) {\n    Serial.println(\"SI1145 not ready, retrying...\");\n    delay(1000);\n  }\n  Serial.println(\"SI1145 is ready!\");\n\n  Serial.println(\"\\nAll sensors initialized!\\n\");\n  delay(500);\n}\n\nvoid loop() {\n  unsigned long currentTime = millis();\n\n  // ----- Read MPU6050: Acceleration, Gyro, and Internal Temperature -----\n  sensors_event_t a, g, temp;\n  mpu.getEvent(&amp;a, &amp;g, &amp;temp);\n\n  // ----- Read DHT11: Temperature and Humidity -----\n  DHT11.read(DHT11PIN);\n\n  // ----- Read SI1145: Visible Light, IR, UV -----\n  uint16_t ir = SI1145.ReadIR();\n  float uvIndex = SI1145.ReadUV() / 100.0;  // Convert raw value to UV index\n\n  // ----- Read Soil Moisture Sensor -----\n  int moistureValue = analogRead(moisturePin);\n  int moisturePercent = map(moistureValue, 4095, 1200, 0, 100);\n  moisturePercent = constrain(moisturePercent, 0, 100);\n\n  // ----- Build JSON Object using ArduinoJson -----\n  StaticJsonDocument&lt;512&gt; jsonDoc;\n  jsonDoc[\"time\"] = currentTime;\n  JsonArray stations = jsonDoc.createNestedArray(\"stations\");\n\n  JsonObject station = stations.createNestedObject();\n  station[\"id\"] = \"STATION_1\";       // Replace with real ID if needed\n  station[\"latitude\"] = 0.1;           // Replace with actual GPS data if available\n  station[\"longitude\"] = 0.1;          // Replace with actual GPS data if available\n  station[\"temperature\"] = DHT11.temperature;\n  station[\"humidity\"] = DHT11.humidity;\n  station[\"uv\"] = uvIndex;\n  station[\"ir\"] = ir;\n  station[\"moisture\"] = moisturePercent;\n\n  JsonObject accel = station.createNestedObject(\"accel\");\n  accel[\"x\"] = a.acceleration.x;\n  accel[\"y\"] = a.acceleration.y;\n  accel[\"z\"] = a.acceleration.z;\n\n  JsonObject angVelocity = station.createNestedObject(\"angVelocity\");\n  angVelocity[\"x\"] = g.gyro.x;\n  angVelocity[\"y\"] = g.gyro.y;\n  angVelocity[\"z\"] = g.gyro.z;\n\n  String jsonPayload;\n  serializeJson(jsonDoc, jsonPayload);\n\n  // ----- Send HTTP POST Request -----\n  HTTPClient http;\n\n  http.begin(serverURL);\n  http.addHeader(\"Content-Type\", \"application/json\");\n  http.addHeader(\"Authorization\", \"Bearer \" + String(authToken));\n\n  int httpResponseCode = http.POST(jsonPayload);\n\n  Serial.println(\"Sending JSON Data...\");\n  Serial.println(jsonPayload);\n\n  if (httpResponseCode &gt; 0) {\n    Serial.print(\"Response Code: \");\n    Serial.println(httpResponseCode);\n    String response = http.getString();\n    Serial.println(\"Server Response: \" + response);\n  } else {\n    Serial.print(\"Error Sending Data. HTTP Response: \");\n    Serial.println(httpResponseCode);\n  }\n\n  http.end();\n  delay(1000);  // Send data every 5 seconds\n}\n\nvoid printWifiStatus() {\n  Serial.print(\"SSID: \");\n  Serial.println(WiFi.SSID());\n\n  IPAddress ip = WiFi.localIP();\n  Serial.print(\"IP Address: \");\n  Serial.println(ip);\n\n  long rssi = WiFi.RSSI();\n  Serial.print(\"Signal strength (RSSI): \");\n  Serial.print(rssi);\n  Serial.println(\" dBm\");\n}\n</code></pre>"},{"location":"smathhacks/smathhacks2025/#frontend","title":"Frontend","text":"<p>The front end was developed using a Next.js framework powered by React, Typescript, Tailwind CSS, and Post CSS. A Postgresql database was deployed using Docker to collect real-time live data from the sensor. To interact with the data, we used Prisma ORM and created our API for the embedded system to send information. We used the Chart.js library to create data visualizations. </p> <p>Postgresql was chosen due to its proficiency in collecting time series and station data structures, which are highly compatible with the relational database and table. Postgresql\u2019s rapid and efficient data storage also allowed us to answer API requests effectively. We deployed our application using docker-compose to securely isolate the database while maintaining client-server interaction. Our front and back end were deployed on a local server, making docker the ideal selection to optimize build speed.. </p> <p>We structured our database to have a table for each station, and specific weather data payloads. This decision allowed us to quickly select databases and find associated weather data  by utilizing prisma\u2019s include parameter when querying.</p> <pre><code>model Station {\n id   String @unique\n name String\n\n\n weatherData WeatherData[]\n}\n\n\nmodel WeatherData {\n id Int @unique @default(autoincrement())\n\n\n latitude  Float\n longitude Float\n\n\n station   Station @relation(fields: [stationId], references: [id])\n stationId String\n\n\n pressure    Float\n temperature Float\n humidity    Float\n co2         Float\n dust        Float\n wind        Float\n uv          Float\n ir          Float\n moisture    Float\n light       Float\n accelX      Float\n accelY      Float\n accelZ      Float\n angVelX     Float\n angVelY     Float\n angVelZ     Float\n\n\n timeTaken DateTime\n}\n</code></pre> <p>We kept this structure consistent between internal data structures, and the format we receive data through the API. We originally included the position in the Station table, but later moved it to the time series data table to account for the stations being uprooted or relocated. </p> <p>In our GET API route, we utilized Prisma\u2019s powerful filtering features to limit and select specific items. We were able to take advantage of this because of our database structure choices. We filter by time, in ascending order, to select the latest entries, which have the largest time stamps. We further filter by only selecting the last 5 hours. Furthermore, we take the data, which contains 60 datapoints per second, and condense it down to 1 datapoint per minute using averages. </p> <pre><code>const data = await prisma.station.findMany({\n  include: {\n  weatherData: {\n   orderBy: {\n    timeTaken: \"asc\"\n   },\n   where: {\n    timeTaken: {\n     gte: new Date(new Date().getMilliseconds() - 1000 * 60 * 60 * 5) \n    }\n   }\n  },\n }\n})\n</code></pre> <p>Our POST API route is used to collect data from the station. It is designed to receive a request from our master node and contains the data for each of the pods. It includes very basic key bearer authentication that simplifies and protects our application. We chose the data structure for the payload in order to be consistent with internal data structures and the database structure, and it was easily implemented on the embedded system on our physical device. We also include data points for our simulated stations that slightly modify the actual data to simulate varying weather conditions in a region. </p> <p>Our UI design choices were based on speed and efficiency. This is why we decided to go with Typescript, as its strongly typed nature allows us to identify type errors before we run code quickly. We also decided to use Tailwindcss in order to quickly style our components. Finally, Next.js allows us to quickly test, build, and deploy. </p> <p>The web interface consists of an interactive map and a navigation bar containing information from the sensors. Different tabs display relevant charts and statistics for corresponding environmental measurements, including Earthquake detection (by calculating an aggregated average of accelerometer values),  soil moisture percentage, average/max/min temperature and humidity, light wavelengths, and geographic position. </p> <p>Using the library ChartJS alongside html canvas, we can create an interactive UI to showcase the data and allow users to vizualize ARES pods. ChartJS allowed us to quickly chart and format our data, and helped us greatly save time and create an appealing front end. Furthermore, html canvas allowed us to place the pods on the map in their exact positions based on their collected latitude and longitude. </p>"},{"location":"smathhacks/smathhacks2025/#integration","title":"Integration","text":"<p>The backend Arduino code integrates seamlessly with the frontend by using JSON as a standardized data format to transmit sensor information. Within the code, sensor readings from multiple devices\u2014such as temperature, humidity, UV index, IR levels, soil moisture, and motion data from the MPU6050\u2014are structured into a JSON document using the ArduinoJson library. This JSON payload encapsulates the sensor data along with metadata like timestamps and station identifiers, making it easy to manage and extend. Once the payload is constructed, it is serialized into a string and sent via an HTTP POST request to a remote API endpoint. This API, which is typically part of a backend service connected to the frontend, receives the JSON data and processes it accordingly. A sample JSON string which is transmitted will look like the following:</p> <pre><code>{\"time\":22651,\"stations\":[{\"id\":\"STATION_1\",\"latitude\":0.1,\"longitude\":0.1,\"temperature\":23,\"humidity\":36,\"uv\":0.02,\"ir\":254,\"moisture\":52,\"accel\":{\"x\":1.019929886,\"y\":-0.325611442,\"z\":9.361329079},\"angVelocity\":{\"x\":-0.041834611,\"y\":0.022915773,\"z\":-0.007993874}}]}\n</code></pre> <p>On the frontend, the received JSON data can be parsed and dynamically rendered using modern web development frameworks and libraries such as React, Angular, or Vue.js. The structured JSON makes it straightforward for the frontend to extract specific sensor readings and display them in an interactive dashboard\u2014using charts, graphs, and real-time status updates. This approach allows for scalable integration where additional sensor data or stations can be added without disrupting the data flow. </p>"},{"location":"smathhacks/smathhacks2025/#impact","title":"Impact","text":"<p>The integration of this system into an IoT-based monitoring framework has significant implications for real-time environmental sensing, data transmission, and decision-making. By systematically acquiring data from multiple sensors\u2014including temperature, humidity, ultraviolet (UV) radiation, infrared (IR) levels, soil moisture, and motion tracking via the MPU6050\u2014the system ensures a comprehensive representation of environmental conditions. The collected data is structured into a standardized JSON format, optimizing interoperability between hardware and software layers. Utilizing HTTP POST requests for data transmission enables continuous and automated logging, reducing the reliance on manual intervention and enhancing data accuracy. This approach facilitates real-time analytics and predictive modeling, supporting applications in smart agriculture, climate monitoring, and industrial process optimization.</p> <p>From a systems engineering perspective, the use of JSON as a lightweight and structured data exchange format enhances system scalability and integration with web-based visualization tools. The modularity of the architecture allows for seamless expansion, enabling the incorporation of additional sensors without substantial modifications to the data processing pipeline. Furthermore, real-time data dissemination to frontend interfaces, such as interactive dashboards, enhances user engagement and decision support capabilities. This architecture enables advanced analytical techniques, including anomaly detection and machine learning-driven forecasting, by providing structured and time-synchronized datasets. The ability to efficiently collect, transmit, and visualize high-frequency sensor data has profound implications for scientific research, precision monitoring, and the development of autonomous systems.</p>"},{"location":"smathhacks/smathhacks2025/#future-work","title":"Future Work","text":"<p>Future developments in this system can focus on enhancing data processing efficiency, expanding sensor integration, and improving real-time analytics capabilities. One key direction is the implementation of edge computing techniques to preprocess sensor data before transmission, reducing network congestion and minimizing latency in real-time applications. By incorporating onboard machine learning models, such as TinyML for anomaly detection or predictive analytics, the system could autonomously identify critical environmental changes and trigger automated responses. Additionally, improving the robustness of data transmission by implementing MQTT or WebSocket protocols could enable more efficient, low-latency communication compared to the current HTTP-based approach. Furthermore, integrating secure encryption mechanisms for JSON payloads and authentication protocols, such as OAuth 2.0, could enhance data integrity and privacy when transmitting sensitive environmental data to cloud-based platforms.</p>"},{"location":"smathhacks/smathhacks2025Submitted/","title":"ARES-M: Atmospheric Remote Environmental System for Mars","text":"<p>Technical Documentation</p> <p>This page includes documentation for ARES-M: Atmospheric Remote Environmental System for Mars, submitted for the Hardware track at the SMathHacks 2025 hackathon. Our team members include Richard Shan (primary contact), Aaditya Sah, Trevor Bedson, and Josh Chilukuri. Our project, ARES, is a remote autonomous sensor suite for Mars, monitoring temperature, humidity, seismic activity, UV/IR, and soil moisture in real time to advance planetary science and environmental research. SMathHacks 2025 was hosted 2/8/2025 to 2/9/2025.</p> <p>For a short summary, our project video, final product, and key images can be seen in the media gallery, accessible by clicking the arrows on either side of the video shown above. Note that the video is an ABRIDGED SUMMARY of the work done, and that the technical documentation is a far more comprehensive resource. Due to limited time, we also recognize that the technical documentation lacks full coverage, but it covers all important steps.</p> <p> </p> <p>ARES is an integrated hardware-software solution featuring a remote sensor suite to detect temperature, humidity, seismic activity, and soil moisture monitoring on Mars. The nature of ARES' scalable mesh network deployment enables real-time environmental data acquisition across a distributed sensor array on multiple varied geographic locations simultaneously. A geospatially interactive interface allows users to access and analyze live telemetry along with a visualization of recently recorded data.</p>"},{"location":"smathhacks/smathhacks2025Submitted/#judge-and-user-guide","title":"Judge and User Guide","text":"<p>Although ARES is at heart, a hardware project, we recognize that it is difficult to judge hardware projects remotely and asynchronously, without access to the hardware and guarantees that it is on and running. Thus, we developed a fully integrated frontend which is publicly accessible here.</p> <p> </p> <p>Clicking any of the stations opens a menu on the left side of the screen. The icons in the toolbar represent:</p> Icon Displayed Information Mars Planet Icon Home Radio Tower Icon Pod Latitude/Longitude Info Plant Icon Live Soil Moisture Over Time Info Sun Icon Live IR and UV Measurements Over Time Info Wind Icon Live Temperature and Humidity Data Over Time Home Icon Live Status of Earthquakes and Seismic Activity <p>During the hackathon, we had time to develop one functional ARES prototype. If ARES is currently on, Perseverance's (Station 1) information and charts will be updated in live time. More realistically, ARES will not be plugged in when judging takes place (as I do have school) and Perseverance (Station 1) will be showing information from the last 10 times ARES sent back data. As we only developed one prototype, Stations 2-6 are displaying simulated data and do not reflect real world information. We simulate the other stations to demonstrate our mesh network proof-of-concept for peer-to-peer information transfer between ARES Pods.</p> <p>This will make a lot more sense after reading the rest of the documentation.</p>"},{"location":"smathhacks/smathhacks2025Submitted/#intended-usage","title":"Intended Usage","text":"<p>ARES is designed to be deployed as a system of dozens or hundreds of pods on the surface of a planet, i.e. Mars. Conventional sensing robots and systems are only able to track a single location (the current location of the rover). ARES' novel approach of building a peer-to-peer mesh network allows the deployment of hundreds of sensors across a large area of Mars, that all concurrently stream data to our frontend data tracker. ARES can be dropped by a rover or dropped out of the sky into remote and unreachable locations, and is able to continuously monitor temperature, humidity, seismic, UV, IR, and moisture levels of its location. ARES is a first-of-a-kind approach to long-term scalable environmental pattern monitoring across a large and distant area.</p>"},{"location":"smathhacks/smathhacks2025Submitted/#inspiration","title":"Inspiration","text":"<p>The idea for ARES emerged from the need for a more efficient and scalable approach to environmental monitoring on Mars. Existing planetary weather stations such as NASA\u2019s InSight lander and Curiosity\u2019s REMS provide valuable atmospheric and seismic data but are limited in coverage and flexibility. InSight, for example, offers stationary readings from a single location, while Curiosity\u2019s sensors can only gather data wherever the rover happens to be. These limitations pose challenges for understanding global weather patterns, seismic activity, and potential water presence on Mars. Given the planet\u2019s extreme conditions\u2014dust storms, drastic temperature shifts, and geological activity\u2014there is a critical need for a distributed, autonomous sensor system capable of continuous real-time data collection across multiple locations.</p> <p> </p> <p>ARES was designed to address these challenges through a scalable mesh network of remote sensor pods that can be deployed across varied Martian terrain. These pods continuously monitor temperature, humidity, seismic activity, and soil moisture, providing high-resolution environmental data to researchers. AI integration allows for real-time processing, adaptive calibration, and anomaly detection, enhancing data accuracy and reliability in Mars\u2019 harsh conditions. The system is paired with a geospatial visualization platform, enabling users to interact with a live map of Mars, click on sensor locations, and access real-time environmental data streams. Inspired by terrestrial meteorological and seismic networks, ARES brings a planetary-scale sensing solution to Mars, laying the groundwork for future habitat planning, climate research, and exploration missions.</p>"},{"location":"smathhacks/smathhacks2025Submitted/#ideation","title":"Ideation","text":"<p>Our initial vision for ARES was centered on creating a comprehensive atmospheric and environmental monitoring system, with a strong focus on air quality analysis. Originally, we planned to incorporate sensors for volatile organic compounds (VOC), carbon dioxide (CO\u2082), and particulate matter (PM2.5/PM10) to better understand Mars\u2019 atmospheric composition and potential hazards. These sensors would have allowed us to analyze dust composition, detect trace gases, and assess air quality, providing insights into both habitability and long-term climate trends on the planet. However, we faced a lot of logistical challenges considering our timeframe was only 2 days long. Our ordered sensors did not arrive in time, and forced us to adapt our approach while maintaining the core functionality of the system.</p> <p>To ensure that ARES remained a robust and scientifically valuable solution, we pivoted to include sunlight sensors (UV/IR) and seismic monitoring instead. This adjustment allowed us to capture even more important environmental data despite our sensors not being shipped properly. Specifically, ultraviolet and infrared radiation monitoring is particularly relevant for Martian exploration since it helps assess surface conditions, radiation exposure risks for future missions, and potential atmospheric dust interactions. The shift also streamlined our hardware integration as it allowed us to develop and assemble the entire device without having any uncertainty about parts. While our final implementation has slightly different measurements than my original plan, our central idea remains the same and our changes have only reinforced the robustness of ARES.</p>"},{"location":"smathhacks/smathhacks2025Submitted/#cad","title":"CAD","text":"<p>Since we developed the entire device in only two days, we created a simple yet highly effective and theme-matching CAD model. We created a hexagonal case for our electronics, with spots on the top for the sunlight and temperature/humidity sensor, as those needed access to light or the air respectively. We created an opening on the bottom of the hexagonal case for the soil moisture sensor to be deployed from. Spikes were added on the bottom as a way for the device to mount into extraterrestrial soil when dropped from a decent height, and serves to secure the device in place. We notate the ARES name and Pod number on the top of each individual pod.</p> <p> </p> <p>Internally, we designed mounts for the temperature/humidity sensor, sunlight sensor, and the MPU6050. The temperature/humidity sensor mount is shown on the left side of this image, and has a mesh on top of it so that the sensor is not completely exposed to the elements whilst also being able to access air for measuring humidity and temperature. The sunlight sensor mount is on the right side of this internal image, and faces directly upward into the sky to access the light. In production, we put a section of clear tape over the hole so that the sensor is protected while also being able to access its necessary measurements. The accelerometer and gyroscope sensor is mounted to the print directly, and is aided in contact area by the mount located in the upper right extrusion in this image. The servo is mounted in production directly to the print.</p> <p> </p> <p>The locations for the temperature/humidity sensor and sunlight sensor are clearly visible in the top view of the design. The mesh underneath the Pod 1 label houses the temperature/humidity sensor. The small cutout above the ARES insignia is where the sunlight sensor is mounted, and protected by a tape-over.</p> <p> </p> <p>Finally, we added spikes on the bottom to act as stakes to secure the ARES Pod to the ground, and protect it in dangerous Martian conditions such as storms and earthquakes. It is important  that ARES is able to survive these events and continue transmitting data, as gathering data about these abnormalities is a key application of ARES.</p> <p> </p>"},{"location":"smathhacks/smathhacks2025Submitted/#sensors-analysis","title":"Sensors Analysis","text":"<p>We plan to be able to measure temperature, humidity, seismic activity, sunlight levels, and soil moisture levels. Temperature and humidity are crucial for understanding environmental conditions, predicting weather patterns, and assessing habitability for both equipment and potential life. Seismic activity measurements help detect ground vibrations, assess tectonic movement, and analyze subsurface structures, providing insights into planetary formation and stability. Sunlight levels, particularly UV and IR radiation, are key for evaluating solar energy availability, studying atmospheric interactions, and identifying potential hazards due to high radiation exposure. Soil moisture detection allows for assessing water content, determining surface composition, and identifying potential regions for resource utilization, which is essential for astrobiology and future exploration. The following table illustrates our choice of sensor based on their availability in a short time frame and their capabilities.</p> Measurement Type Sensor Model Parameters Measured Communication Notes Temperature DHT11 Temperature (\u00b0C) Digital Used for recording temperatures Humidity DHT11 Relative Humidity (%) Digital Integrated with temperature sensor Seismic Activity MPU6050 Acceleration (X, Y, Z), Gyro (X, Y, Z) I2C Used to detect ground vibrations &amp; movement, important for measuring seismic activity on Mars UV &amp; IR SI1145 UV Index, Infrared I2C UV and IR have many uses. Some include the ability to assess habitability, detect water ice, track dust storms, and optimize solar power for future exploration. Soil Moisture Capacitive Soil Moisture Sensor Soil Moisture Level Analog Moisture level is based on capacitance, which correlates with water concentration level. Measuring soil moisture on Mars and exoplanets aids in detecting water, analyzing geochemistry, and identifying potentially habitable environments for microbial life. <p>We ensure that the sensors we want to use are available. We thank Mr. Michael D\u2019Argenio from NCSSM for securing access to the hardware necessary for this project. </p> Metric Relevant Sensors Available Temperature DHT11 \u2705 Humidity DHT11 \u2705 Seismic Activity MPU6050 \u2705 UV Radiation SI1145 \u2705 Infrared Radiation SI1145 \u2705 Soil Moisture Capacitive Soil Moisture Sensor \u2705 <p>Now that we know what sensors we will use, we will analyze their pinouts and datasheets to ensure that we are able to implement them with the chip we want to develop on (Xiao Seeed ESP32S3). We would prefer to run the sensors at 5V, but do have a 3V3 available.</p> Sensor Model Communication Pinout Power Datasheet DHT11 Digital (Single-wire) VCC, GND, Data 5V DHT11 Datasheet MPU6050 I2C VCC, GND, SCL, SDA, INT 5V MPU6050 Datasheet SI1145 I2C VCC, GND, SCL, SDA 3.3V SI1145 Datasheet Capacitive Soil Moisture Sensor Analog VCC, GND, Analog Output 3.3V Capacitive Soil Moisture Sensor Datasheet <p>Now that we've verified the feasibility of our project in theory, it is time to implement it. Note that we implemented a wind speed tracker in the GUI expecting the sensor to arrive the afternoon of 2/9, but it never arrived. Thus, we simulate wind speed data in the frontend interface. Wind speed data is not taken from our sensors and does not represent real world information.</p>"},{"location":"smathhacks/smathhacks2025Submitted/#electrical-engineering","title":"Electrical Engineering","text":"<p>We first assembled the entire circuit in a basic schematic to plan out everything. Unfortunately, during these two days we were unable to access a desktop CNC machine to mill a PCB, so we decided to not create an actual board design to save time and instead simply referred to the schematic during design and soldering. Due to limited time and lack of needing a proper PCB, Richard decided to forego foraging for footprints and instead spoofed them with labelled conn headers corresponding to sensor pinouts, a strategy that saved time and is sufficient for our current situation. In the future, we plan to continue this project after the hackathon and will design a PCB proper for production. </p> <p> </p> <p>Before we assembled the finalized soldered circuit, we first tested using a test ESP32 Feather V2 board on a breadboard circuit. We used the same wiring that would be our final design, excepting the servo. We later decided to implement a servo mechanism to lower the soil moisture sensor into the soil itself, to get more accurate readings.</p> <p> </p> <p>As the ESP32 Feather V2 board was NCSSM property, we switched over to a spare Xiao Seeed ESP32S3 chip that Richard had. We chose the ESP32 architecture due to its native Wi-Fi capabilities, as we knew we would send data over a wireless network to the frontend. This involved minor changes in our code, discussed later. Here is what our circuit looks like when it is integrated into the system. We strove for color-coding wires properly, but had to prototype without a PCB. Alas!</p> <p> </p> <p>As discussed earlier, we now implement a servo on the bottom of ARES to allow the soil moisture sensor to be deployed into the soil without being harmed by the initial ground impact. Devpost unfortunately doesn't allow for video displays in Markdown, so a short video of the servo in action can be found here. The following image shows the servo in the fully extended position with the moisture sensor at its furthest point in the ground.</p> <p> </p> <p>The following is a picture of our final product, placed on a table to emphasize the under-body spikes and moisture sensor positioning. In practice, the body of ARES sits above ground, while the spikes serve as stakes to secure ARES in place and the moisture sensor is underground to fulfill its mission of water detection.</p> <p> </p>"},{"location":"smathhacks/smathhacks2025Submitted/#backend-software","title":"Backend Software","text":"<p>Our following ESP32-based embedded system script establishes a Wi-Fi connection to facilitate remote environmental and motion data acquisition, processing, and transmission. It integrates multiple sensor modalities, including an MPU6050 inertial measurement unit (IMU) for three-axis acceleration and angular velocity, a DHT11 for temperature and humidity sensing, an SI1145 for ultraviolet (UV) and infrared (IR) irradiance measurement, and a capacitive soil moisture sensor for substrate hydration assessment. A servo motor is utilized for lowering the soil moisture sensor into the ground for more accurate readings.</p> <p>The system utilizes I2C communication protocol to acquire real-time sensor readings, which are encapsulated within a structured JSON object via the ArduinoJson library. This JSON payload is transmitted via an HTTP POST request to a remote server hosting the frontend API endpoint and is secured through authentication tokens. The pod transmits data through the mesh network to the endpoint every 5 seconds to ensure live-time data measurements. The system concurrently logs network status and signal strength to the serial interface for diagnostic purposes, since we experienced many issues with Wi-Fi during development.</p> <pre><code>#include &lt;WiFi.h&gt;\n#include &lt;HTTPClient.h&gt;\n#include &lt;ArduinoJson.h&gt;\n#include &lt;Wire.h&gt;\n#include &lt;Adafruit_MPU6050.h&gt;\n#include &lt;Adafruit_Sensor.h&gt;\n#include &lt;dht11.h&gt;\n#include \"SI114X.h\"\n#include &lt;ESP32Servo.h&gt;\n\n// Wi-Fi Credentials\nchar ssid[] = \"redacted\";\nchar pass[] = \"redacted\";\n\n\n// API Endpoint and Auth\nconst char* serverURL = \"https://mars.prorickey.xyz/api\";\nconst char* authToken = \"CYcagejUv3tJwTfEQMWxrF2ALnkVm87RP5hNybHZBDXdGu9zsK\";\n\n// MPU6050 (Gyroscope/Accelerometer)\nAdafruit_MPU6050 mpu;\n\n// DHT11 (Temperature/Humidity)\n#define DHT11PIN 3\ndht11 DHT11;\n\n// SI1145 (Sunlight sensor)\nSI114X SI1145 = SI114X();\n\n#define SERVO_PIN 1 \n\nServo myServo;\n\n// Soil Moisture sensor (analog pin A3)\nconst int moisturePin = 4;\n\nvoid setup() {\n  // ----- Serial &amp; Wi-Fi Setup -----\n  myServo.attach(SERVO_PIN);\n  Serial.println(\"Servo initialized successfully!\");\n  myServo.write(0);\n\n  Serial.begin(115200);\n  while (!Serial) { }\n\n  Serial.print(\"Attempting to connect to SSID: \");\n  Serial.println(ssid);\n\n  //WiFi.useStaticBuffers(true);\n  WiFi.mode(WIFI_STA);\n  WiFi.begin(ssid, pass);\n  while (WiFi.status() != WL_CONNECTED) {\n    delay(500);\n    Serial.print(\".\");\n  }\n\n  delay(500);\n\n  Serial.println(\"\");\n  Serial.println(\"Connected to WiFi\");\n  printWifiStatus();\n\n  myServo.write(90);\n  Serial.println(\"Moving to Down (90\u00b0)\");\n  delay(1000);  // Hold for 1 second\n\n  // ----- I2C Initialization -----\n  Wire.begin();\n\n  // ----- Sensor Initialization -----\n  Serial.println(\"Initializing sensors...\");\n\n  // MPU6050 Initialization\n  if (!mpu.begin()) {\n    Serial.println(\"Failed to find MPU6050 chip\");\n    while (1) { delay(10); }\n  }\n  Serial.println(\"MPU6050 Found!\");\n\n  // Set MPU6050 ranges and bandwidth\n  mpu.setAccelerometerRange(MPU6050_RANGE_8_G);\n  mpu.setGyroRange(MPU6050_RANGE_500_DEG);\n  mpu.setFilterBandwidth(MPU6050_BAND_5_HZ);\n\n  // SI1145 (Sunlight Sensor) Initialization\n  Serial.println(\"Initializing SI1145 (Sunlight Sensor)...\");\n  while (!SI1145.Begin()) {\n    Serial.println(\"SI1145 not ready, retrying...\");\n    delay(1000);\n  }\n  Serial.println(\"SI1145 is ready!\");\n\n  Serial.println(\"\\nAll sensors initialized!\\n\");\n  delay(500);\n}\n\nvoid loop() {\n  unsigned long currentTime = millis();\n\n  // ----- Read MPU6050: Acceleration, Gyro, and Internal Temperature -----\n  sensors_event_t a, g, temp;\n  mpu.getEvent(&amp;a, &amp;g, &amp;temp);\n\n  // ----- Read DHT11: Temperature and Humidity -----\n  DHT11.read(DHT11PIN);\n\n  // ----- Read SI1145: Visible Light, IR, UV -----\n  uint16_t ir = SI1145.ReadIR();\n  float uvIndex = SI1145.ReadUV() / 100.0;  // Convert raw value to UV index\n\n  // ----- Read Soil Moisture Sensor -----\n  int moistureValue = analogRead(moisturePin);\n  int moisturePercent = map(moistureValue, 4095, 1200, 0, 100);\n  moisturePercent = constrain(moisturePercent, 0, 100);\n\n  // ----- Build JSON Object using ArduinoJson -----\n  StaticJsonDocument&lt;512&gt; jsonDoc;\n  jsonDoc[\"time\"] = currentTime;\n  JsonArray stations = jsonDoc.createNestedArray(\"stations\");\n\n  JsonObject station = stations.createNestedObject();\n  station[\"id\"] = \"STATION_1\";       // Changed based on which ARES Pod is programmed\n  station[\"latitude\"] = 0.1;           // Hard coded for hackathon proof of concept, varies in deployment\n  station[\"longitude\"] = 0.1;          // Hard coded for hackathon proof of concept, varies in deployment\n  station[\"temperature\"] = DHT11.temperature;\n  station[\"humidity\"] = DHT11.humidity;\n  station[\"uv\"] = uvIndex;\n  station[\"ir\"] = ir;\n  station[\"moisture\"] = moisturePercent;\n\n  JsonObject accel = station.createNestedObject(\"accel\");\n  accel[\"x\"] = a.acceleration.x;\n  accel[\"y\"] = a.acceleration.y;\n  accel[\"z\"] = a.acceleration.z;\n\n  JsonObject angVelocity = station.createNestedObject(\"angVelocity\");\n  angVelocity[\"x\"] = g.gyro.x;\n  angVelocity[\"y\"] = g.gyro.y;\n  angVelocity[\"z\"] = g.gyro.z;\n\n  String jsonPayload;\n  serializeJson(jsonDoc, jsonPayload);\n\n  // ----- Send HTTP POST Request -----\n  HTTPClient http;\n\n  http.begin(serverURL);\n  http.addHeader(\"Content-Type\", \"application/json\");\n  http.addHeader(\"Authorization\", \"Bearer \" + String(authToken));\n\n  int httpResponseCode = http.POST(jsonPayload);\n\n  Serial.println(\"Sending JSON Data...\");\n  Serial.println(jsonPayload);\n\n  if (httpResponseCode &gt; 0) {\n    Serial.print(\"Response Code: \");\n    Serial.println(httpResponseCode);\n    String response = http.getString();\n    Serial.println(\"Server Response: \" + response);\n  } else {\n    Serial.print(\"Error Sending Data. HTTP Response: \");\n    Serial.println(httpResponseCode);\n  }\n\n  http.end();\n  delay(1000);  // Send data every 5 seconds\n}\n\nvoid printWifiStatus() {\n  Serial.print(\"SSID: \");\n  Serial.println(WiFi.SSID());\n\n  IPAddress ip = WiFi.localIP();\n  Serial.print(\"IP Address: \");\n  Serial.println(ip);\n\n  long rssi = WiFi.RSSI();\n  Serial.print(\"Signal strength (RSSI): \");\n  Serial.print(rssi);\n  Serial.println(\" dBm\");\n}\n</code></pre>"},{"location":"smathhacks/smathhacks2025Submitted/#frontend","title":"Frontend","text":"<p>The front end was developed using a Next.js framework powered by React, Typescript, Tailwind CSS, and Post CSS. A Postgresql database was deployed using Docker to collect real-time live data from the sensor. To interact with the data, we used Prisma ORM and created our API for the embedded system to send information. We used the Chart.js library to create data visualizations. </p> <p>Postgresql was chosen due to its proficiency in collecting time series and station data structures, which are highly compatible with the relational database and table. Postgresql\u2019s rapid and efficient data storage also allowed us to answer API requests effectively. We deployed our application using docker-compose to securely isolate the database while maintaining client-server interaction. Our front and back end were deployed on a local server, making docker the ideal selection to optimize build speed.. </p> <p>We structured our database to have a table for each station, and specific weather data payloads. This decision allowed us to quickly select databases and find associated weather data  by utilizing prisma\u2019s include parameter when querying.</p> <pre><code>model Station {\n id   String @unique\n name String\n\n\n weatherData WeatherData[]\n}\n\n\nmodel WeatherData {\n id Int @unique @default(autoincrement())\n\n\n latitude  Float\n longitude Float\n\n\n station   Station @relation(fields: [stationId], references: [id])\n stationId String\n\n\n pressure    Float\n temperature Float\n humidity    Float\n co2         Float\n dust        Float\n wind        Float\n uv          Float\n ir          Float\n moisture    Float\n light       Float\n accelX      Float\n accelY      Float\n accelZ      Float\n angVelX     Float\n angVelY     Float\n angVelZ     Float\n\n\n timeTaken DateTime\n}\n</code></pre> <p>We kept this structure consistent between internal data structures, and the format we receive data through the API. We originally included the position in the Station table, but later moved it to the time series data table to account for the stations being uprooted or relocated. </p> <p>In our GET API route, we utilized Prisma\u2019s powerful filtering features to limit and select specific items. We were able to take advantage of this because of our database structure choices. We filter by time, in ascending order, to select the latest entries, which have the largest time stamps. We further filter by only selecting the last 5 hours. Furthermore, we take the data, which contains 60 datapoints per second, and condense it down to 1 datapoint per minute using averages. </p> <pre><code>const data = await prisma.station.findMany({\n  include: {\n  weatherData: {\n   orderBy: {\n    timeTaken: \"asc\"\n   },\n   where: {\n    timeTaken: {\n     gte: new Date(new Date().getMilliseconds() - 1000 * 60 * 60 * 5) \n    }\n   }\n  },\n }\n})\n</code></pre> <p>Our POST API route is used to collect data from the station. It is designed to receive a request from our master node and contains the data for each of the pods. It includes very basic key bearer authentication that simplifies and protects our application. We chose the data structure for the payload in order to be consistent with internal data structures and the database structure, and it was easily implemented on the embedded system on our physical device. We also include data points for our simulated stations that slightly modify the actual data to simulate varying weather conditions in a region. </p> <p>Our UI design choices were based on speed and efficiency. This is why we decided to go with Typescript, as its strongly typed nature allows us to identify type errors before we run code quickly. We also decided to use Tailwindcss in order to quickly style our components. Finally, Next.js allows us to quickly test, build, and deploy. </p> <p>The web interface consists of an interactive map and a navigation bar containing information from the sensors. Different tabs display relevant charts and statistics for corresponding environmental measurements, including Earthquake detection (by calculating an aggregated average of accelerometer values),  soil moisture percentage, average/max/min temperature and humidity, light wavelengths, and geographic position. </p> <p>Using the library ChartJS alongside html canvas, we can create an interactive UI to showcase the data and allow users to vizualize ARES pods. ChartJS allowed us to quickly chart and format our data, and helped us greatly save time and create an appealing front end. Furthermore, html canvas allowed us to place the pods on the map in their exact positions based on their collected latitude and longitude. </p>"},{"location":"smathhacks/smathhacks2025Submitted/#integration","title":"Integration","text":"<p>The backend Arduino code integrates seamlessly with the frontend by using JSON as a standardized data format to transmit sensor information. Within the code, sensor readings from multiple devices\u2014such as temperature, humidity, UV index, IR levels, soil moisture, and motion data from the MPU6050\u2014are structured into a JSON document using the ArduinoJson library. This JSON payload encapsulates the sensor data along with metadata like timestamps and station identifiers, making it easy to manage and extend. Once the payload is constructed, it is serialized into a string and sent via an HTTP POST request to a remote API endpoint. This API, which is typically part of a backend service connected to the frontend, receives the JSON data and processes it accordingly. A sample JSON string which is transmitted will look like the following:</p> <pre><code>{\"time\":22651,\"stations\":[{\"id\":\"STATION_1\",\"latitude\":0.1,\"longitude\":0.1,\"temperature\":23,\"humidity\":36,\"uv\":0.02,\"ir\":254,\"moisture\":52,\"accel\":{\"x\":1.019929886,\"y\":-0.325611442,\"z\":9.361329079},\"angVelocity\":{\"x\":-0.041834611,\"y\":0.022915773,\"z\":-0.007993874}}]}\n</code></pre> <p>On the frontend, the received JSON data can be parsed and dynamically rendered using modern web development frameworks and libraries such as React, Angular, or Vue.js. The structured JSON makes it straightforward for the frontend to extract specific sensor readings and display them in an interactive dashboard\u2014using charts, graphs, and real-time status updates. This approach allows for scalable integration where additional sensor data or stations can be added without disrupting the data flow. </p>"},{"location":"smathhacks/smathhacks2025Submitted/#impact","title":"Impact","text":"<p>The integration of this system into an IoT-based monitoring framework has significant implications for real-time environmental sensing, data transmission, and decision-making. By systematically acquiring data from multiple sensors\u2014including temperature, humidity, ultraviolet (UV) radiation, infrared (IR) levels, soil moisture, and motion tracking via the MPU6050\u2014the system ensures a comprehensive representation of environmental conditions. The collected data is structured into a standardized JSON format, optimizing interoperability between hardware and software layers. Utilizing HTTP POST requests for data transmission enables continuous and automated logging, reducing the reliance on manual intervention and enhancing data accuracy. This approach facilitates real-time analytics and predictive modeling, supporting applications in smart agriculture, climate monitoring, and industrial process optimization.</p> <p>From a systems engineering perspective, the use of JSON as a lightweight and structured data exchange format enhances system scalability and integration with web-based visualization tools. The modularity of the architecture allows for seamless expansion, enabling the incorporation of additional sensors without substantial modifications to the data processing pipeline. Furthermore, real-time data dissemination to frontend interfaces, such as interactive dashboards, enhances user engagement and decision support capabilities. This architecture enables advanced analytical techniques, including anomaly detection and machine learning-driven forecasting, by providing structured and time-synchronized datasets. </p> <p>The system is designed for deployment on Mars, where real-time environmental monitoring is critical for both scientific research and mission sustainability. Mars presents a uniquely hostile environment with extreme temperature variations, low atmospheric pressure, high levels of ultraviolet and infrared radiation, and frequent dust storms. Continuous measurement of atmospheric conditions, radiation exposure, seismic activity, and soil moisture provides critical data for assessing planetary habitability, evaluating geological activity, and informing mission planning. The integration of the MPU6050 inertial measurement unit enables seismic detection, offering insights into subsurface dynamics that could inform structural stability assessments for future landing sites. Data collected from the system is structured in a lightweight JSON format, allowing for efficient transmission to orbiters, surface rovers, or Earth-based mission control centers, facilitating real-time analysis and remote decision-making.</p> <p>Beyond its role in environmental sensing, this system supports the development of autonomous habitat management and resource utilization for sustained human presence on Mars. The ability to monitor radiation levels and atmospheric conditions enables predictive hazard detection, which is crucial for safeguarding both robotic and human-operated systems. Soil moisture analysis contributes to in situ resource utilization strategies, particularly in the search for accessible water reserves necessary for life support, agriculture, and fuel production. The modular architecture ensures adaptability for future missions, allowing for integration with additional sensor arrays suited to evolving scientific and operational requirements. The real-time data stream enhances mission oversight by enabling remote monitoring through interactive visualization platforms, ensuring continuous situational awareness. By providing high-resolution structured environmental data, this system advances the capability of autonomous exploration and habitat sustainability, supporting both near-term scientific missions and long-term human settlement on Mars.</p>"},{"location":"smathhacks/smathhacks2025Submitted/#future-work","title":"Future Work","text":"<p>Future developments in this system can focus on enhancing data processing efficiency, expanding sensor integration, and improving real-time analytics capabilities. One key direction is the implementation of edge computing techniques to preprocess sensor data before transmission, reducing network congestion and minimizing latency in real-time applications. By incorporating onboard machine learning models, such as TinyML for anomaly detection or predictive analytics, the system could autonomously identify critical environmental changes and trigger automated responses. Additionally, improving the robustness of data transmission by implementing MQTT or WebSocket protocols could enable more efficient, low-latency communication compared to the current HTTP-based approach. Furthermore, integrating secure encryption mechanisms for JSON payloads and authentication protocols, such as OAuth 2.0, could enhance data integrity and privacy when transmitting sensitive environmental data to cloud-based platforms.</p>"},{"location":"smathhacks/smathhacks2025Submitted/#work-distribution","title":"Work Distribution","text":"Task Effector Ideation All Documentation Richard Backend Software Development Richard Schematics Richard Sensors Analysis Richard Electrical Engineering Richard, Aaditya Networking and I2C Richard, Aaditya CAD Aaditya Moisture Servo Mount Aaditya System Integration Richard, Aaditya, Trevor Frontend Development Trevor, Josh API Development Trevor Video Script Josh Video Editing Josh Data Processing Trevor, Josh DevOps Trevor"}]}